{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain RAG (Retrieval-Augmented Generation) - Complete Guide\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a comprehensive guide to building a **Retrieval-Augmented Generation (RAG)** system using LangChain. RAG is a powerful technique that combines the strengths of large language models (LLMs) with external knowledge retrieval.\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "RAG enhances LLM responses by:\n",
    "1. **Retrieving** relevant documents from a knowledge base\n",
    "2. **Augmenting** the prompt with retrieved context\n",
    "3. **Generating** informed responses based on both the LLM's knowledge and retrieved documents\n",
    "\n",
    "### RAG Architecture\n",
    "\n",
    "```\n",
    "User Query ‚Üí Embedding ‚Üí Vector Search ‚Üí Retrieved Docs ‚Üí LLM ‚Üí Response\n",
    "                ‚Üì                          ‚Üì\n",
    "         Vector Store ‚Üê Embeddings ‚Üê Document Chunks ‚Üê Documents\n",
    "```\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- Document loading and preprocessing\n",
    "- Text splitting strategies\n",
    "- **OpenAI vs HuggingFace embeddings** (with comparisons)\n",
    "- Vector store creation with FAISS\n",
    "- Different retrieval strategies (Similarity vs MMR)\n",
    "- RAG chain construction\n",
    "- Performance evaluation and best practices\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, we'll install all required dependencies. This includes:\n",
    "- **langchain**: Core framework\n",
    "- **langchain-community**: Community integrations\n",
    "- **langchain-openai**: OpenAI integrations\n",
    "- **langchain-huggingface**: HuggingFace integrations\n",
    "- **openai**: OpenAI API client\n",
    "- **faiss-cpu**: Vector similarity search\n",
    "- **tiktoken**: Token counting\n",
    "- **sentence-transformers**: For HuggingFace embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.14.0' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q langchain langchain-community langchain-openai langchain-huggingface openai faiss-cpu tiktoken sentence-transformers\n",
    "%pip install -q beautifulsoup4 python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure API Keys\n",
    "\n",
    "**Security Best Practice**: Never hardcode API keys in your notebooks. Use environment variables or secure secret management.\n",
    "\n",
    "For Google Colab, use the `userdata` feature. For local environments, use a `.env` file or environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Load API keys from env file and set them explicitly in os.environ\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "# Ensure the keys are set in the environment for libraries to use\n",
    "if OPENAI_API_KEY:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
    "if HUGGINGFACE_API_KEY:\n",
    "    os.environ[\"HUGGINGFACE_API_KEY\"] = HUGGINGFACE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that API keys are loaded correctly\n",
    "print(\"API Keys Status:\")\n",
    "print(f\"OPENAI_API_KEY: {'‚úì Loaded' if OPENAI_API_KEY else '‚úó Not loaded'}\")\n",
    "print(f\"HUGGINGFACE_API_KEY: {'‚úì Loaded' if HUGGINGFACE_API_KEY else '‚úó Not loaded (optional)'}\")\n",
    "\n",
    "# Show first and last 4 characters for security\n",
    "if OPENAI_API_KEY:\n",
    "    print(f\"\\nOpenAI Key Preview: {OPENAI_API_KEY[:7]}...{OPENAI_API_KEY[-4:]}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: OPENAI_API_KEY is not set!\")\n",
    "    \n",
    "if HUGGINGFACE_API_KEY:\n",
    "    print(f\"HuggingFace Key Preview: {HUGGINGFACE_API_KEY[:7]}...{HUGGINGFACE_API_KEY[-4:]}\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è  HuggingFace API key not set (not required for local embeddings)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the OpenAI API key directly\n",
    "from openai import OpenAI\n",
    "\n",
    "print(\"Testing OpenAI API key...\")\n",
    "try:\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    # Try a simple API call\n",
    "    response = client.models.list()\n",
    "    print(\"‚úì API key is VALID! Connection successful.\")\n",
    "    print(f\"  Available models: {len(list(response.data))} models found\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó API key is INVALID!\")\n",
    "    print(f\"  Error: {str(e)}\")\n",
    "    print(\"\\n‚ö†Ô∏è  Please verify your OpenAI API key:\")\n",
    "    print(\"  1. Go to https://platform.openai.com/api-keys\")\n",
    "    print(\"  2. Create a new API key\")\n",
    "    print(\"  3. Update the .env file with the new key\")\n",
    "    print(\"  4. Restart the kernel and rerun from the beginning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Document Loading\n",
    "\n",
    "The first step in building a RAG system is loading documents. LangChain supports various document loaders:\n",
    "- **WebBaseLoader**: Load content from web pages\n",
    "- **PyPDFLoader**: Load PDF files\n",
    "- **TextLoader**: Load plain text files\n",
    "- **DirectoryLoader**: Load multiple files from a directory\n",
    "\n",
    "In this example, we'll load LangChain documentation pages about RAG and related topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import datetime\n",
    "\n",
    "# Define URLs for LangChain documentation on RAG\n",
    "urls = [\n",
    "    \"https://python.langchain.com/docs/use_cases/question_answering/\",\n",
    "    \"https://python.langchain.com/docs/modules/data_connection/retrievers/\",\n",
    "    \"https://python.langchain.com/docs/modules/model_io/llms/\",\n",
    "    \"https://python.langchain.com/docs/use_cases/chatbots/\"\n",
    "]\n",
    "\n",
    "# Initialize WebBaseLoader and load documents\n",
    "print(\"Loading documents from web...\")\n",
    "loader = WebBaseLoader(urls)\n",
    "docs = loader.load()\n",
    "print(f\"‚úì Loaded {len(docs)} documents\")\n",
    "\n",
    "# Add custom metadata to documents\n",
    "# This is useful for filtering and source attribution\n",
    "current_date = datetime.date.today().isoformat()\n",
    "for doc in docs:\n",
    "    doc.metadata['source_type'] = 'web_documentation'\n",
    "    doc.metadata['process_date'] = current_date\n",
    "    doc.metadata['domain'] = 'langchain'\n",
    "\n",
    "print(\"‚úì Added custom metadata to all documents\")\n",
    "\n",
    "# Display first document info\n",
    "if docs:\n",
    "    print(\"\\n--- First Document ---\")\n",
    "    print(f\"Source: {docs[0].metadata.get('source', 'N/A')}\")\n",
    "    print(f\"Content preview (first 300 chars):\\n{docs[0].page_content[:300]}...\")\n",
    "    print(f\"\\nMetadata: {docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Text Splitting Strategies\n",
    "\n",
    "Large documents must be split into smaller chunks for effective retrieval. The key parameters are:\n",
    "\n",
    "- **chunk_size**: Maximum number of characters per chunk\n",
    "- **chunk_overlap**: Number of overlapping characters between chunks\n",
    "\n",
    "### Why Overlap Matters\n",
    "\n",
    "Overlap ensures that context isn't lost at chunk boundaries. For example, if a sentence is split between two chunks, overlap helps preserve its meaning.\n",
    "\n",
    "### Strategy Comparison\n",
    "\n",
    "We'll compare two strategies:\n",
    "1. **Strategy A**: chunk_size=1000, chunk_overlap=200 (better for longer context)\n",
    "2. **Strategy B**: chunk_size=500, chunk_overlap=100 (better for precise retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Strategy A: Larger chunks with more overlap\n",
    "print(\"=== Strategy A: Larger Chunks ===\")\n",
    "text_splitter_a = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "chunks_a = text_splitter_a.split_documents(docs)\n",
    "print(f\"Created {len(chunks_a)} chunks with chunk_size=1000, chunk_overlap=200\")\n",
    "\n",
    "# Strategy B: Smaller chunks with less overlap\n",
    "print(\"\\n=== Strategy B: Smaller Chunks ===\")\n",
    "text_splitter_b = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "chunks_b = text_splitter_b.split_documents(docs)\n",
    "print(f\"Created {len(chunks_b)} chunks with chunk_size=500, chunk_overlap=100\")\n",
    "\n",
    "# Compare strategies\n",
    "print(\"\\n=== Comparison ===\")\n",
    "print(f\"Strategy A: {len(chunks_a)} chunks (fewer, longer chunks)\")\n",
    "print(f\"Strategy B: {len(chunks_b)} chunks (more, shorter chunks)\")\n",
    "print(f\"Ratio: {len(chunks_b) / len(chunks_a):.2f}x more chunks with Strategy B\")\n",
    "\n",
    "# Display sample chunks\n",
    "print(\"\\n--- Strategy A - Sample Chunk ---\")\n",
    "print(f\"Length: {len(chunks_a[0].page_content)} chars\")\n",
    "print(f\"Content: {chunks_a[0].page_content[:300]}...\")\n",
    "\n",
    "print(\"\\n--- Strategy B - Sample Chunk ---\")\n",
    "print(f\"Length: {len(chunks_b[0].page_content)} chars\")\n",
    "print(f\"Content: {chunks_b[0].page_content[:300]}...\")\n",
    "\n",
    "# We'll use Strategy A for the rest of the notebook\n",
    "chunks = chunks_a\n",
    "print(f\"\\n‚úì Using Strategy A ({len(chunks)} chunks) for subsequent examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Embeddings: OpenAI vs HuggingFace\n",
    "\n",
    "Embeddings convert text into numerical vectors that capture semantic meaning. Similar texts have similar vector representations.\n",
    "\n",
    "### Comparison\n",
    "\n",
    "| Feature | OpenAI Embeddings | HuggingFace Embeddings |\n",
    "|---------|-------------------|------------------------|\n",
    "| **Cost** | Pay per token | Free (local) |\n",
    "| **Speed** | Fast (API) | Slower (local compute) |\n",
    "| **Quality** | Very high | Good to high (model-dependent) |\n",
    "| **Privacy** | Data sent to OpenAI | Data stays local |\n",
    "| **Internet** | Required | Not required |\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "- **OpenAI**: Production systems, high quality needed, budget available\n",
    "- **HuggingFace**: Privacy-sensitive data, cost constraints, offline operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 OpenAI Embeddings\n",
    "\n",
    "OpenAI's `text-embedding-3-small` model provides high-quality embeddings with good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "import time\n",
    "\n",
    "print(\"Initializing OpenAI Embeddings...\")\n",
    "openai_embeddings = OpenAIEmbeddings()\n",
    "print(\"‚úì OpenAI Embeddings initialized\")\n",
    "\n",
    "# Test embedding generation\n",
    "test_text = \"What is retrieval-augmented generation?\"\n",
    "start_time = time.time()\n",
    "test_embedding = openai_embeddings.embed_query(test_text)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTest Query: '{test_text}'\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")\n",
    "print(f\"Time taken: {elapsed:.3f}s\")\n",
    "print(f\"First 5 values: {test_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 HuggingFace Embeddings\n",
    "\n",
    "We'll use `sentence-transformers/all-MiniLM-L6-v2`, a popular open-source model that provides good quality embeddings with reasonable speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "print(\"Initializing HuggingFace Embeddings...\")\n",
    "print(\"(First run will download the model - this may take a minute)\\n\")\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "print(\"‚úì HuggingFace Embeddings initialized\")\n",
    "\n",
    "# Test embedding generation\n",
    "start_time = time.time()\n",
    "test_embedding_hf = hf_embeddings.embed_query(test_text)\n",
    "elapsed_hf = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTest Query: '{test_text}'\")\n",
    "print(f\"Embedding dimension: {len(test_embedding_hf)}\")\n",
    "print(f\"Time taken: {elapsed_hf:.3f}s\")\n",
    "print(f\"First 5 values: {test_embedding_hf[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Side-by-Side Comparison\n",
    "\n",
    "Let's compare the two embedding approaches with the same test query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"=== Embeddings Comparison ===\")\n",
    "print(f\"\\nTest Query: '{test_text}'\\n\")\n",
    "\n",
    "comparison_data = [\n",
    "    [\"Feature\", \"OpenAI\", \"HuggingFace\"],\n",
    "    [\"Dimension\", len(test_embedding), len(test_embedding_hf)],\n",
    "    [\"Time (s)\", f\"{elapsed:.3f}\", f\"{elapsed_hf:.3f}\"],\n",
    "    [\"Mean value\", f\"{np.mean(test_embedding):.4f}\", f\"{np.mean(test_embedding_hf):.4f}\"],\n",
    "    [\"Std dev\", f\"{np.std(test_embedding):.4f}\", f\"{np.std(test_embedding_hf):.4f}\"]\n",
    "]\n",
    "\n",
    "# Print comparison table\n",
    "col_widths = [max(len(str(row[i])) for row in comparison_data) + 2 for i in range(3)]\n",
    "for i, row in enumerate(comparison_data):\n",
    "    print(\"\".join(str(item).ljust(col_widths[j]) for j, item in enumerate(row)))\n",
    "    if i == 0:\n",
    "        print(\"-\" * sum(col_widths))\n",
    "\n",
    "print(\"\\nüí° Key Takeaway:\")\n",
    "print(\"   - OpenAI: Higher dimension (1536), typically higher quality\")\n",
    "print(\"   - HuggingFace: Lower dimension (384), faster and free\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Vector Store Creation\n",
    "\n",
    "Vector stores enable efficient similarity search over embeddings. We use **FAISS** (Facebook AI Similarity Search), which provides:\n",
    "- Fast similarity search\n",
    "- Efficient memory usage\n",
    "- Support for large-scale datasets\n",
    "\n",
    "We'll create two vector stores to compare both embedding approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Vector Store with OpenAI Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print(\"Creating FAISS vector store with OpenAI embeddings...\")\n",
    "start_time = time.time()\n",
    "\n",
    "vectorstore_openai = FAISS.from_documents(chunks, openai_embeddings)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"‚úì Vector store created in {elapsed:.2f}s\")\n",
    "print(f\"  - {len(chunks)} documents indexed\")\n",
    "print(\"  - Embedding dimension: 1536\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Vector Store with HuggingFace Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating FAISS vector store with HuggingFace embeddings...\")\n",
    "start_time = time.time()\n",
    "\n",
    "vectorstore_hf = FAISS.from_documents(chunks, hf_embeddings)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"‚úì Vector store created in {elapsed:.2f}s\")\n",
    "print(f\"  - {len(chunks)} documents indexed\")\n",
    "print(\"  - Embedding dimension: 384\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Test Similarity Search\n",
    "\n",
    "Let's test similarity search on both vector stores to see how they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How to build a RAG agent with LangChain?\"\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test OpenAI vector store\n",
    "print(\"\\n--- OpenAI Embeddings Results ---\")\n",
    "results_openai = vectorstore_openai.similarity_search(query, k=3)\n",
    "for i, doc in enumerate(results_openai, 1):\n",
    "    print(f\"\\n{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   Content: {doc.page_content[:200]}...\")\n",
    "\n",
    "# Test HuggingFace vector store\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n--- HuggingFace Embeddings Results ---\")\n",
    "results_hf = vectorstore_hf.similarity_search(query, k=3)\n",
    "for i, doc in enumerate(results_hf, 1):\n",
    "    print(f\"\\n{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   Content: {doc.page_content[:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüí° Notice how both retrievers find relevant documents, though ordering may differ.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Retrieval Strategies\n",
    "\n",
    "Different retrieval strategies optimize for different goals:\n",
    "\n",
    "### Similarity Search\n",
    "- Returns documents most similar to the query\n",
    "- Simple and fast\n",
    "- May return redundant documents\n",
    "\n",
    "### MMR (Maximal Marginal Relevance)\n",
    "- Balances relevance with diversity\n",
    "- Reduces redundancy in results\n",
    "- Particularly useful when documents contain similar information\n",
    "- Controlled by `lambda_mult` parameter:\n",
    "  - `lambda_mult=1.0`: Pure relevance (like similarity search)\n",
    "  - `lambda_mult=0.0`: Pure diversity\n",
    "  - `lambda_mult=0.5`: Balanced (recommended)\n",
    "\n",
    "Let's compare both strategies using the OpenAI vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Standard Similarity Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create similarity-based retriever\n",
    "similarity_retriever = vectorstore_openai.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}  # Retrieve top 4 documents\n",
    ")\n",
    "\n",
    "print(\"‚úì Similarity retriever created\")\n",
    "print(\"  - Search type: similarity\")\n",
    "print(\"  - Documents to retrieve: 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 MMR (Maximal Marginal Relevance) Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MMR-based retriever\n",
    "mmr_retriever = vectorstore_openai.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 4,              # Number of documents to return\n",
    "        \"fetch_k\": 20,       # Number of documents to fetch before MMR\n",
    "        \"lambda_mult\": 0.5   # Balance between relevance (1.0) and diversity (0.0)\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úì MMR retriever created\")\n",
    "print(\"  - Search type: mmr\")\n",
    "print(\"  - Documents to retrieve: 4\")\n",
    "print(\"  - Fetch size: 20\")\n",
    "print(\"  - Lambda (relevance/diversity): 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Compare Retrieval Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the steps to build a RAG agent with LangChain?\"\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test similarity retriever\n",
    "print(\"\\n--- Similarity Search Results ---\")\n",
    "similarity_docs = similarity_retriever.get_relevant_documents(query)\n",
    "for i, doc in enumerate(similarity_docs, 1):\n",
    "    print(f\"\\n{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   Content: {doc.page_content[:200]}...\")\n",
    "\n",
    "# Test MMR retriever\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n--- MMR Search Results ---\")\n",
    "mmr_docs = mmr_retriever.get_relevant_documents(query)\n",
    "for i, doc in enumerate(mmr_docs, 1):\n",
    "    print(f\"\\n{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   Content: {doc.page_content[:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüí° Key Observation:\")\n",
    "print(\"   MMR results should show more diversity in sources and content\")\n",
    "print(\"   while still maintaining relevance to the query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. RAG Chain Construction\n",
    "\n",
    "Now we'll build complete RAG chains that combine:\n",
    "1. **LLM**: Generates answers\n",
    "2. **Retriever**: Finds relevant documents\n",
    "3. **Prompt**: Structures the input\n",
    "4. **Chain**: Orchestrates the flow\n",
    "\n",
    "The chain workflow:\n",
    "```\n",
    "User Query ‚Üí Retriever ‚Üí Retrieved Docs ‚Üí Prompt + LLM ‚Üí Final Answer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Initialize LLM\n",
    "\n",
    "We'll use GPT-4o-mini for cost-effectiveness. For production, consider GPT-4 for higher quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0  # Deterministic responses\n",
    ")\n",
    "\n",
    "print(\"‚úì ChatOpenAI LLM initialized\")\n",
    "print(\"  - Model: gpt-4o-mini\")\n",
    "print(\"  - Temperature: 0 (deterministic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Create Prompt Template\n",
    "\n",
    "The prompt instructs the LLM on how to use the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful AI assistant. Answer the user's question based on the context provided below.\n",
    "    \n",
    "If the context doesn't contain enough information to answer the question, say so clearly.\n",
    "Always cite which parts of the context you used to formulate your answer.\n",
    "\n",
    "Context:\n",
    "{context}\"\"\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "print(\"‚úì Prompt template created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Build Document Chain and Retrieval Chains\n",
    "\n",
    "We'll create:\n",
    "1. A document chain that combines LLM with prompt\n",
    "2. Retrieval chains for both similarity and MMR strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "# Create document combining chain\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "print(\"‚úì Document chain created\")\n",
    "\n",
    "# Create retrieval chain with similarity search\n",
    "similarity_retrieval_chain = create_retrieval_chain(\n",
    "    similarity_retriever, \n",
    "    document_chain\n",
    ")\n",
    "print(\"‚úì Similarity retrieval chain created\")\n",
    "\n",
    "# Create retrieval chain with MMR\n",
    "mmr_retrieval_chain = create_retrieval_chain(\n",
    "    mmr_retriever, \n",
    "    document_chain\n",
    ")\n",
    "print(\"‚úì MMR retrieval chain created\")\n",
    "\n",
    "print(\"\\n‚úì RAG chains ready for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. RAG in Action: Comparison & Evaluation\n",
    "\n",
    "Let's test our RAG chains with various queries and compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Test Query with Similarity Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "user_query = \"How to build a RAG agent with LangChain?\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"QUERY: {user_query}\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n--- SIMILARITY RETRIEVAL CHAIN ---\\n\")\n",
    "\n",
    "# Invoke similarity retrieval chain\n",
    "response_similarity = similarity_retrieval_chain.invoke({\"input\": user_query})\n",
    "\n",
    "print(\"Retrieved Documents (Context):\")\n",
    "print(\"-\" * 80)\n",
    "for i, doc in enumerate(response_similarity[\"context\"], 1):\n",
    "    print(f\"\\n{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   Content: {doc.page_content[:300]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATED ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(response_similarity[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Test Same Query with MMR Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(f\"QUERY: {user_query}\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n--- MMR RETRIEVAL CHAIN ---\\n\")\n",
    "\n",
    "# Invoke MMR retrieval chain\n",
    "response_mmr = mmr_retrieval_chain.invoke({\"input\": user_query})\n",
    "\n",
    "print(\"Retrieved Documents (Context):\")\n",
    "print(\"-\" * 80)\n",
    "for i, doc in enumerate(response_mmr[\"context\"], 1):\n",
    "    print(f\"\\n{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   Content: {doc.page_content[:300]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATED ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(response_mmr[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Compare OpenAI vs HuggingFace Embeddings\n",
    "\n",
    "Let's create a retrieval chain using the HuggingFace vector store and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create retriever from HuggingFace vector store\n",
    "hf_retriever = vectorstore_hf.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "\n",
    "# Create retrieval chain with HuggingFace embeddings\n",
    "hf_retrieval_chain = create_retrieval_chain(hf_retriever, document_chain)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"QUERY: {user_query}\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n--- HUGGINGFACE EMBEDDINGS CHAIN ---\\n\")\n",
    "\n",
    "# Invoke HuggingFace retrieval chain\n",
    "response_hf = hf_retrieval_chain.invoke({\"input\": user_query})\n",
    "\n",
    "print(\"Retrieved Documents (Context):\")\n",
    "print(\"-\" * 80)\n",
    "for i, doc in enumerate(response_hf[\"context\"], 1):\n",
    "    print(f\"\\n{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   Content: {doc.page_content[:300]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATED ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(response_hf[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Summary Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. OpenAI Embeddings + Similarity Search\")\n",
    "print(f\"   Answer length: {len(response_similarity['answer'])} chars\")\n",
    "print(f\"   Documents retrieved: {len(response_similarity['context'])}\")\n",
    "\n",
    "print(\"\\n2. OpenAI Embeddings + MMR Search\")\n",
    "print(f\"   Answer length: {len(response_mmr['answer'])} chars\")\n",
    "print(f\"   Documents retrieved: {len(response_mmr['context'])}\")\n",
    "\n",
    "print(\"\\n3. HuggingFace Embeddings + Similarity Search\")\n",
    "print(f\"   Answer length: {len(response_hf['answer'])} chars\")\n",
    "print(f\"   Documents retrieved: {len(response_hf['context'])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   - All approaches provide relevant answers\")\n",
    "print(\"   - MMR may provide more diverse context\")\n",
    "print(\"   - HuggingFace embeddings are competitive and free\")\n",
    "print(\"   - Choice depends on: budget, privacy needs, and quality requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Advanced Features\n",
    "\n",
    "### 9.1 Custom Metadata Filtering\n",
    "\n",
    "We added custom metadata earlier. Now let's use it to filter results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Filter by source domain\n",
    "print(\"Sample metadata from our documents:\")\n",
    "sample_doc = chunks[0]\n",
    "print(f\"\\nMetadata: {sample_doc.metadata}\")\n",
    "\n",
    "# Create a retriever with metadata filter\n",
    "filtered_retriever = vectorstore_openai.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 4,\n",
    "        \"filter\": {\"source_type\": \"web_documentation\"}  # Filter by our custom metadata\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Filtered retriever created\")\n",
    "print(\"  - Filter: source_type = 'web_documentation'\")\n",
    "\n",
    "# Test filtered retrieval\n",
    "query = \"What is a retriever in LangChain?\"\n",
    "filtered_docs = filtered_retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "print(f\"Retrieved {len(filtered_docs)} documents with metadata filter\\n\")\n",
    "\n",
    "for i, doc in enumerate(filtered_docs[:2], 1):\n",
    "    print(f\"{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   Source Type: {doc.metadata.get('source_type', 'N/A')}\")\n",
    "    print(f\"   Process Date: {doc.metadata.get('process_date', 'N/A')}\")\n",
    "    print(f\"   Content: {doc.page_content[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Source Attribution\n",
    "\n",
    "Show which sources were used to generate the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the key components of a RAG system?\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "response = similarity_retrieval_chain.invoke({\"input\": query})\n",
    "\n",
    "print(\"\\nANSWER:\")\n",
    "print(\"-\" * 80)\n",
    "print(response[\"answer\"])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nSOURCES USED:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Extract unique sources\n",
    "sources = set()\n",
    "for doc in response[\"context\"]:\n",
    "    source = doc.metadata.get('source', 'Unknown')\n",
    "    sources.add(source)\n",
    "\n",
    "for i, source in enumerate(sorted(sources), 1):\n",
    "    print(f\"{i}. {source}\")\n",
    "\n",
    "print(\"\\nüí° Always cite sources to build trust and enable verification!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Best Practices & Common Pitfalls\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Chunk Size Selection**\n",
    "   - Smaller chunks (300-500): Better for precise information retrieval\n",
    "   - Larger chunks (800-1200): Better for context-heavy questions\n",
    "   - Always use overlap (100-200 chars) to preserve context\n",
    "\n",
    "2. **Embedding Selection**\n",
    "   - **OpenAI**: Best quality, suitable for production, requires API key\n",
    "   - **HuggingFace**: Free, private, good for development and privacy-sensitive data\n",
    "   - Test both with your specific use case\n",
    "\n",
    "3. **Retrieval Strategy**\n",
    "   - **Similarity**: Use for most cases, simple and effective\n",
    "   - **MMR**: Use when you need diverse results and want to avoid redundancy\n",
    "   - Experiment with `k` (number of documents) - typically 3-5 is good\n",
    "\n",
    "4. **Prompt Engineering**\n",
    "   - Always instruct the model to say when it doesn't know\n",
    "   - Request source citations for transparency\n",
    "   - Be specific about the expected format\n",
    "\n",
    "5. **Metadata Management**\n",
    "   - Add custom metadata for filtering and attribution\n",
    "   - Include source URLs, dates, document types\n",
    "   - Use metadata for access control in production\n",
    "\n",
    "### Common Pitfalls to Avoid\n",
    "\n",
    "#### 1. Undefined Variables\n",
    "```python\n",
    "# ‚ùå WRONG: Using retriever before defining it\n",
    "chain = create_retrieval_chain(mmr_retriever, document_chain)\n",
    "mmr_retriever = vectorstore.as_retriever(search_type=\"mmr\")\n",
    "\n",
    "# ‚úÖ CORRECT: Define before using\n",
    "mmr_retriever = vectorstore.as_retriever(search_type=\"mmr\")\n",
    "chain = create_retrieval_chain(mmr_retriever, document_chain)\n",
    "```\n",
    "\n",
    "#### 2. Not Using Created Objects\n",
    "```python\n",
    "# ‚ùå WRONG: Creating embeddings but not using them\n",
    "hf_embeddings = HuggingFaceEmbeddings()\n",
    "vectorstore = FAISS.from_documents(chunks, openai_embeddings)  # Uses OpenAI instead!\n",
    "\n",
    "# ‚úÖ CORRECT: Use what you create\n",
    "hf_embeddings = HuggingFaceEmbeddings()\n",
    "vectorstore = FAISS.from_documents(chunks, hf_embeddings)\n",
    "```\n",
    "\n",
    "#### 3. Incorrect Retriever Configuration\n",
    "```python\n",
    "# ‚ùå WRONG: Invalid search_type\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr_search\")  # Invalid type!\n",
    "\n",
    "# ‚úÖ CORRECT: Valid search types\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\")\n",
    "# OR\n",
    "retriever = vectorstore.as_retriever(search_type=\"mmr\")\n",
    "```\n",
    "\n",
    "#### 4. Missing Dependencies\n",
    "```python\n",
    "# ‚ùå WRONG: Importing wrong package\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # Deprecated!\n",
    "\n",
    "# ‚úÖ CORRECT: Use the right package\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "```\n",
    "\n",
    "#### 5. Not Handling Edge Cases\n",
    "```python\n",
    "# ‚ùå WRONG: No error handling\n",
    "response = chain.invoke({\"input\": query})\n",
    "print(response[\"answer\"])\n",
    "\n",
    "# ‚úÖ CORRECT: Handle potential errors\n",
    "try:\n",
    "    response = chain.invoke({\"input\": query})\n",
    "    if \"answer\" in response:\n",
    "        print(response[\"answer\"])\n",
    "    else:\n",
    "        print(\"No answer generated\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "```\n",
    "\n",
    "### Performance Optimization Tips\n",
    "\n",
    "1. **Batch Processing**: Process multiple documents at once when creating embeddings\n",
    "2. **Caching**: Save and load vector stores instead of recreating them\n",
    "3. **Async Operations**: Use async methods for parallel processing\n",
    "4. **Indexing**: For large datasets, consider more sophisticated indexing strategies\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "1. **Error Handling**: Add comprehensive error handling and logging\n",
    "2. **Rate Limiting**: Respect API rate limits (especially OpenAI)\n",
    "3. **Monitoring**: Track retrieval quality, latency, and costs\n",
    "4. **Security**: Sanitize user inputs, manage API keys securely\n",
    "5. **Versioning**: Track model versions and embeddings for reproducibility\n",
    "6. **Evaluation**: Regularly evaluate RAG quality with test questions\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You now have a complete, production-ready RAG system! This notebook covered:\n",
    "\n",
    "‚úÖ Document loading and preprocessing  \n",
    "‚úÖ Text splitting strategies  \n",
    "‚úÖ OpenAI vs HuggingFace embeddings comparison  \n",
    "‚úÖ Vector store creation  \n",
    "‚úÖ Similarity vs MMR retrieval strategies  \n",
    "‚úÖ Complete RAG chain construction  \n",
    "‚úÖ Advanced features (metadata, filtering, source attribution)  \n",
    "‚úÖ Best practices and common pitfalls  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Experiment** with different chunk sizes and retrieval strategies\n",
    "2. **Evaluate** performance on your specific use case\n",
    "3. **Add** your own documents and data sources\n",
    "4. **Enhance** with conversational memory for multi-turn dialogues\n",
    "5. **Deploy** to production with proper monitoring\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [FAISS Documentation](https://github.com/facebookresearch/faiss)\n",
    "- [OpenAI Embeddings](https://platform.openai.com/docs/guides/embeddings)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Building! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
