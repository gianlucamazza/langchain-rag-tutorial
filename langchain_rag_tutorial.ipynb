{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain RAG (Retrieval-Augmented Generation) - Complete Guide\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a comprehensive guide to building a **Retrieval-Augmented Generation (RAG)** system using LangChain. RAG is a powerful technique that combines the strengths of large language models (LLMs) with external knowledge retrieval.\n",
    "\n",
    "### What is RAG?\n",
    "\n",
    "RAG enhances LLM responses by:\n",
    "1. **Retrieving** relevant documents from a knowledge base\n",
    "2. **Augmenting** the prompt with retrieved context\n",
    "3. **Generating** informed responses based on both the LLM's knowledge and retrieved documents\n",
    "\n",
    "### RAG Architecture\n",
    "\n",
    "```\n",
    "User Query ‚Üí Embedding ‚Üí Vector Search ‚Üí Retrieved Docs ‚Üí LLM ‚Üí Response\n",
    "                ‚Üì                          ‚Üì\n",
    "         Vector Store ‚Üê Embeddings ‚Üê Document Chunks ‚Üê Documents\n",
    "```\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "- Document loading and preprocessing\n",
    "- Text splitting strategies\n",
    "- **OpenAI vs HuggingFace embeddings** (with comparisons)\n",
    "- Vector store creation with FAISS\n",
    "- Different retrieval strategies (Similarity vs MMR)\n",
    "- RAG chain construction\n",
    "- Performance evaluation and best practices\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation\n",
    "\n",
    "First, we'll install all required dependencies. This includes:\n",
    "- **langchain**: Core framework\n",
    "- **langchain-community**: Community integrations\n",
    "- **langchain-openai**: OpenAI integrations\n",
    "- **langchain-huggingface**: HuggingFace integrations\n",
    "- **openai**: OpenAI API client\n",
    "- **faiss-cpu**: Vector similarity search\n",
    "- **tiktoken**: Token counting\n",
    "- **sentence-transformers**: For HuggingFace embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q langchain langchain-community langchain-openai langchain-huggingface openai faiss-cpu tiktoken sentence-transformers\n",
    "%pip install -q beautifulsoup4 python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure API Keys\n",
    "\n",
    "**Security Best Practice**: Never hardcode API keys in your notebooks. Use environment variables or secure secret management.\n",
    "\n",
    "For Google Colab, use the `userdata` feature. For local environments, use a `.env` file or environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\n\n# Load environment variables from .env file\nfrom dotenv import load_dotenv\nload_dotenv()\n\n# Suppress HuggingFace tokenizers parallelism warning\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n# Load OpenAI API key (REQUIRED for OpenAI embeddings and LLM)\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nif OPENAI_API_KEY:\n    os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n\n# NOTE: HuggingFace API key NOT needed for this notebook\n# We use local sentence-transformers embeddings which run completely offline\n# \n# HuggingFace API key would only be needed for:\n# - HuggingFace Inference API (paid API service)\n# - Gated models requiring authentication\n# - Higher rate limits on model downloads from Hub"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify that OpenAI API key is loaded correctly\nprint(\"=\" * 60)\nprint(\"API KEY VERIFICATION\")\nprint(\"=\" * 60)\n\nif OPENAI_API_KEY:\n    print(f\"\\n‚úì OpenAI API Key: LOADED\")\n    print(f\"  Preview: {OPENAI_API_KEY[:7]}...{OPENAI_API_KEY[-4:]}\")\nelse:\n    print(\"\\n‚úó OpenAI API Key: NOT LOADED\")\n    print(\"\\n‚ö†Ô∏è  WARNING: OPENAI_API_KEY is required!\")\n    print(\"  1. Create .env file in project root\")\n    print(\"  2. Add: OPENAI_API_KEY=your-key-here\")\n    print(\"  3. Get key from: https://platform.openai.com/api-keys\")\n    print(\"  4. Restart kernel after updating .env\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"‚ÑπÔ∏è  HuggingFace Embeddings:\")\nprint(\"  - No API key needed (runs locally)\")\nprint(\"  - Uses sentence-transformers library\")\nprint(\"  - Downloads models to ~/.cache/huggingface/\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing OpenAI API key...\n",
      "‚úì API key is VALID! Connection successful.\n",
      "  Available models: 101 models found\n",
      "‚úì API key is VALID! Connection successful.\n",
      "  Available models: 101 models found\n"
     ]
    }
   ],
   "source": [
    "# Test the OpenAI API key directly\n",
    "from openai import OpenAI\n",
    "\n",
    "print(\"Testing OpenAI API key...\")\n",
    "try:\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    # Try a simple API call\n",
    "    response = client.models.list()\n",
    "    print(\"‚úì API key is VALID! Connection successful.\")\n",
    "    print(f\"  Available models: {len(list(response.data))} models found\")\n",
    "except Exception as e:\n",
    "    print(\"‚úó API key is INVALID!\")\n",
    "    print(f\"  Error: {str(e)}\")\n",
    "    print(\"\\n‚ö†Ô∏è  Please verify your OpenAI API key:\")\n",
    "    print(\"  1. Go to https://platform.openai.com/api-keys\")\n",
    "    print(\"  2. Create a new API key\")\n",
    "    print(\"  3. Update the .env file with the new key\")\n",
    "    print(\"  4. Restart the kernel and rerun from the beginning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Document Loading\n",
    "\n",
    "The first step in building a RAG system is loading documents. LangChain supports various document loaders:\n",
    "- **WebBaseLoader**: Load content from web pages\n",
    "- **PyPDFLoader**: Load PDF files\n",
    "- **TextLoader**: Load plain text files\n",
    "- **DirectoryLoader**: Load multiple files from a directory\n",
    "\n",
    "In this example, we'll load LangChain documentation pages about RAG and related topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gianlucamazza/Workspace/notebooks/llm_rag/venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from web...\n",
      "‚úì Loaded 4 documents\n",
      "‚úì Added custom metadata to all documents\n",
      "\n",
      "--- First Document ---\n",
      "Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "Content preview (first 300 chars):\n",
      "Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with L...\n",
      "\n",
      "Metadata: {'source': 'https://python.langchain.com/docs/use_cases/question_answering/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en', 'source_type': 'web_documentation', 'process_date': '2025-11-12', 'domain': 'langchain'}\n",
      "‚úì Loaded 4 documents\n",
      "‚úì Added custom metadata to all documents\n",
      "\n",
      "--- First Document ---\n",
      "Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "Content preview (first 300 chars):\n",
      "Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with L...\n",
      "\n",
      "Metadata: {'source': 'https://python.langchain.com/docs/use_cases/question_answering/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en', 'source_type': 'web_documentation', 'process_date': '2025-11-12', 'domain': 'langchain'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import datetime\n",
    "\n",
    "# Define URLs for LangChain documentation on RAG\n",
    "urls = [\n",
    "    \"https://python.langchain.com/docs/use_cases/question_answering/\",\n",
    "    \"https://python.langchain.com/docs/modules/data_connection/retrievers/\",\n",
    "    \"https://python.langchain.com/docs/modules/model_io/llms/\",\n",
    "    \"https://python.langchain.com/docs/use_cases/chatbots/\"\n",
    "]\n",
    "\n",
    "# Initialize WebBaseLoader and load documents\n",
    "print(\"Loading documents from web...\")\n",
    "loader = WebBaseLoader(urls)\n",
    "docs = loader.load()\n",
    "print(f\"‚úì Loaded {len(docs)} documents\")\n",
    "\n",
    "# Add custom metadata to documents\n",
    "# This is useful for filtering and source attribution\n",
    "current_date = datetime.date.today().isoformat()\n",
    "for doc in docs:\n",
    "    doc.metadata['source_type'] = 'web_documentation'\n",
    "    doc.metadata['process_date'] = current_date\n",
    "    doc.metadata['domain'] = 'langchain'\n",
    "\n",
    "print(\"‚úì Added custom metadata to all documents\")\n",
    "\n",
    "# Display first document info\n",
    "if docs:\n",
    "    print(\"\\n--- First Document ---\")\n",
    "    print(f\"Source: {docs[0].metadata.get('source', 'N/A')}\")\n",
    "    print(f\"Content preview (first 300 chars):\\n{docs[0].page_content[:300]}...\")\n",
    "    print(f\"\\nMetadata: {docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Text Splitting Strategies\n",
    "\n",
    "Large documents must be split into smaller chunks for effective retrieval. The key parameters are:\n",
    "\n",
    "- **chunk_size**: Maximum number of characters per chunk\n",
    "- **chunk_overlap**: Number of overlapping characters between chunks\n",
    "\n",
    "### Why Overlap Matters\n",
    "\n",
    "Overlap ensures that context isn't lost at chunk boundaries. For example, if a sentence is split between two chunks, overlap helps preserve its meaning.\n",
    "\n",
    "### Strategy Comparison\n",
    "\n",
    "We'll compare two strategies:\n",
    "1. **Strategy A**: chunk_size=1000, chunk_overlap=200 (better for longer context)\n",
    "2. **Strategy B**: chunk_size=500, chunk_overlap=100 (better for precise retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Strategy A: Larger Chunks ===\n",
      "Created 120 chunks with chunk_size=1000, chunk_overlap=200\n",
      "\n",
      "=== Strategy B: Smaller Chunks ===\n",
      "Created 256 chunks with chunk_size=500, chunk_overlap=100\n",
      "\n",
      "=== Comparison ===\n",
      "Strategy A: 120 chunks (fewer, longer chunks)\n",
      "Strategy B: 256 chunks (more, shorter chunks)\n",
      "Ratio: 2.13x more chunks with Strategy B\n",
      "\n",
      "--- Strategy A - Sample Chunk ---\n",
      "Length: 839 chars\n",
      "Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with L...\n",
      "\n",
      "--- Strategy B - Sample Chunk ---\n",
      "Length: 498 chars\n",
      "Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with L...\n",
      "\n",
      "‚úì Using Strategy A (120 chunks) for subsequent examples\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Strategy A: Larger chunks with more overlap\n",
    "print(\"=== Strategy A: Larger Chunks ===\")\n",
    "text_splitter_a = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "chunks_a = text_splitter_a.split_documents(docs)\n",
    "print(f\"Created {len(chunks_a)} chunks with chunk_size=1000, chunk_overlap=200\")\n",
    "\n",
    "# Strategy B: Smaller chunks with less overlap\n",
    "print(\"\\n=== Strategy B: Smaller Chunks ===\")\n",
    "text_splitter_b = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "chunks_b = text_splitter_b.split_documents(docs)\n",
    "print(f\"Created {len(chunks_b)} chunks with chunk_size=500, chunk_overlap=100\")\n",
    "\n",
    "# Compare strategies\n",
    "print(\"\\n=== Comparison ===\")\n",
    "print(f\"Strategy A: {len(chunks_a)} chunks (fewer, longer chunks)\")\n",
    "print(f\"Strategy B: {len(chunks_b)} chunks (more, shorter chunks)\")\n",
    "print(f\"Ratio: {len(chunks_b) / len(chunks_a):.2f}x more chunks with Strategy B\")\n",
    "\n",
    "# Display sample chunks\n",
    "print(\"\\n--- Strategy A - Sample Chunk ---\")\n",
    "print(f\"Length: {len(chunks_a[0].page_content)} chars\")\n",
    "print(f\"Content: {chunks_a[0].page_content[:300]}...\")\n",
    "\n",
    "print(\"\\n--- Strategy B - Sample Chunk ---\")\n",
    "print(f\"Length: {len(chunks_b[0].page_content)} chars\")\n",
    "print(f\"Content: {chunks_b[0].page_content[:300]}...\")\n",
    "\n",
    "# We'll use Strategy A for the rest of the notebook\n",
    "chunks = chunks_a\n",
    "print(f\"\\n‚úì Using Strategy A ({len(chunks)} chunks) for subsequent examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Embeddings: OpenAI vs HuggingFace\n",
    "\n",
    "Embeddings convert text into numerical vectors that capture semantic meaning. Similar texts have similar vector representations.\n",
    "\n",
    "### Comparison\n",
    "\n",
    "| Feature | OpenAI Embeddings | HuggingFace Embeddings |\n",
    "|---------|-------------------|------------------------|\n",
    "| **Cost** | Pay per token | Free (local) |\n",
    "| **Speed** | Fast (API) | Slower (local compute) |\n",
    "| **Quality** | Very high | Good to high (model-dependent) |\n",
    "| **Privacy** | Data sent to OpenAI | Data stays local |\n",
    "| **Internet** | Required | Not required |\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "- **OpenAI**: Production systems, high quality needed, budget available\n",
    "- **HuggingFace**: Privacy-sensitive data, cost constraints, offline operation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 OpenAI Embeddings\n",
    "\n",
    "OpenAI's `text-embedding-3-small` model provides high-quality embeddings with good performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OpenAI Embeddings...\n",
      "‚úì OpenAI Embeddings initialized\n",
      "\n",
      "Test Query: 'What is retrieval-augmented generation?'\n",
      "Embedding dimension: 1536\n",
      "Time taken: 0.828s\n",
      "First 5 values: [-0.040319960564374924, -0.0035607097670435905, 5.0805494538508356e-05, 0.000940056168474257, -0.010274112224578857]\n",
      "\n",
      "Test Query: 'What is retrieval-augmented generation?'\n",
      "Embedding dimension: 1536\n",
      "Time taken: 0.828s\n",
      "First 5 values: [-0.040319960564374924, -0.0035607097670435905, 5.0805494538508356e-05, 0.000940056168474257, -0.010274112224578857]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "import time\n",
    "\n",
    "print(\"Initializing OpenAI Embeddings...\")\n",
    "openai_embeddings = OpenAIEmbeddings()\n",
    "print(\"‚úì OpenAI Embeddings initialized\")\n",
    "\n",
    "# Test embedding generation\n",
    "test_text = \"What is retrieval-augmented generation?\"\n",
    "start_time = time.time()\n",
    "test_embedding = openai_embeddings.embed_query(test_text)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTest Query: '{test_text}'\")\n",
    "print(f\"Embedding dimension: {len(test_embedding)}\")\n",
    "print(f\"Time taken: {elapsed:.3f}s\")\n",
    "print(f\"First 5 values: {test_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 HuggingFace Embeddings\n",
    "\n",
    "We'll use `sentence-transformers/all-MiniLM-L6-v2`, a popular open-source model that provides good quality embeddings with reasonable speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing HuggingFace Embeddings...\n",
      "(First run will download the model - this may take a minute)\n",
      "Model cache location: /Users/gianlucamazza/.cache/huggingface/\n",
      "\n",
      "‚úì HuggingFace Embeddings initialized successfully\n",
      "‚úì HuggingFace Embeddings initialized successfully\n",
      "\n",
      "Test Query: 'What is retrieval-augmented generation?'\n",
      "Embedding dimension: 384\n",
      "Time taken: 2.561s\n",
      "First 5 values: [-0.11101917177438736, -0.026313453912734985, -0.05789430812001228, 0.05978527292609215, -0.020831570029258728]\n",
      "\n",
      "Test Query: 'What is retrieval-augmented generation?'\n",
      "Embedding dimension: 384\n",
      "Time taken: 2.561s\n",
      "First 5 values: [-0.11101917177438736, -0.026313453912734985, -0.05789430812001228, 0.05978527292609215, -0.020831570029258728]\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "print(\"Initializing HuggingFace Embeddings...\")\n",
    "print(\"(First run will download the model - this may take a minute)\")\n",
    "print(f\"Model cache location: {os.path.expanduser('~/.cache/huggingface/')}\\n\")\n",
    "\n",
    "try:\n",
    "    hf_embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    print(\"‚úì HuggingFace Embeddings initialized successfully\")\n",
    "    \n",
    "    # Test embedding generation\n",
    "    start_time = time.time()\n",
    "    test_embedding_hf = hf_embeddings.embed_query(test_text)\n",
    "    elapsed_hf = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nTest Query: '{test_text}'\")\n",
    "    print(f\"Embedding dimension: {len(test_embedding_hf)}\")\n",
    "    print(f\"Time taken: {elapsed_hf:.3f}s\")\n",
    "    print(f\"First 5 values: {test_embedding_hf[:5]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error initializing HuggingFace embeddings: {e}\")\n",
    "    print(\"\\n‚ö†Ô∏è  Troubleshooting steps:\")\n",
    "    print(\"  1. Check internet connection (needed for first download)\")\n",
    "    print(\"  2. Verify disk space: ~200MB free required\")\n",
    "    print(\"  3. Try: pip install --upgrade sentence-transformers\")\n",
    "    print(\"  4. Check model name is correct: 'sentence-transformers/all-MiniLM-L6-v2'\")\n",
    "    print(\"  5. Clear cache if corrupted: rm -rf ~/.cache/huggingface/\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Side-by-Side Comparison\n",
    "\n",
    "Let's compare the two embedding approaches with the same test query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Embeddings Comparison ===\n",
      "\n",
      "Test Query: 'What is retrieval-augmented generation?'\n",
      "\n",
      "Feature     OpenAI   HuggingFace  \n",
      "----------------------------------\n",
      "Dimension   1536     384          \n",
      "Time (s)    0.828    2.561        \n",
      "Mean value  -0.0008  0.0002       \n",
      "Std dev     0.0255   0.0510       \n",
      "\n",
      "üí° Key Takeaway:\n",
      "   - OpenAI: Higher dimension (1536), typically higher quality\n",
      "   - HuggingFace: Lower dimension (384), faster and free\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"=== Embeddings Comparison ===\")\n",
    "print(f\"\\nTest Query: '{test_text}'\\n\")\n",
    "\n",
    "comparison_data = [\n",
    "    [\"Feature\", \"OpenAI\", \"HuggingFace\"],\n",
    "    [\"Dimension\", len(test_embedding), len(test_embedding_hf)],\n",
    "    [\"Time (s)\", f\"{elapsed:.3f}\", f\"{elapsed_hf:.3f}\"],\n",
    "    [\"Mean value\", f\"{np.mean(test_embedding):.4f}\", f\"{np.mean(test_embedding_hf):.4f}\"],\n",
    "    [\"Std dev\", f\"{np.std(test_embedding):.4f}\", f\"{np.std(test_embedding_hf):.4f}\"]\n",
    "]\n",
    "\n",
    "# Print comparison table\n",
    "col_widths = [max(len(str(row[i])) for row in comparison_data) + 2 for i in range(3)]\n",
    "for i, row in enumerate(comparison_data):\n",
    "    print(\"\".join(str(item).ljust(col_widths[j]) for j, item in enumerate(row)))\n",
    "    if i == 0:\n",
    "        print(\"-\" * sum(col_widths))\n",
    "\n",
    "print(\"\\nüí° Key Takeaway:\")\n",
    "print(\"   - OpenAI: Higher dimension (1536), typically higher quality\")\n",
    "print(\"   - HuggingFace: Lower dimension (384), faster and free\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Vector Store Creation\n",
    "\n",
    "Vector stores enable efficient similarity search over embeddings. We use **FAISS** (Facebook AI Similarity Search), which provides:\n",
    "- Fast similarity search\n",
    "- Efficient memory usage\n",
    "- Support for large-scale datasets\n",
    "\n",
    "We'll create two vector stores to compare both embedding approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Vector Store with OpenAI Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS vector store with OpenAI embeddings...\n",
      "‚úì Vector store created in 0.95s\n",
      "  - 120 documents indexed\n",
      "  - Embedding dimension: 1536\n",
      "‚úì Vector store created in 0.95s\n",
      "  - 120 documents indexed\n",
      "  - Embedding dimension: 1536\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print(\"Creating FAISS vector store with OpenAI embeddings...\")\n",
    "start_time = time.time()\n",
    "\n",
    "vectorstore_openai = FAISS.from_documents(chunks, openai_embeddings)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"‚úì Vector store created in {elapsed:.2f}s\")\n",
    "print(f\"  - {len(chunks)} documents indexed\")\n",
    "print(\"  - Embedding dimension: 1536\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Vector Store with HuggingFace Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FAISS vector store with HuggingFace embeddings...\n",
      "‚úì Vector store created in 2.15s\n",
      "  - 120 documents indexed\n",
      "  - Embedding dimension: 384\n",
      "‚úì Vector store created in 2.15s\n",
      "  - 120 documents indexed\n",
      "  - Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating FAISS vector store with HuggingFace embeddings...\")\n",
    "start_time = time.time()\n",
    "\n",
    "vectorstore_hf = FAISS.from_documents(chunks, hf_embeddings)\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"‚úì Vector store created in {elapsed:.2f}s\")\n",
    "print(f\"  - {len(chunks)} documents indexed\")\n",
    "print(\"  - Embedding dimension: 384\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Vector Store Persistence (Best Practice)\n",
    "\n",
    "**Production Best Practice**: Save vector stores to avoid recomputing embeddings.\n",
    "\n",
    "Benefits:\n",
    "- **Skip re-embedding** on kernel restart (saves time and API costs)\n",
    "- **Share indexes** across team members\n",
    "- **Version control** your vector stores\n",
    "- **Faster development** iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving vector stores...\n",
      "‚úì Saved OpenAI vector store to ./vector_stores/faiss_openai\n",
      "‚úì Saved HuggingFace vector store to ./vector_stores/faiss_hf\n",
      "\n",
      "üìÇ Vector stores saved! To load later, use:\n",
      "\n",
      "# Load OpenAI vector store\n",
      "from langchain_community.vectorstores import FAISS\n",
      "from langchain_openai import OpenAIEmbeddings\n",
      "\n",
      "openai_embeddings = OpenAIEmbeddings()\n",
      "vectorstore_openai = FAISS.load_local(\n",
      "    \"./vector_stores/faiss_openai\",\n",
      "    openai_embeddings,\n",
      "    allow_dangerous_deserialization=True  # Required for pickle files\n",
      ")\n",
      "\n",
      "# Load HuggingFace vector store\n",
      "from langchain_huggingface import HuggingFaceEmbeddings\n",
      "\n",
      "hf_embeddings = HuggingFaceEmbeddings(\n",
      "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
      ")\n",
      "vectorstore_hf = FAISS.load_local(\n",
      "    \"./vector_stores/faiss_hf\",\n",
      "    hf_embeddings,\n",
      "    allow_dangerous_deserialization=True\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Save vector stores for future use\n",
    "import os\n",
    "\n",
    "# Create directory for vector stores\n",
    "os.makedirs(\"./vector_stores\", exist_ok=True)\n",
    "\n",
    "print(\"üíæ Saving vector stores...\")\n",
    "\n",
    "# Save OpenAI vector store\n",
    "vectorstore_openai.save_local(\"./vector_stores/faiss_openai\")\n",
    "print(\"‚úì Saved OpenAI vector store to ./vector_stores/faiss_openai\")\n",
    "\n",
    "# Save HuggingFace vector store  \n",
    "vectorstore_hf.save_local(\"./vector_stores/faiss_hf\")\n",
    "print(\"‚úì Saved HuggingFace vector store to ./vector_stores/faiss_hf\")\n",
    "\n",
    "print(\"\\nüìÇ Vector stores saved! To load later, use:\")\n",
    "print(\"\"\"\n",
    "# Load OpenAI vector store\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "openai_embeddings = OpenAIEmbeddings()\n",
    "vectorstore_openai = FAISS.load_local(\n",
    "    \"./vector_stores/faiss_openai\",\n",
    "    openai_embeddings,\n",
    "    allow_dangerous_deserialization=True  # Required for pickle files\n",
    ")\n",
    "\n",
    "# Load HuggingFace vector store\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "hf_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "vectorstore_hf = FAISS.load_local(\n",
    "    \"./vector_stores/faiss_hf\",\n",
    "    hf_embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Test Similarity Search\n",
    "\n",
    "Let's test similarity search on both vector stores to see how they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'How to build a RAG agent with LangChain?'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- OpenAI Embeddings Results ---\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n",
      "\n",
      "3. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Content: We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\n",
      "You can add a deeper level of control and customization using the LangGraph framework directly‚Äî fo...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- HuggingFace Embeddings Results ---\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n",
      "\n",
      "3. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Content: We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\n",
      "You can add a deeper level of control and customization using the LangGraph framework directly‚Äî fo...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- HuggingFace Embeddings Results ---\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n",
      "\n",
      "3. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Task decomposition can be done by...\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Con...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üí° Notice how both retrievers find relevant documents, though ordering may differ.\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n",
      "\n",
      "3. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Task decomposition can be done by...\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Con...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üí° Notice how both retrievers find relevant documents, though ordering may differ.\n"
     ]
    }
   ],
   "source": [
    "query = \"How to build a RAG agent with LangChain?\"\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test OpenAI vector store\n",
    "print(\"\\n--- OpenAI Embeddings Results ---\")\n",
    "results_openai = vectorstore_openai.similarity_search(query, k=3)\n",
    "for i, doc in enumerate(results_openai, 1):\n",
    "    print(f\"\\n{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   Content: {doc.page_content[:200]}...\")\n",
    "\n",
    "# Test HuggingFace vector store\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n--- HuggingFace Embeddings Results ---\")\n",
    "results_hf = vectorstore_hf.similarity_search(query, k=3)\n",
    "for i, doc in enumerate(results_hf, 1):\n",
    "    print(f\"\\n{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   Content: {doc.page_content[:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüí° Notice how both retrievers find relevant documents, though ordering may differ.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Retrieval Strategies\n",
    "\n",
    "Different retrieval strategies optimize for different goals:\n",
    "\n",
    "### Similarity Search\n",
    "- Returns documents most similar to the query\n",
    "- Simple and fast\n",
    "- May return redundant documents\n",
    "\n",
    "### MMR (Maximal Marginal Relevance)\n",
    "- Balances relevance with diversity\n",
    "- Reduces redundancy in results\n",
    "- Particularly useful when documents contain similar information\n",
    "- Controlled by `lambda_mult` parameter:\n",
    "  - `lambda_mult=1.0`: Pure relevance (like similarity search)\n",
    "  - `lambda_mult=0.0`: Pure diversity\n",
    "  - `lambda_mult=0.5`: Balanced (recommended)\n",
    "\n",
    "Let's compare both strategies using the OpenAI vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Standard Similarity Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Similarity retriever created\n",
      "  - Search type: similarity\n",
      "  - Documents to retrieve: 4\n"
     ]
    }
   ],
   "source": [
    "# Create similarity-based retriever\n",
    "similarity_retriever = vectorstore_openai.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}  # Retrieve top 4 documents\n",
    ")\n",
    "\n",
    "print(\"‚úì Similarity retriever created\")\n",
    "print(\"  - Search type: similarity\")\n",
    "print(\"  - Documents to retrieve: 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 MMR (Maximal Marginal Relevance) Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì MMR retriever created\n",
      "  - Search type: mmr\n",
      "  - Documents to retrieve: 4\n",
      "  - Fetch size: 20\n",
      "  - Lambda (relevance/diversity): 0.5\n"
     ]
    }
   ],
   "source": [
    "# Create MMR-based retriever\n",
    "mmr_retriever = vectorstore_openai.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": 4,              # Number of documents to return\n",
    "        \"fetch_k\": 20,       # Number of documents to fetch before MMR\n",
    "        \"lambda_mult\": 0.5   # Balance between relevance (1.0) and diversity (0.0)\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úì MMR retriever created\")\n",
    "print(\"  - Search type: mmr\")\n",
    "print(\"  - Documents to retrieve: 4\")\n",
    "print(\"  - Fetch size: 20\")\n",
    "print(\"  - Lambda (relevance/diversity): 0.5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Compare Retrieval Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'What are the steps to build a RAG agent with LangChain?'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- Similarity Search Results ---\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n",
      "\n",
      "3. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Content: We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\n",
      "You can add a deeper level of control and customization using the LangGraph framework directly‚Äî fo...\n",
      "\n",
      "4. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\n",
      "You can add a deeper level of control and customization using the LangGraph framework directly‚Äî fo...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- MMR Search Results ---\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n",
      "\n",
      "3. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Content: We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\n",
      "You can add a deeper level of control and customization using the LangGraph framework directly‚Äî fo...\n",
      "\n",
      "4. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\n",
      "You can add a deeper level of control and customization using the LangGraph framework directly‚Äî fo...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "--- MMR Search Results ---\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Content: We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\n",
      "You can add a deeper level of control and customization using the LangGraph framework directly‚Äî fo...\n",
      "\n",
      "3. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: ‚ÄãNext steps\n",
      "Now that we‚Äôve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\n",
      "\n",
      "Stream tokens and other information for responsive user experie...\n",
      "\n",
      "4. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: For more details, see our Installation guide.\n",
      "‚ÄãLangSmith\n",
      "Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üí° Key Observation:\n",
      "   MMR results should show more diversity in sources and content\n",
      "   while still maintaining relevance to the query.\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Content: We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\n",
      "You can add a deeper level of control and customization using the LangGraph framework directly‚Äî fo...\n",
      "\n",
      "3. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: ‚ÄãNext steps\n",
      "Now that we‚Äôve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\n",
      "\n",
      "Stream tokens and other information for responsive user experie...\n",
      "\n",
      "4. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: For more details, see our Installation guide.\n",
      "‚ÄãLangSmith\n",
      "Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üí° Key Observation:\n",
      "   MMR results should show more diversity in sources and content\n",
      "   while still maintaining relevance to the query.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the steps to build a RAG agent with LangChain?\"\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test similarity retriever\n",
    "print(\"\\n--- Similarity Search Results ---\")\n",
    "similarity_docs = similarity_retriever.invoke(query)\n",
    "for i, doc in enumerate(similarity_docs, 1):\n",
    "    print(f\"\\n{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   Content: {doc.page_content[:200]}...\")\n",
    "\n",
    "# Test MMR retriever\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n--- MMR Search Results ---\")\n",
    "mmr_docs = mmr_retriever.invoke(query)\n",
    "for i, doc in enumerate(mmr_docs, 1):\n",
    "    print(f\"\\n{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   Content: {doc.page_content[:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüí° Key Observation:\")\n",
    "print(\"   MMR results should show more diversity in sources and content\")\n",
    "print(\"   while still maintaining relevance to the query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. RAG Chain Construction\n",
    "\n",
    "Now we'll build complete RAG chains that combine:\n",
    "1. **LLM**: Generates answers\n",
    "2. **Retriever**: Finds relevant documents\n",
    "3. **Prompt**: Structures the input\n",
    "4. **Chain**: Orchestrates the flow\n",
    "\n",
    "The chain workflow:\n",
    "```\n",
    "User Query ‚Üí Retriever ‚Üí Retrieved Docs ‚Üí Prompt + LLM ‚Üí Final Answer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Initialize LLM\n",
    "\n",
    "We'll use GPT-4o-mini for cost-effectiveness. For production, consider GPT-5 for higher quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ChatOpenAI LLM initialized\n",
      "  - Model: gpt-4o-mini\n",
      "  - Temperature: 0 (deterministic)\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0  # Deterministic responses\n",
    ")\n",
    "\n",
    "print(\"‚úì ChatOpenAI LLM initialized\")\n",
    "print(\"  - Model: gpt-4o-mini\")\n",
    "print(\"  - Temperature: 0 (deterministic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Create Prompt Template\n",
    "\n",
    "The prompt instructs the LLM on how to use the retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Prompt template created\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful AI assistant. Answer the user's question based on the context provided below.\n",
    "    \n",
    "If the context doesn't contain enough information to answer the question, say so clearly.\n",
    "Always cite which parts of the context you used to formulate your answer.\n",
    "\n",
    "Context:\n",
    "{context}\"\"\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "print(\"‚úì Prompt template created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Build Document Chain and Retrieval Chains\n",
    "\n",
    "We'll create:\n",
    "1. A document chain that combines LLM with prompt\n",
    "2. Retrieval chains for both similarity and MMR strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Similarity retrieval chain created\n",
      "‚úì MMR retrieval chain created\n",
      "\n",
      "‚úì RAG chains ready for inference\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a simple RAG chain using LCEL (LangChain Expression Language)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Build the similarity retrieval chain\n",
    "similarity_retrieval_chain = (\n",
    "    {\"context\": similarity_retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"‚úì Similarity retrieval chain created\")\n",
    "\n",
    "# Build the MMR retrieval chain\n",
    "mmr_retrieval_chain = (\n",
    "    {\"context\": mmr_retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"‚úì MMR retrieval chain created\")\n",
    "\n",
    "print(\"\\n‚úì RAG chains ready for inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. RAG in Action: Comparison & Evaluation\n",
    "\n",
    "Let's test our RAG chains with various queries and compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Test Query with Similarity Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUERY: How to build a RAG agent with LangChain?\n",
      "================================================================================\n",
      "\n",
      "--- SIMILARITY RETRIEVAL CHAIN ---\n",
      "\n",
      "Retrieved Documents (Context):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with L...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with L...\n",
      "\n",
      "3. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Content: We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\n",
      "You can add a deeper level of control and customization using the LangGraph framework directly‚Äî for example, you can add steps to grade document relevance and rewrite search queries. Check out LangG...\n",
      "\n",
      "4. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\n",
      "You can add a deeper level of control and customization using the LangGraph framework directly‚Äî for example, you can add steps to grade document relevance and rewrite search queries. Check out LangG...\n",
      "\n",
      "================================================================================\n",
      "GENERATED ANSWER:\n",
      "================================================================================\n",
      "The context provided does not contain specific step-by-step instructions on how to build a RAG agent with LangChain. It mentions components like indexing, loading documents, splitting documents, storing documents, and retrieval and generation, but does not elaborate on the actual process of building a RAG agent.\n",
      "\n",
      "For detailed instructions, you may need to refer to the LangChain documentation or tutorials directly.\n"
     ]
    }
   ],
   "source": [
    "user_query = \"How to build a RAG agent with LangChain?\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"QUERY: {user_query}\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n--- SIMILARITY RETRIEVAL CHAIN ---\\n\")\n",
    "\n",
    "# First, retrieve the documents using the retriever\n",
    "retrieved_docs = similarity_retriever.invoke(user_query)\n",
    "\n",
    "print(\"Retrieved Documents (Context):\")\n",
    "print(\"-\" * 80)\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"\\n{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   Content: {doc.page_content[:300]}...\")\n",
    "\n",
    "# Now invoke the chain to get the answer\n",
    "response_similarity = similarity_retrieval_chain.invoke(user_query)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATED ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(response_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 Test Same Query with MMR Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUERY: How to build a RAG agent with LangChain?\n",
      "================================================================================\n",
      "\n",
      "--- MMR RETRIEVAL CHAIN ---\n",
      "\n",
      "Retrieved Documents (Context):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with L...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/modules/model_io/llms/\n",
      "   Content: Advanced streaming topics\"Auto-streaming\" chat modelsLangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when you‚Äôre not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming invoke method but ...\n",
      "\n",
      "3. Source: https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
      "   Content: LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorpo...\n",
      "\n",
      "4. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
      "Generate: A model produces an answer using a prompt that includes both the question with the retrieved data\n",
      "\n",
      "\n",
      "Now let‚Äôs write the actual application logic. We want to create a simple application that takes a ...\n",
      "\n",
      "================================================================================\n",
      "GENERATED ANSWER:\n",
      "================================================================================\n",
      "To build a RAG (Retrieval-Augmented Generation) agent with LangChain, you can follow these general steps based on the provided context:\n",
      "\n",
      "1. **Setup and Installation**: First, ensure you have Python 3.10+ installed and then install LangChain using the command:\n",
      "   ```bash\n",
      "   pip install -U langchain\n",
      "   ```\n",
      "\n",
      "2. **Indexing Documents**: Load your documents, split them into manageable parts, and store them in a vector store for retrieval.\n",
      "\n",
      "3. **Retrieval and Generation**:\n",
      "   - Implement a retrieval mechanism that takes user input and retrieves relevant document splits from storage using a Retriever.\n",
      "   - Use a model to generate an answer by passing the retrieved documents along with the user's question.\n",
      "\n",
      "4. **Building the RAG Agent**: You can create a simple RAG agent by implementing a tool that wraps your vector store. This agent will execute searches based on user queries.\n",
      "\n",
      "5. **Application Logic**: Write the application logic to handle user questions, perform searches, and return answers. You can create a two-step RAG chain that uses a single LLM call per query for efficiency.\n",
      "\n",
      "The context mentions that LangChain simplifies the process of building agents and applications powered by LLMs, allowing you to get started quickly with minimal code.\n",
      "\n",
      "For more detailed instructions, you may want to refer to the specific tutorials and documentation provided by LangChain.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(f\"QUERY: {user_query}\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n--- MMR RETRIEVAL CHAIN ---\\n\")\n",
    "\n",
    "# First, retrieve the documents using the MMR retriever\n",
    "retrieved_docs_mmr = mmr_retriever.invoke(user_query)\n",
    "\n",
    "print(\"Retrieved Documents (Context):\")\n",
    "print(\"-\" * 80)\n",
    "for i, doc in enumerate(retrieved_docs_mmr, 1):\n",
    "    print(f\"\\n{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   Content: {doc.page_content[:300]}...\")\n",
    "\n",
    "# Now invoke the chain to get the answer\n",
    "response_mmr = mmr_retrieval_chain.invoke(user_query)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATED ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(response_mmr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Compare OpenAI vs HuggingFace Embeddings\n",
    "\n",
    "Let's create a retrieval chain using the HuggingFace vector store and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUERY: How to build a RAG agent with LangChain?\n",
      "================================================================================\n",
      "\n",
      "--- HUGGINGFACE EMBEDDINGS CHAIN ---\n",
      "\n",
      "Retrieved Documents (Context):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with L...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...‚åòKGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with L...\n",
      "\n",
      "3. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Content: Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Task decomposition can be done by...\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Component One: Planning...\n",
      "================================== Ai Message =====================...\n",
      "\n",
      "4. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Content: Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Task decomposition can be done by...\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Component One: Planning...\n",
      "================================== Ai Message =====================...\n",
      "\n",
      "================================================================================\n",
      "GENERATED ANSWER:\n",
      "================================================================================\n",
      "To build a RAG (Retrieval-Augmented Generation) agent with LangChain, you can follow these general steps based on the context provided:\n",
      "\n",
      "1. **Setup and Installation**: You need to install the necessary LangChain dependencies. You can do this using pip with the following command:\n",
      "   ```\n",
      "   pip install langchain langchain-text-splitters langchain-community bs4\n",
      "   ```\n",
      "\n",
      "2. **Indexing**: This involves loading documents, splitting them, and storing them for retrieval. The specific steps for indexing are not detailed in the context, but typically, you would load your documents into the system, split them into manageable pieces, and store them in a way that allows for efficient retrieval.\n",
      "\n",
      "3. **Retrieval and Generation**: After indexing, you would set up the retrieval mechanism and the generation process. This is where the RAG agent retrieves relevant information from the indexed documents and generates responses based on that information.\n",
      "\n",
      "4. **Components**: The context mentions components like planning and task decomposition, which are likely part of the overall architecture of the RAG agent.\n",
      "\n",
      "5. **Next Steps**: After setting up the basic components, you would typically look into tutorials and additional resources provided by LangChain to refine and enhance your RAG agent.\n",
      "\n",
      "For more detailed instructions, you may want to refer to the LangChain documentation or specific tutorials related to building RAG agents.\n",
      "\n",
      "(Note: The specific details on each step are not fully provided in the context, so you may need to consult additional resources for comprehensive guidance.)\n"
     ]
    }
   ],
   "source": [
    "# Create retriever from HuggingFace vector store\n",
    "hf_retriever = vectorstore_hf.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "\n",
    "# Create a helper function for formatting docs (if not already defined)\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create retrieval chain with HuggingFace embeddings using LCEL\n",
    "hf_retrieval_chain = (\n",
    "    {\"context\": hf_retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"QUERY: {user_query}\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n--- HUGGINGFACE EMBEDDINGS CHAIN ---\\n\")\n",
    "\n",
    "# First, retrieve the documents using the HF retriever\n",
    "retrieved_docs_hf = hf_retriever.invoke(user_query)\n",
    "\n",
    "print(\"Retrieved Documents (Context):\")\n",
    "print(\"-\" * 80)\n",
    "for i, doc in enumerate(retrieved_docs_hf, 1):\n",
    "    print(f\"\\n{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   Content: {doc.page_content[:300]}...\")\n",
    "\n",
    "# Now invoke the chain to get the answer\n",
    "response_hf = hf_retrieval_chain.invoke(user_query)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATED ANSWER:\")\n",
    "print(\"=\" * 80)\n",
    "print(response_hf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Summary Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPARISON SUMMARY\n",
      "================================================================================\n",
      "\n",
      "1. OpenAI Embeddings + Similarity Search\n",
      "   Answer length: 417 chars\n",
      "   Documents retrieved: 4\n",
      "\n",
      "2. OpenAI Embeddings + MMR Search\n",
      "   Answer length: 1380 chars\n",
      "   Documents retrieved: 4\n",
      "\n",
      "3. HuggingFace Embeddings + Similarity Search\n",
      "   Answer length: 1585 chars\n",
      "   Documents retrieved: 4\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üí° Key Insights:\n",
      "   - All approaches provide relevant answers\n",
      "   - MMR may provide more diverse context\n",
      "   - HuggingFace embeddings are competitive and free\n",
      "   - Choice depends on: budget, privacy needs, and quality requirements\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"COMPARISON SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. OpenAI Embeddings + Similarity Search\")\n",
    "print(f\"   Answer length: {len(response_similarity)} chars\")\n",
    "print(f\"   Documents retrieved: {len(retrieved_docs)}\")\n",
    "\n",
    "print(\"\\n2. OpenAI Embeddings + MMR Search\")\n",
    "print(f\"   Answer length: {len(response_mmr)} chars\")\n",
    "print(f\"   Documents retrieved: {len(retrieved_docs_mmr)}\")\n",
    "\n",
    "print(\"\\n3. HuggingFace Embeddings + Similarity Search\")\n",
    "print(f\"   Answer length: {len(response_hf)} chars\")\n",
    "print(f\"   Documents retrieved: {len(retrieved_docs_hf)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"   - All approaches provide relevant answers\")\n",
    "print(\"   - MMR may provide more diverse context\")\n",
    "print(\"   - HuggingFace embeddings are competitive and free\")\n",
    "print(\"   - Choice depends on: budget, privacy needs, and quality requirements\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Advanced Features\n",
    "\n",
    "### 9.1 Custom Metadata Filtering\n",
    "\n",
    "We added custom metadata earlier. Now let's use it to filter results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample metadata from our documents:\n",
      "\n",
      "Metadata: {'source': 'https://python.langchain.com/docs/use_cases/question_answering/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en', 'source_type': 'web_documentation', 'process_date': '2025-11-12', 'domain': 'langchain'}\n",
      "\n",
      "‚úì Filtered retriever created\n",
      "  - Filter: source_type = 'web_documentation'\n",
      "\n",
      "Query: 'What is a retriever in LangChain?'\n",
      "Retrieved 4 documents with metadata filter\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
      "   Source Type: web_documentation\n",
      "   Process Date: 2025-11-12\n",
      "   Content: LangChain overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/modules/model_io/llms/\n",
      "   Source Type: web_documentation\n",
      "   Process Date: 2025-11-12\n",
      "   Content: Models - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain h...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Filter by source domain\n",
    "print(\"Sample metadata from our documents:\")\n",
    "sample_doc = chunks[0]\n",
    "print(f\"\\nMetadata: {sample_doc.metadata}\")\n",
    "\n",
    "# Create a retriever with metadata filter\n",
    "filtered_retriever = vectorstore_openai.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 4,\n",
    "        \"filter\": {\"source_type\": \"web_documentation\"}  # Filter by our custom metadata\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\n‚úì Filtered retriever created\")\n",
    "print(\"  - Filter: source_type = 'web_documentation'\")\n",
    "\n",
    "# Test filtered retrieval\n",
    "query = \"What is a retriever in LangChain?\"\n",
    "filtered_docs = filtered_retriever.invoke(query)\n",
    "\n",
    "print(f\"\\nQuery: '{query}'\")\n",
    "print(f\"Retrieved {len(filtered_docs)} documents with metadata filter\\n\")\n",
    "\n",
    "for i, doc in enumerate(filtered_docs[:2], 1):\n",
    "    print(f\"{i}. Source: {doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"   Source Type: {doc.metadata.get('source_type', 'N/A')}\")\n",
    "    print(f\"   Process Date: {doc.metadata.get('process_date', 'N/A')}\")\n",
    "    print(f\"   Content: {doc.page_content[:150]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Source Attribution\n",
    "\n",
    "Show which sources were used to generate the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Production Optimizations\n",
    "\n",
    "**Batch Processing for Better Performance:**\n",
    "\n",
    "When processing many documents, use batch embedding operations instead of individual calls:\n",
    "\n",
    "```python\n",
    "# ‚ùå Inefficient: Individual embedding calls\n",
    "for doc in docs:\n",
    "    embedding = embeddings.embed_query(doc.page_content)\n",
    "    \n",
    "# ‚úì Efficient: Batch embedding\n",
    "texts = [doc.page_content for doc in docs]\n",
    "embeddings_batch = embeddings.embed_documents(texts)  # Much faster!\n",
    "```\n",
    "\n",
    "**Performance Gains:**\n",
    "- OpenAI API: Batch requests reduce latency overhead\n",
    "- HuggingFace: GPU/CPU vectorization works better on batches\n",
    "- Typical speedup: 2-5x faster for large document sets\n",
    "\n",
    "**Memory Management:**\n",
    "\n",
    "For large datasets (>10,000 documents):\n",
    "- Use quantized FAISS indexes: `IndexIVFFlat` or `IndexIVFPQ`\n",
    "- Consider production vector databases: Pinecone, Weaviate, Chroma\n",
    "- Stream document processing instead of loading all at once\n",
    "\n",
    "**Cost Optimization:**\n",
    "\n",
    "```python\n",
    "# Track OpenAI embedding costs\n",
    "import tiktoken\n",
    "\n",
    "def estimate_embedding_cost(texts, model=\"text-embedding-3-small\"):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")  # Approximation\n",
    "    total_tokens = sum(len(encoding.encode(text)) for text in texts)\n",
    "    cost_per_million = 0.02  # $0.02 per 1M tokens for text-embedding-3-small\n",
    "    estimated_cost = (total_tokens / 1_000_000) * cost_per_million\n",
    "    return total_tokens, estimated_cost\n",
    "\n",
    "tokens, cost = estimate_embedding_cost([doc.page_content for doc in chunks])\n",
    "print(f\"Estimated tokens: {tokens:,}\")\n",
    "print(f\"Estimated cost: ${cost:.4f}\")\n",
    "```\n",
    "\n",
    "**Async Operations for Scale:**\n",
    "\n",
    "```python\n",
    "# For production systems handling many concurrent requests\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Use async methods\n",
    "async def embed_documents_async(texts):\n",
    "    return await embeddings.aembed_documents(texts)\n",
    "\n",
    "# Enables concurrent processing of multiple requests\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the key components of a RAG system?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ANSWER:\n",
      "--------------------------------------------------------------------------------\n",
      "The key components of a RAG (Retrieval-Augmented Generation) system, as outlined in the context, include:\n",
      "\n",
      "1. **Indexing**: This is a pipeline for ingesting data from a source and indexing it, which usually occurs in a separate process.\n",
      "\n",
      "2. **Retrieval and Generation**: This is the actual RAG process that takes the user query at runtime, retrieves the relevant data from the index, and then passes that data to the model for generation.\n",
      "\n",
      "These components work together to enable the RAG system to effectively respond to user queries by combining retrieval of relevant information with generative capabilities.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "SOURCES USED:\n",
      "--------------------------------------------------------------------------------\n",
      "1. https://python.langchain.com/docs/use_cases/chatbots/\n",
      "2. https://python.langchain.com/docs/use_cases/question_answering/\n",
      "\n",
      "üí° Always cite sources to build trust and enable verification!\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the key components of a RAG system?\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get the documents first\n",
    "docs_for_attribution = similarity_retriever.invoke(query)\n",
    "\n",
    "# Get the answer\n",
    "response = similarity_retrieval_chain.invoke(query)\n",
    "\n",
    "print(\"\\nANSWER:\")\n",
    "print(\"-\" * 80)\n",
    "print(response)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nSOURCES USED:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Extract unique sources from the retrieved documents\n",
    "sources = set()\n",
    "for doc in docs_for_attribution:\n",
    "    source = doc.metadata.get('source', 'Unknown')\n",
    "    sources.add(source)\n",
    "\n",
    "for i, source in enumerate(sorted(sources), 1):\n",
    "    print(f\"{i}. {source}\")\n",
    "\n",
    "print(\"\\nüí° Always cite sources to build trust and enable verification!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 10. Best Practices\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Chunk Size Selection**\n",
    "   - Smaller chunks (300-500): Better for precise information retrieval\n",
    "   - Larger chunks (800-1200): Better for context-heavy questions\n",
    "   - Always use overlap (100-200 chars) to preserve context\n",
    "\n",
    "2. **Embedding Selection**\n",
    "   - **OpenAI**: Best quality, suitable for production, requires API key\n",
    "   - **HuggingFace**: Free, private, good for development and privacy-sensitive data\n",
    "   - Test both with your specific use case\n",
    "\n",
    "3. **Retrieval Strategy**\n",
    "   - **Similarity**: Use for most cases, simple and effective\n",
    "   - **MMR**: Use when you need diverse results and want to avoid redundancy\n",
    "   - Experiment with `k` (number of documents) - typically 3-5 is good\n",
    "\n",
    "4. **Prompt Engineering**\n",
    "   - Always instruct the model to say when it doesn't know\n",
    "   - Request source citations for transparency\n",
    "   - Be specific about the expected format\n",
    "\n",
    "5. **Metadata Management**\n",
    "   - Add custom metadata for filtering and attribution\n",
    "   - Include source URLs, dates, document types\n",
    "   - Use metadata for access control in production\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}