# ============================================================================
# LANGCHAIN RAG - CONFIGURATION TEMPLATE
# ============================================================================
# INSTRUCTIONS:
# 1. Copy this file to .env: cp .env.example .env
# 2. Replace placeholder values with your actual configuration
# 3. NEVER commit the .env file to git (it's in .gitignore)

# ============================================================================
# REQUIRED API KEYS
# ============================================================================

# OpenAI API Key (REQUIRED)
# Get your API key from: https://platform.openai.com/api-keys
# Sign up: https://platform.openai.com/signup
# Add billing: https://platform.openai.com/account/billing
OPENAI_API_KEY=your-openai-api-key-here

# ============================================================================
# OPTIONAL API KEYS
# ============================================================================

# HuggingFace API Key (NOT NEEDED for local embeddings)
# Local embeddings use sentence-transformers models that run offline
# Only needed if using HuggingFace Inference API or gated models
# Get it from: https://huggingface.co/settings/tokens
# HUGGINGFACE_API_KEY=your-huggingface-api-key-here

# Tavily API Key (Optional - for premium web search in CRAG)
# Tavily provides high-quality web search results for RAG applications
# Get it from: https://tavily.com/
# Free tier: 1000 searches/month
# TAVILY_API_KEY=your-tavily-api-key-here

# LangSmith API Key (Optional - for LangSmith tracing and monitoring)
# Get it from: https://smith.langchain.com/
# Enable for debugging and production monitoring
# Useful for tracking RAG chain performance and debugging
# LANGSMITH_API_KEY=your-langsmith-api-key-here

# ============================================================================
# ENVIRONMENT & DEBUG SETTINGS
# ============================================================================

# Environment type: dev, test, prod
# Affects logging verbosity and error handling
# Default: dev
ENVIRONMENT=dev

# Enable debug mode (verbose logging)
# Set to true for detailed logs, false for production
# Default: false
DEBUG_MODE=false

# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
# Recommended: INFO for dev, WARNING for prod
# Default: INFO
LOG_LEVEL=INFO

# ============================================================================
# LLM MODEL CONFIGURATION
# ============================================================================

# OpenAI model to use for generation
# Options: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
# Recommended: gpt-4o-mini (fast + cost-effective)
# Default: gpt-4o-mini
DEFAULT_MODEL=gpt-4o-mini

# Temperature for LLM responses (0.0 = deterministic, 1.0 = creative)
# Recommended: 0 for RAG (factual answers), 0.7+ for creative tasks
# Default: 0
DEFAULT_TEMPERATURE=0

# Embeddings model for OpenAI
# Options:
#   - text-embedding-3-small (1536 dimensions, faster, cheaper)
#   - text-embedding-3-large (3072 dimensions, more accurate, slower)
# Default: text-embedding-3-small
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# ============================================================================
# RETRIEVAL CONFIGURATION
# ============================================================================

# Number of documents to retrieve for RAG context
# Higher = more context but slower and more expensive
# Recommended: 3-5 for most use cases
# Default: 4
DEFAULT_K=4

# Number of documents to fetch before MMR (Maximal Marginal Relevance) filtering
# MMR reduces redundancy by selecting diverse documents
# Should be higher than DEFAULT_K (typically 4-5x)
# Default: 20
DEFAULT_MMR_FETCH_K=20

# MMR Lambda: balance between relevance (1.0) and diversity (0.0)
# 1.0 = pure relevance (may be redundant)
# 0.0 = pure diversity (may lose relevance)
# Recommended: 0.5 for balanced, 0.7-0.8 for relevance-heavy
# Default: 0.5
DEFAULT_MMR_LAMBDA=0.5

# ============================================================================
# TEXT PROCESSING CONFIGURATION
# ============================================================================

# Chunk size for text splitting (characters)
# Larger chunks = more context but less granular retrieval
# Smaller chunks = more precise but may lose context
# Recommended: 500-1500 depending on document type
# Default: 1000
DEFAULT_CHUNK_SIZE=1000

# Overlap between chunks (characters)
# Prevents context loss at chunk boundaries
# Typically 10-20% of chunk_size
# Default: 200
DEFAULT_CHUNK_OVERLAP=200

# ============================================================================
# HUGGINGFACE CONFIGURATION
# ============================================================================

# HuggingFace embedding model (runs locally, no API key needed)
# This model is downloaded and cached on first use (~90MB)
# Popular options:
#   - sentence-transformers/all-MiniLM-L6-v2 (lightweight, fast)
#   - sentence-transformers/all-mpnet-base-v2 (more accurate, slower)
# Default: sentence-transformers/all-MiniLM-L6-v2
HF_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2

# ============================================================================
# DISPLAY & OUTPUT
# ============================================================================

# Section width for console output (characters)
# Used for formatting headers and separators in notebook output
# Default: 80
SECTION_WIDTH=80

# Characters to show in document previews
# Used when displaying retrieved document snippets
# Default: 300
PREVIEW_LENGTH=300

# ============================================================================
# ADVANCED / INTERNAL
# ============================================================================

# Suppress HuggingFace tokenizers parallelism warning
# Set to false to avoid warning messages in Jupyter
# Default: false
TOKENIZERS_PARALLELISM=false

# User Agent for HTTP requests (OpenAI, web scraping, etc.)
# Identifies your application in API requests
# Format: AppName/Version (Contact)
# Default: LangChain-RAG-Tutorial/1.0
USER_AGENT=LangChain-RAG-Tutorial/1.0

# Enable LangSmith tracing (requires LANGSMITH_API_KEY above)
# Set to true to enable detailed tracing of LangChain operations
# Useful for debugging and performance monitoring
# Default: false (commented out)
# LANGSMITH_TRACING=true

# ============================================================================
# NOTES
# ============================================================================
# - Do NOT use quotes around values (KEY=value, not KEY="value")
# - Boolean values: true/false (lowercase)
# - Numbers: no quotes needed
# - Strings: no quotes needed (unless they contain special characters)
# - Comments: Use # at line start
# - Commented variables (starting with #) use default values from shared/config.py
# - After modifying this file, restart your Jupyter kernel to apply changes
# - SECURITY: Never commit .env with real API keys to version control
