{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Embeddings Comparison\n",
    "\n",
    "This notebook compares three embedding approaches for RAG:\n",
    "\n",
    "1. **OpenAI Embeddings** - High quality, API-based\n",
    "2. **HuggingFace Embeddings (Legacy)** - sentence-transformers/all-MiniLM-L6-v2\n",
    "3. **HuggingFace BGE** - BAAI/bge-small-en-v1.5 (upgraded model)\n",
    "\n",
    "We'll create FAISS vector stores and compare performance and quality.\n",
    "\n",
    "**Prerequisites:**\n",
    "- 01_setup_and_basics.ipynb completed\n",
    "- OpenAI API key configured\n",
    "\n",
    "**Duration:** ~10 minutes\n",
    "\n",
    "**Outputs:**\n",
    "- Vector stores saved to `data/vector_stores/`\n",
    "- Performance comparison metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Load necessary modules and prepare documents/chunks from previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING DOCUMENTS AND CHUNKS\n",
      "================================================================================\n",
      "\n",
      "Loading 4 documents from web...\n",
      "  - https://python.langchain.com/docs/use_cases/question_answering/\n",
      "  - https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
      "  - https://python.langchain.com/docs/modules/model_io/llms/\n",
      "  - https://python.langchain.com/docs/use_cases/chatbots/\n",
      "âœ“ Loaded 4 documents\n",
      "âœ“ Added custom metadata to all documents\n",
      "Splitting documents...\n",
      "  - Chunk size: 1000\n",
      "  - Chunk overlap: 200\n",
      "âœ“ Created 120 chunks\n",
      "\n",
      "  Sample chunk:\n",
      "    - Length: 839 chars\n",
      "    - Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "    - Preview: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Rea...\n",
      "\n",
      "âœ… Ready with 4 documents and 120 chunks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from shared.config import OPENAI_VECTOR_STORE_PATH, HF_VECTOR_STORE_PATH\n",
    "from shared.loaders import load_and_split\n",
    "from shared.utils import print_section_header, save_vector_store\n",
    "import time\n",
    "\n",
    "print_section_header(\"Loading Documents and Chunks\")\n",
    "\n",
    "# Load and split documents\n",
    "docs, chunks = load_and_split(verbose=True)\n",
    "\n",
    "print(f\"\\nâœ… Ready with {len(docs)} documents and {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OpenAI Embeddings\n",
    "\n",
    "### Features\n",
    "- Model: `text-embedding-3-small`\n",
    "- Dimensions: 1536\n",
    "- Cost: ~$0.02 per 1M tokens\n",
    "- Quality: Excellent\n",
    "\n",
    "### Use When\n",
    "- Production quality required\n",
    "- Budget available\n",
    "- Internet connection reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPENAI EMBEDDINGS\n",
      "================================================================================\n",
      "\n",
      "Initializing OpenAI embeddings...\n",
      "âœ“ OpenAI embeddings initialized\n",
      "\n",
      "Testing with query: 'What is retrieval-augmented generation?'\n",
      "\n",
      "Results:\n",
      "  Dimension: 1536\n",
      "  Time: 0.241s\n",
      "  First 5 values: ['-0.0403', '-0.0036', '0.0001', '0.0009', '-0.0103']\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "print_section_header(\"OpenAI Embeddings\")\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "print(\"Initializing OpenAI embeddings...\")\n",
    "openai_embeddings = OpenAIEmbeddings()\n",
    "print(\"âœ“ OpenAI embeddings initialized\")\n",
    "\n",
    "# Test with a sample query\n",
    "test_query = \"What is retrieval-augmented generation?\"\n",
    "print(f\"\\nTesting with query: '{test_query}'\")\n",
    "\n",
    "start_time = time.time()\n",
    "test_embedding = openai_embeddings.embed_query(test_query)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(\"\\nResults:\")\n",
    "print(f\"  Dimension: {len(test_embedding)}\")\n",
    "print(f\"  Time: {elapsed:.3f}s\")\n",
    "print(f\"  First 5 values: {[f'{v:.4f}' for v in test_embedding[:5]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HuggingFace Embeddings\n",
    "\n",
    "### Features\n",
    "- Model: `sentence-transformers/all-MiniLM-L6-v2`\n",
    "- Dimensions: 384\n",
    "- Cost: Free (runs locally)\n",
    "- Quality: Very good\n",
    "\n",
    "### Use When\n",
    "- Privacy is critical\n",
    "- Offline operation needed\n",
    "- Cost is a constraint\n",
    "- Development/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HUGGINGFACE EMBEDDINGS\n",
      "================================================================================\n",
      "\n",
      "Initializing HuggingFace embeddings...\n",
      "(First run downloads model ~90MB - may take 1-2 minutes)\n",
      "Cache location: /Users/gianlucamazza/.cache/huggingface/\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "print_section_header(\"HuggingFace Embeddings\")\n",
    "\n",
    "print(\"Initializing HuggingFace embeddings...\")\n",
    "print(\"(First run downloads model ~90MB - may take 1-2 minutes)\")\n",
    "print(f\"Cache location: {os.path.expanduser('~/.cache/huggingface/')}\\n\")\n",
    "\n",
    "try:\n",
    "    hf_embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    print(\"âœ“ HuggingFace embeddings initialized\")\n",
    "    \n",
    "    # Test with same query\n",
    "    print(f\"\\nTesting with query: '{test_query}'\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_embedding_hf = hf_embeddings.embed_query(test_query)\n",
    "    elapsed_hf = time.time() - start_time\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"  Dimension: {len(test_embedding_hf)}\")\n",
    "    print(f\"  Time: {elapsed_hf:.3f}s\")\n",
    "    print(f\"  First 5 values: {[f'{v:.4f}' for v in test_embedding_hf[:5]]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. Check internet connection (first run only)\")\n",
    "    print(\"  2. Verify disk space (~200MB needed)\")\n",
    "    print(\"  3. Try: pip install --upgrade sentence-transformers\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. HuggingFace BGE (Upgraded)\n",
    "\n",
    "### Features\n",
    "- Model: `BAAI/bge-small-en-v1.5` (BGE = BAAI General Embedding)\n",
    "- Dimensions: 384 (same as all-MiniLM)\n",
    "- Cost: Free (runs locally)\n",
    "- Quality: **Superior** - competitive with OpenAI\n",
    "\n",
    "### Improvements over all-MiniLM-L6-v2\n",
    "- Better semantic understanding\n",
    "- Optimized specifically for retrieval tasks\n",
    "- Top performance on MTEB benchmark\n",
    "- Same dimensions = drop-in replacement\n",
    "\n",
    "### Use When\n",
    "- Same as all-MiniLM, but with better quality\n",
    "- **Recommended** for new projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HUGGINGFACE BGE EMBEDDINGS\n",
      "================================================================================\n",
      "\n",
      "Initializing BGE embeddings...\n",
      "(First run downloads model ~130MB - may take 1-2 minutes)\n",
      "Cache location: /Users/gianlucamazza/.cache/huggingface/\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a19ff1008894cf08dd9743c7097b636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73a29d20610a46e39178ab36ec30a5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6ea8d0bfde4dff867a54ce694cb451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2c7969933b418185d688a0d49256b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63f53443a5f49918464e6b46b11d403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e408e24a7a4a36ab5da203368dc98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3133891725744f21a93be918af09cdad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2a1eff7fd534c94ba0cfedc66ad8aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b3ea1d9eece449092b47bd7155778de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1d76332d3d4f929a3b9f493a280f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf7d135b3594bd9a4fb69afe5be11c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ BGE embeddings initialized\n",
      "\n",
      "Testing with query: 'What is retrieval-augmented generation?'\n",
      "\n",
      "Results:\n",
      "  Dimension: 384\n",
      "  Time: 0.024s\n",
      "  First 5 values: ['-0.0331', '0.0233', '-0.0349', '-0.0267', '0.0155']\n",
      "\n",
      "ðŸ’¡ BGE is now the default HuggingFace model in config.py!\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"HuggingFace BGE Embeddings\")\n",
    "\n",
    "print(\"Initializing BGE embeddings...\")\n",
    "print(\"(First run downloads model ~130MB - may take 1-2 minutes)\")\n",
    "print(f\"Cache location: {os.path.expanduser('~/.cache/huggingface/')}\\n\")\n",
    "\n",
    "try:\n",
    "    bge_embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"BAAI/bge-small-en-v1.5\"\n",
    "    )\n",
    "    print(\"âœ“ BGE embeddings initialized\")\n",
    "    \n",
    "    # Test with same query\n",
    "    print(f\"\\nTesting with query: '{test_query}'\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_embedding_bge = bge_embeddings.embed_query(test_query)\n",
    "    elapsed_bge = time.time() - start_time\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"  Dimension: {len(test_embedding_bge)}\")\n",
    "    print(f\"  Time: {elapsed_bge:.3f}s\")\n",
    "    print(f\"  First 5 values: {[f'{v:.4f}' for v in test_embedding_bge[:5]]}\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ BGE is now the default HuggingFace model in config.py!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. Check internet connection (first run only)\")\n",
    "    print(\"  2. Verify disk space (~300MB needed)\")\n",
    "    print(\"  3. Try: pip install --upgrade sentence-transformers\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Side-by-Side Comparison\n\nNow let's compare all three embedding models to see their differences in quality, speed, and characteristics.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "from shared.utils import print_comparison_table\nimport numpy as np\n\nprint_section_header(\"Embeddings Comparison\")\n\ndata = [\n    [\"Feature\", \"OpenAI\", \"HF (Legacy)\", \"BGE (New)\"],\n    [\"Model\", \"text-emb-3-small\", \"all-MiniLM-L6-v2\", \"bge-small-en-v1.5\"],\n    [\"Dimension\", len(test_embedding), len(test_embedding_hf), len(test_embedding_bge)],\n    [\"Time (s)\", f\"{elapsed:.3f}\", f\"{elapsed_hf:.3f}\", f\"{elapsed_bge:.3f}\"],\n    [\"Mean\", f\"{np.mean(test_embedding):.4f}\", f\"{np.mean(test_embedding_hf):.4f}\", f\"{np.mean(test_embedding_bge):.4f}\"],\n    [\"Std Dev\", f\"{np.std(test_embedding):.4f}\", f\"{np.std(test_embedding_hf):.4f}\", f\"{np.std(test_embedding_bge):.4f}\"],\n    [\"Cost\", \"Paid\", \"Free\", \"Free\"],\n    [\"Privacy\", \"Cloud\", \"Local\", \"Local\"],\n    [\"Quality\", \"Excellent\", \"Very Good\", \"Excellent\"]\n]\n\nprint_comparison_table(data)\n\nprint(\"\\nðŸ’¡ Key Takeaways:\")\nprint(\"   - OpenAI: Highest dimension (1536), cloud-based, paid\")\nprint(\"   - HF Legacy: Good quality, local, free\")\nprint(\"   - BGE: BEST VALUE - OpenAI-level quality, local, free!\")\nprint(\"   - BGE is now the default in this tutorial\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Create Vector Stores\n\nNow we'll create FAISS vector stores for OpenAI and BGE embeddings. These will be saved for reuse in all subsequent notebooks.\n\n**Note:** We're using BGE (not all-MiniLM-L6-v2) as the default HuggingFace model going forward."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_community.vectorstores import FAISS\n\nprint_section_header(\"Creating Vector Stores\")\n\n# Create OpenAI vector store\nprint(\"Creating FAISS vector store with OpenAI embeddings...\")\nstart_time = time.time()\nvectorstore_openai = FAISS.from_documents(chunks, openai_embeddings)\nelapsed_openai = time.time() - start_time\n\nprint(f\"âœ“ OpenAI vector store created in {elapsed_openai:.2f}s\")\nprint(f\"  - {len(chunks)} documents indexed\")\nprint(\"  - Embedding dimension: 1536\")\n\n# Create BGE vector store (now the default HuggingFace model)\nprint(\"\\nCreating FAISS vector store with BGE embeddings...\")\nstart_time = time.time()\nvectorstore_hf = FAISS.from_documents(chunks, bge_embeddings)\nelapsed_bge_vs = time.time() - start_time\n\nprint(f\"âœ“ BGE vector store created in {elapsed_bge_vs:.2f}s\")\nprint(f\"  - {len(chunks)} documents indexed\")\nprint(\"  - Embedding dimension: 384\")\nprint(\"  - Model: BAAI/bge-small-en-v1.5\")\n\nprint(\"\\nðŸ“Š Performance:\")\nprint(f\"   OpenAI: {elapsed_openai:.2f}s\")\nprint(f\"   BGE: {elapsed_bge_vs:.2f}s\")\nprint(f\"   BGE is {elapsed_openai/elapsed_bge_vs:.1f}x faster!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Vector Stores\n",
    "\n",
    "**IMPORTANT:** We save the vector stores to disk to avoid re-embedding in every notebook. This:\n",
    "- Saves time (~3-4 seconds per notebook)\n",
    "- Reduces API costs (OpenAI charges per embedding)\n",
    "- Ensures consistency across notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING VECTOR STORES\n",
      "================================================================================\n",
      "\n",
      "âœ“ Saved vector store to /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/fundamentals/../../data/vector_stores/openai_embeddings\n",
      "âœ“ Saved vector store to /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/fundamentals/../../data/vector_stores/huggingface_embeddings\n",
      "\n",
      "âœ… Vector stores saved successfully!\n",
      "\n",
      "ðŸ“‚ Locations:\n",
      "   OpenAI: /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/fundamentals/../../data/vector_stores/openai_embeddings\n",
      "   HuggingFace: /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/fundamentals/../../data/vector_stores/huggingface_embeddings\n",
      "\n",
      "ðŸ’¡ These will be loaded in subsequent notebooks to avoid re-embedding.\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Saving Vector Stores\")\n",
    "\n",
    "# Save OpenAI vector store\n",
    "save_vector_store(vectorstore_openai, OPENAI_VECTOR_STORE_PATH, verbose=True)\n",
    "\n",
    "# Save HuggingFace vector store\n",
    "save_vector_store(vectorstore_hf, HF_VECTOR_STORE_PATH, verbose=True)\n",
    "\n",
    "print(\"\\nâœ… Vector stores saved successfully!\")\n",
    "print(\"\\nðŸ“‚ Locations:\")\n",
    "print(f\"   OpenAI: {OPENAI_VECTOR_STORE_PATH}\")\n",
    "print(f\"   HuggingFace: {HF_VECTOR_STORE_PATH}\")\n",
    "print(\"\\nðŸ’¡ These will be loaded in subsequent notebooks to avoid re-embedding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 7. Test Similarity Search\n\nQuick test to verify the vector stores work correctly and compare retrieval quality.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from shared.utils import print_results\n\nprint_section_header(\"Testing Similarity Search\")\n\nquery = \"How to build a RAG agent with LangChain?\"\nprint(f\"Query: '{query}'\\n\")\n\n# Test OpenAI\nprint(\"=\" * 80)\nprint(\"=== OpenAI Embeddings ===\")\nprint(\"=\" * 80)\nresults_openai = vectorstore_openai.similarity_search(query, k=3)\nprint_results(results_openai, max_docs=3, preview_length=150)\n\n# Test BGE\nprint(\"\\n\" + \"=\" * 80)\nprint(\"=== BGE Embeddings (BAAI/bge-small-en-v1.5) ===\")\nprint(\"=\" * 80)\nresults_bge = vectorstore_hf.similarity_search(query, k=3)\nprint_results(results_bge, max_docs=3, preview_length=150)\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"\\nâœ… Both vector stores working correctly!\")\nprint(\"\\nðŸ’¡ Notice: BGE retrieves high-quality results competitive with OpenAI!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Summary\n\nIn this notebook, we:\n\nâœ… Compared three embedding models (OpenAI, HF Legacy, BGE)  \nâœ… Created FAISS vector stores with BGE as default  \nâœ… Saved vector stores to disk for reuse  \nâœ… Tested similarity search quality  \n\n### Key Takeaways\n\n| Feature | OpenAI | HF Legacy | BGE (New) |\n|---------|--------|-----------|-----------|\n| Quality | Excellent | Very Good | **Excellent** |\n| Dimension | 1536 | 384 | 384 |\n| Cost | $0.02/1M | Free | **Free** |\n| Privacy | Cloud | Local | **Local** |\n| Speed | API latency | Fast | **Fast** |\n\n### Recommendation\n\n- **Best Value:** **BGE** (OpenAI quality, free, local) â­â­â­\n- **Production (Cloud):** OpenAI (established, reliable)\n- **Privacy-sensitive:** BGE or HF (data stays local)\n- **Offline:** BGE or HF (no internet after download)\n- **Legacy projects:** all-MiniLM-L6-v2 (still good)\n\n### Upgrade Path\n\nThe tutorial now uses **BGE** as the default HuggingFace model:\n- âœ… Better quality than all-MiniLM-L6-v2\n- âœ… Same 384 dimensions (compatible)\n- âœ… Drop-in replacement\n- âœ… Configured in `shared/config.py`\n- âœ… Vector stores now use BGE\n\n### Next Steps\n\nContinue to **[03_simple_rag.ipynb](03_simple_rag.ipynb)** to:\n- Create retrievers from vector stores\n- Build complete RAG chains\n- Compare retrieval strategies (Similarity vs MMR)\n- Evaluate RAG performance\n\n---\n\n**ðŸ’¾ Important:** Vector stores now use **BGE embeddings** (BAAI/bge-small-en-v1.5) and are ready for all subsequent notebooks!",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}