{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Embeddings Comparison\n",
    "\n",
    "This notebook compares two embedding approaches for RAG:\n",
    "\n",
    "1. **OpenAI Embeddings** - High quality, API-based\n",
    "2. **HuggingFace Embeddings** - Local, free, privacy-friendly\n",
    "\n",
    "We'll create FAISS vector stores for both and compare performance.\n",
    "\n",
    "**Prerequisites:**\n",
    "- 01_setup_and_basics.ipynb completed\n",
    "- OpenAI API key configured\n",
    "\n",
    "**Duration:** ~8 minutes\n",
    "\n",
    "**Outputs:**\n",
    "- Vector stores saved to `data/vector_stores/`\n",
    "- Performance comparison metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Load necessary modules and prepare documents/chunks from previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gianlucamazza/Workspace/notebooks/llm_rag/venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING DOCUMENTS AND CHUNKS\n",
      "================================================================================\n",
      "\n",
      "Loading 4 documents from web...\n",
      "  - https://python.langchain.com/docs/use_cases/question_answering/\n",
      "  - https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
      "  - https://python.langchain.com/docs/modules/model_io/llms/\n",
      "  - https://python.langchain.com/docs/use_cases/chatbots/\n",
      "âœ“ Loaded 4 documents\n",
      "âœ“ Added custom metadata to all documents\n",
      "Splitting documents...\n",
      "  - Chunk size: 1000\n",
      "  - Chunk overlap: 200\n",
      "âœ“ Created 120 chunks\n",
      "\n",
      "  Sample chunk:\n",
      "    - Length: 839 chars\n",
      "    - Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "    - Preview: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Rea...\n",
      "\n",
      "âœ… Ready with 4 documents and 120 chunks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from shared.config import OPENAI_VECTOR_STORE_PATH, HF_VECTOR_STORE_PATH\n",
    "from shared.loaders import load_and_split\n",
    "from shared.utils import print_section_header, save_vector_store\n",
    "import time\n",
    "\n",
    "print_section_header(\"Loading Documents and Chunks\")\n",
    "\n",
    "# Load and split documents\n",
    "docs, chunks = load_and_split(verbose=True)\n",
    "\n",
    "print(f\"\\nâœ… Ready with {len(docs)} documents and {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OpenAI Embeddings\n",
    "\n",
    "### Features\n",
    "- Model: `text-embedding-3-small`\n",
    "- Dimensions: 1536\n",
    "- Cost: ~$0.02 per 1M tokens\n",
    "- Quality: Excellent\n",
    "\n",
    "### Use When\n",
    "- Production quality required\n",
    "- Budget available\n",
    "- Internet connection reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "OPENAI EMBEDDINGS\n",
      "================================================================================\n",
      "\n",
      "Initializing OpenAI embeddings...\n",
      "âœ“ OpenAI embeddings initialized\n",
      "\n",
      "Testing with query: 'What is retrieval-augmented generation?'\n",
      "\n",
      "Results:\n",
      "  Dimension: 1536\n",
      "  Time: 0.894s\n",
      "  First 5 values: ['-0.0403', '-0.0036', '0.0001', '0.0009', '-0.0103']\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "print_section_header(\"OpenAI Embeddings\")\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "print(\"Initializing OpenAI embeddings...\")\n",
    "openai_embeddings = OpenAIEmbeddings()\n",
    "print(\"âœ“ OpenAI embeddings initialized\")\n",
    "\n",
    "# Test with a sample query\n",
    "test_query = \"What is retrieval-augmented generation?\"\n",
    "print(f\"\\nTesting with query: '{test_query}'\")\n",
    "\n",
    "start_time = time.time()\n",
    "test_embedding = openai_embeddings.embed_query(test_query)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Dimension: {len(test_embedding)}\")\n",
    "print(f\"  Time: {elapsed:.3f}s\")\n",
    "print(f\"  First 5 values: {[f'{v:.4f}' for v in test_embedding[:5]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HuggingFace Embeddings\n",
    "\n",
    "### Features\n",
    "- Model: `sentence-transformers/all-MiniLM-L6-v2`\n",
    "- Dimensions: 384\n",
    "- Cost: Free (runs locally)\n",
    "- Quality: Very good\n",
    "\n",
    "### Use When\n",
    "- Privacy is critical\n",
    "- Offline operation needed\n",
    "- Cost is a constraint\n",
    "- Development/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HUGGINGFACE EMBEDDINGS\n",
      "================================================================================\n",
      "\n",
      "Initializing HuggingFace embeddings...\n",
      "(First run downloads model ~90MB - may take 1-2 minutes)\n",
      "Cache location: /Users/gianlucamazza/.cache/huggingface/\n",
      "\n",
      "âœ“ HuggingFace embeddings initialized\n",
      "\n",
      "Testing with query: 'What is retrieval-augmented generation?'\n",
      "\n",
      "Results:\n",
      "  Dimension: 384\n",
      "  Time: 0.310s\n",
      "  First 5 values: ['-0.1110', '-0.0263', '-0.0579', '0.0598', '-0.0208']\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "print_section_header(\"HuggingFace Embeddings\")\n",
    "\n",
    "print(\"Initializing HuggingFace embeddings...\")\n",
    "print(\"(First run downloads model ~90MB - may take 1-2 minutes)\")\n",
    "print(f\"Cache location: {os.path.expanduser('~/.cache/huggingface/')}\\n\")\n",
    "\n",
    "try:\n",
    "    hf_embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    print(\"âœ“ HuggingFace embeddings initialized\")\n",
    "    \n",
    "    # Test with same query\n",
    "    print(f\"\\nTesting with query: '{test_query}'\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_embedding_hf = hf_embeddings.embed_query(test_query)\n",
    "    elapsed_hf = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Dimension: {len(test_embedding_hf)}\")\n",
    "    print(f\"  Time: {elapsed_hf:.3f}s\")\n",
    "    print(f\"  First 5 values: {[f'{v:.4f}' for v in test_embedding_hf[:5]]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. Check internet connection (first run only)\")\n",
    "    print(\"  2. Verify disk space (~200MB needed)\")\n",
    "    print(\"  3. Try: pip install --upgrade sentence-transformers\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EMBEDDINGS COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Feature    OpenAI   HuggingFace  \n",
      "---------------------------------\n",
      "Dimension  1536     384          \n",
      "Time (s)   0.894    0.310        \n",
      "Mean       -0.0008  0.0002       \n",
      "Std Dev    0.0255   0.0510       \n",
      "Cost       Paid     Free         \n",
      "Privacy    Cloud    Local        \n",
      "\n",
      "ðŸ’¡ Key Takeaways:\n",
      "   - OpenAI: Higher dimension, cloud-based, paid\n",
      "   - HuggingFace: Lower dimension, local, free\n",
      "   - Both produce high-quality embeddings\n",
      "   - Choice depends on requirements and constraints\n"
     ]
    }
   ],
   "source": [
    "from shared.utils import print_comparison_table\n",
    "import numpy as np\n",
    "\n",
    "print_section_header(\"Embeddings Comparison\")\n",
    "\n",
    "data = [\n",
    "    [\"Feature\", \"OpenAI\", \"HuggingFace\"],\n",
    "    [\"Dimension\", len(test_embedding), len(test_embedding_hf)],\n",
    "    [\"Time (s)\", f\"{elapsed:.3f}\", f\"{elapsed_hf:.3f}\"],\n",
    "    [\"Mean\", f\"{np.mean(test_embedding):.4f}\", f\"{np.mean(test_embedding_hf):.4f}\"],\n",
    "    [\"Std Dev\", f\"{np.std(test_embedding):.4f}\", f\"{np.std(test_embedding_hf):.4f}\"],\n",
    "    [\"Cost\", \"Paid\", \"Free\"],\n",
    "    [\"Privacy\", \"Cloud\", \"Local\"]\n",
    "]\n",
    "\n",
    "print_comparison_table(data)\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Takeaways:\")\n",
    "print(\"   - OpenAI: Higher dimension, cloud-based, paid\")\n",
    "print(\"   - HuggingFace: Lower dimension, local, free\")\n",
    "print(\"   - Both produce high-quality embeddings\")\n",
    "print(\"   - Choice depends on requirements and constraints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Vector Stores\n",
    "\n",
    "Now we'll create FAISS vector stores for both embedding types. These will be saved for reuse in all subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CREATING VECTOR STORES\n",
      "================================================================================\n",
      "\n",
      "Creating FAISS vector store with OpenAI embeddings...\n",
      "âœ“ OpenAI vector store created in 1.19s\n",
      "  - 120 documents indexed\n",
      "  - Embedding dimension: 1536\n",
      "\n",
      "Creating FAISS vector store with HuggingFace embeddings...\n",
      "âœ“ HuggingFace vector store created in 0.54s\n",
      "  - 120 documents indexed\n",
      "  - Embedding dimension: 384\n",
      "\n",
      "ðŸ“Š Performance:\n",
      "   OpenAI: 1.19s\n",
      "   HuggingFace: 0.54s\n",
      "   Ratio: 0.46x\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print_section_header(\"Creating Vector Stores\")\n",
    "\n",
    "# Create OpenAI vector store\n",
    "print(\"Creating FAISS vector store with OpenAI embeddings...\")\n",
    "start_time = time.time()\n",
    "vectorstore_openai = FAISS.from_documents(chunks, openai_embeddings)\n",
    "elapsed_openai = time.time() - start_time\n",
    "\n",
    "print(f\"âœ“ OpenAI vector store created in {elapsed_openai:.2f}s\")\n",
    "print(f\"  - {len(chunks)} documents indexed\")\n",
    "print(f\"  - Embedding dimension: 1536\")\n",
    "\n",
    "# Create HuggingFace vector store\n",
    "print(\"\\nCreating FAISS vector store with HuggingFace embeddings...\")\n",
    "start_time = time.time()\n",
    "vectorstore_hf = FAISS.from_documents(chunks, hf_embeddings)\n",
    "elapsed_hf = time.time() - start_time\n",
    "\n",
    "print(f\"âœ“ HuggingFace vector store created in {elapsed_hf:.2f}s\")\n",
    "print(f\"  - {len(chunks)} documents indexed\")\n",
    "print(f\"  - Embedding dimension: 384\")\n",
    "\n",
    "print(\"\\nðŸ“Š Performance:\")\n",
    "print(f\"   OpenAI: {elapsed_openai:.2f}s\")\n",
    "print(f\"   HuggingFace: {elapsed_hf:.2f}s\")\n",
    "print(f\"   Ratio: {elapsed_hf/elapsed_openai:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Vector Stores\n",
    "\n",
    "**IMPORTANT:** We save the vector stores to disk to avoid re-embedding in every notebook. This:\n",
    "- Saves time (~3-4 seconds per notebook)\n",
    "- Reduces API costs (OpenAI charges per embedding)\n",
    "- Ensures consistency across notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAVING VECTOR STORES\n",
      "================================================================================\n",
      "\n",
      "âœ“ Saved vector store to /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/fundamentals/../../data/vector_stores/openai_embeddings\n",
      "âœ“ Saved vector store to /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/fundamentals/../../data/vector_stores/huggingface_embeddings\n",
      "\n",
      "âœ… Vector stores saved successfully!\n",
      "\n",
      "ðŸ“‚ Locations:\n",
      "   OpenAI: /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/fundamentals/../../data/vector_stores/openai_embeddings\n",
      "   HuggingFace: /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/fundamentals/../../data/vector_stores/huggingface_embeddings\n",
      "\n",
      "ðŸ’¡ These will be loaded in subsequent notebooks to avoid re-embedding.\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Saving Vector Stores\")\n",
    "\n",
    "# Save OpenAI vector store\n",
    "save_vector_store(vectorstore_openai, OPENAI_VECTOR_STORE_PATH, verbose=True)\n",
    "\n",
    "# Save HuggingFace vector store\n",
    "save_vector_store(vectorstore_hf, HF_VECTOR_STORE_PATH, verbose=True)\n",
    "\n",
    "print(f\"\\nâœ… Vector stores saved successfully!\")\n",
    "print(f\"\\nðŸ“‚ Locations:\")\n",
    "print(f\"   OpenAI: {OPENAI_VECTOR_STORE_PATH}\")\n",
    "print(f\"   HuggingFace: {HF_VECTOR_STORE_PATH}\")\n",
    "print(f\"\\nðŸ’¡ These will be loaded in subsequent notebooks to avoid re-embedding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Similarity Search\n",
    "\n",
    "Quick test to verify the vector stores work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TESTING SIMILARITY SEARCH\n",
      "================================================================================\n",
      "\n",
      "Query: 'How to build a RAG agent with LangChain?'\n",
      "\n",
      "\n",
      "=== OpenAI Embeddings ===\n",
      "\n",
      "Retrieved Documents\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n",
      "\n",
      "3. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\n",
      "You can add a deeper level of control and customization using the LangGraph framework directlyâ€” fo...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "=== HuggingFace Embeddings ===\n",
      "\n",
      "Retrieved Documents\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n",
      "\n",
      "3. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Content: Task decomposition can be done by...\n",
      "\n",
      "Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}\n",
      "Con...\n",
      "\n",
      "âœ… Both vector stores working correctly!\n"
     ]
    }
   ],
   "source": [
    "from shared.utils import print_results\n",
    "\n",
    "print_section_header(\"Testing Similarity Search\")\n",
    "\n",
    "query = \"How to build a RAG agent with LangChain?\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "# Test OpenAI\n",
    "print(\"\\n=== OpenAI Embeddings ===\")\n",
    "results_openai = vectorstore_openai.similarity_search(query, k=3)\n",
    "print_results(results_openai, max_docs=3, preview_length=200)\n",
    "\n",
    "# Test HuggingFace\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n=== HuggingFace Embeddings ===\")\n",
    "results_hf = vectorstore_hf.similarity_search(query, k=3)\n",
    "print_results(results_hf, max_docs=3, preview_length=200)\n",
    "\n",
    "print(\"\\nâœ… Both vector stores working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "âœ… Compared OpenAI vs HuggingFace embeddings  \n",
    "âœ… Created FAISS vector stores for both  \n",
    "âœ… Saved vector stores to disk for reuse  \n",
    "âœ… Tested similarity search  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Feature | OpenAI | HuggingFace |\n",
    "|---------|--------|-------------|\n",
    "| Quality | Excellent | Very Good |\n",
    "| Dimension | 1536 | 384 |\n",
    "| Cost | $0.02/1M tokens | Free |\n",
    "| Privacy | Cloud | Local |\n",
    "| Speed | Fast API | Local compute |\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "- **Production:** OpenAI (higher quality, reliable)\n",
    "- **Development:** HuggingFace (free, fast iteration)\n",
    "- **Privacy-sensitive:** HuggingFace (data stays local)\n",
    "- **Offline:** HuggingFace (no internet needed after download)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Continue to **[03_simple_rag.ipynb](03_simple_rag.ipynb)** to:\n",
    "- Create retrievers from vector stores\n",
    "- Build complete RAG chains\n",
    "- Compare retrieval strategies (Similarity vs MMR)\n",
    "- Evaluate RAG performance\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¾ Important:** Vector stores are now saved and ready for all subsequent notebooks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
