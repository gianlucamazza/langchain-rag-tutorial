{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Embeddings Comparison\n",
    "\n",
    "This notebook compares two embedding approaches for RAG:\n",
    "\n",
    "1. **OpenAI Embeddings** - High quality, API-based\n",
    "2. **HuggingFace Embeddings** - Local, free, privacy-friendly\n",
    "\n",
    "We'll create FAISS vector stores for both and compare performance.\n",
    "\n",
    "**Prerequisites:**\n",
    "- 01_setup_and_basics.ipynb completed\n",
    "- OpenAI API key configured\n",
    "\n",
    "**Duration:** ~8 minutes\n",
    "\n",
    "**Outputs:**\n",
    "- Vector stores saved to `data/vector_stores/`\n",
    "- Performance comparison metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Load necessary modules and prepare documents/chunks from previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from shared.config import OPENAI_VECTOR_STORE_PATH, HF_VECTOR_STORE_PATH\n",
    "from shared.loaders import load_and_split\n",
    "from shared.utils import print_section_header, save_vector_store\n",
    "import time\n",
    "\n",
    "print_section_header(\"Loading Documents and Chunks\")\n",
    "\n",
    "# Load and split documents\n",
    "docs, chunks = load_and_split(verbose=True)\n",
    "\n",
    "print(f\"\\nâœ… Ready with {len(docs)} documents and {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. OpenAI Embeddings\n",
    "\n",
    "### Features\n",
    "- Model: `text-embedding-3-small`\n",
    "- Dimensions: 1536\n",
    "- Cost: ~$0.02 per 1M tokens\n",
    "- Quality: Excellent\n",
    "\n",
    "### Use When\n",
    "- Production quality required\n",
    "- Budget available\n",
    "- Internet connection reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "print_section_header(\"OpenAI Embeddings\")\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "print(\"Initializing OpenAI embeddings...\")\n",
    "openai_embeddings = OpenAIEmbeddings()\n",
    "print(\"âœ“ OpenAI embeddings initialized\")\n",
    "\n",
    "# Test with a sample query\n",
    "test_query = \"What is retrieval-augmented generation?\"\n",
    "print(f\"\\nTesting with query: '{test_query}'\")\n",
    "\n",
    "start_time = time.time()\n",
    "test_embedding = openai_embeddings.embed_query(test_query)\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Dimension: {len(test_embedding)}\")\n",
    "print(f\"  Time: {elapsed:.3f}s\")\n",
    "print(f\"  First 5 values: {[f'{v:.4f}' for v in test_embedding[:5]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HuggingFace Embeddings\n",
    "\n",
    "### Features\n",
    "- Model: `sentence-transformers/all-MiniLM-L6-v2`\n",
    "- Dimensions: 384\n",
    "- Cost: Free (runs locally)\n",
    "- Quality: Very good\n",
    "\n",
    "### Use When\n",
    "- Privacy is critical\n",
    "- Offline operation needed\n",
    "- Cost is a constraint\n",
    "- Development/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "\n",
    "print_section_header(\"HuggingFace Embeddings\")\n",
    "\n",
    "print(\"Initializing HuggingFace embeddings...\")\n",
    "print(\"(First run downloads model ~90MB - may take 1-2 minutes)\")\n",
    "print(f\"Cache location: {os.path.expanduser('~/.cache/huggingface/')}\\n\")\n",
    "\n",
    "try:\n",
    "    hf_embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    )\n",
    "    print(\"âœ“ HuggingFace embeddings initialized\")\n",
    "    \n",
    "    # Test with same query\n",
    "    print(f\"\\nTesting with query: '{test_query}'\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    test_embedding_hf = hf_embeddings.embed_query(test_query)\n",
    "    elapsed_hf = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Dimension: {len(test_embedding_hf)}\")\n",
    "    print(f\"  Time: {elapsed_hf:.3f}s\")\n",
    "    print(f\"  First 5 values: {[f'{v:.4f}' for v in test_embedding_hf[:5]]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— Error: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. Check internet connection (first run only)\")\n",
    "    print(\"  2. Verify disk space (~200MB needed)\")\n",
    "    print(\"  3. Try: pip install --upgrade sentence-transformers\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Side-by-Side Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.utils import print_comparison_table\n",
    "import numpy as np\n",
    "\n",
    "print_section_header(\"Embeddings Comparison\")\n",
    "\n",
    "data = [\n",
    "    [\"Feature\", \"OpenAI\", \"HuggingFace\"],\n",
    "    [\"Dimension\", len(test_embedding), len(test_embedding_hf)],\n",
    "    [\"Time (s)\", f\"{elapsed:.3f}\", f\"{elapsed_hf:.3f}\"],\n",
    "    [\"Mean\", f\"{np.mean(test_embedding):.4f}\", f\"{np.mean(test_embedding_hf):.4f}\"],\n",
    "    [\"Std Dev\", f\"{np.std(test_embedding):.4f}\", f\"{np.std(test_embedding_hf):.4f}\"],\n",
    "    [\"Cost\", \"Paid\", \"Free\"],\n",
    "    [\"Privacy\", \"Cloud\", \"Local\"]\n",
    "]\n",
    "\n",
    "print_comparison_table(data)\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Takeaways:\")\n",
    "print(\"   - OpenAI: Higher dimension, cloud-based, paid\")\n",
    "print(\"   - HuggingFace: Lower dimension, local, free\")\n",
    "print(\"   - Both produce high-quality embeddings\")\n",
    "print(\"   - Choice depends on requirements and constraints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create Vector Stores\n",
    "\n",
    "Now we'll create FAISS vector stores for both embedding types. These will be saved for reuse in all subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print_section_header(\"Creating Vector Stores\")\n",
    "\n",
    "# Create OpenAI vector store\n",
    "print(\"Creating FAISS vector store with OpenAI embeddings...\")\n",
    "start_time = time.time()\n",
    "vectorstore_openai = FAISS.from_documents(chunks, openai_embeddings)\n",
    "elapsed_openai = time.time() - start_time\n",
    "\n",
    "print(f\"âœ“ OpenAI vector store created in {elapsed_openai:.2f}s\")\n",
    "print(f\"  - {len(chunks)} documents indexed\")\n",
    "print(f\"  - Embedding dimension: 1536\")\n",
    "\n",
    "# Create HuggingFace vector store\n",
    "print(\"\\nCreating FAISS vector store with HuggingFace embeddings...\")\n",
    "start_time = time.time()\n",
    "vectorstore_hf = FAISS.from_documents(chunks, hf_embeddings)\n",
    "elapsed_hf = time.time() - start_time\n",
    "\n",
    "print(f\"âœ“ HuggingFace vector store created in {elapsed_hf:.2f}s\")\n",
    "print(f\"  - {len(chunks)} documents indexed\")\n",
    "print(f\"  - Embedding dimension: 384\")\n",
    "\n",
    "print(\"\\nðŸ“Š Performance:\")\n",
    "print(f\"   OpenAI: {elapsed_openai:.2f}s\")\n",
    "print(f\"   HuggingFace: {elapsed_hf:.2f}s\")\n",
    "print(f\"   Ratio: {elapsed_hf/elapsed_openai:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Vector Stores\n",
    "\n",
    "**IMPORTANT:** We save the vector stores to disk to avoid re-embedding in every notebook. This:\n",
    "- Saves time (~3-4 seconds per notebook)\n",
    "- Reduces API costs (OpenAI charges per embedding)\n",
    "- Ensures consistency across notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Saving Vector Stores\")\n",
    "\n",
    "# Save OpenAI vector store\n",
    "save_vector_store(vectorstore_openai, OPENAI_VECTOR_STORE_PATH, verbose=True)\n",
    "\n",
    "# Save HuggingFace vector store\n",
    "save_vector_store(vectorstore_hf, HF_VECTOR_STORE_PATH, verbose=True)\n",
    "\n",
    "print(f\"\\nâœ… Vector stores saved successfully!\")\n",
    "print(f\"\\nðŸ“‚ Locations:\")\n",
    "print(f\"   OpenAI: {OPENAI_VECTOR_STORE_PATH}\")\n",
    "print(f\"   HuggingFace: {HF_VECTOR_STORE_PATH}\")\n",
    "print(f\"\\nðŸ’¡ These will be loaded in subsequent notebooks to avoid re-embedding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Similarity Search\n",
    "\n",
    "Quick test to verify the vector stores work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.utils import print_results\n",
    "\n",
    "print_section_header(\"Testing Similarity Search\")\n",
    "\n",
    "query = \"How to build a RAG agent with LangChain?\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "# Test OpenAI\n",
    "print(\"\\n=== OpenAI Embeddings ===\")\n",
    "results_openai = vectorstore_openai.similarity_search(query, k=3)\n",
    "print_results(results_openai, max_docs=3, preview_length=200)\n",
    "\n",
    "# Test HuggingFace\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n=== HuggingFace Embeddings ===\")\n",
    "results_hf = vectorstore_hf.similarity_search(query, k=3)\n",
    "print_results(results_hf, max_docs=3, preview_length=200)\n",
    "\n",
    "print(\"\\nâœ… Both vector stores working correctly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "âœ… Compared OpenAI vs HuggingFace embeddings  \n",
    "âœ… Created FAISS vector stores for both  \n",
    "âœ… Saved vector stores to disk for reuse  \n",
    "âœ… Tested similarity search  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Feature | OpenAI | HuggingFace |\n",
    "|---------|--------|-------------|\n",
    "| Quality | Excellent | Very Good |\n",
    "| Dimension | 1536 | 384 |\n",
    "| Cost | $0.02/1M tokens | Free |\n",
    "| Privacy | Cloud | Local |\n",
    "| Speed | Fast API | Local compute |\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "- **Production:** OpenAI (higher quality, reliable)\n",
    "- **Development:** HuggingFace (free, fast iteration)\n",
    "- **Privacy-sensitive:** HuggingFace (data stays local)\n",
    "- **Offline:** HuggingFace (no internet needed after download)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Continue to **[03_simple_rag.ipynb](03_simple_rag.ipynb)** to:\n",
    "- Create retrievers from vector stores\n",
    "- Build complete RAG chains\n",
    "- Compare retrieval strategies (Similarity vs MMR)\n",
    "- Evaluate RAG performance\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¾ Important:** Vector stores are now saved and ready for all subsequent notebooks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
