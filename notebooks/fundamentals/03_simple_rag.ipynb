{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Simple RAG\n",
    "\n",
    "This notebook implements the complete Simple RAG architecture:\n",
    "\n",
    "```\n",
    "User Query â†’ Retriever â†’ LLM + Prompt â†’ Response\n",
    "```\n",
    "\n",
    "We'll compare two retrieval strategies:\n",
    "1. **Similarity Search** - Returns most similar documents\n",
    "2. **MMR** - Balances relevance with diversity\n",
    "\n",
    "**Prerequisites:**\n",
    "- 01_setup_and_basics.ipynb\n",
    "- 02_embeddings_comparison.ipynb\n",
    "- Vector stores created\n",
    "\n",
    "**Duration:** ~15 minutes\n",
    "\n",
    "**Outputs:**\n",
    "- Working RAG chains\n",
    "- Baseline performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Vector Stores\n",
    "\n",
    "Load the pre-built vector stores from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from shared.config import OPENAI_VECTOR_STORE_PATH, DEFAULT_K\n",
    "from shared.utils import load_vector_store, print_section_header\n",
    "\n",
    "print_section_header(\"Loading Vector Store\")\n",
    "\n",
    "# Initialize embeddings\n",
    "openai_embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Load pre-built vector store\n",
    "vectorstore = load_vector_store(\n",
    "    OPENAI_VECTOR_STORE_PATH,\n",
    "    openai_embeddings,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Vector store loaded and ready!\")\n",
    "print(f\"   Using k={DEFAULT_K} documents per query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieval Strategies\n",
    "\n",
    "### Similarity Search\n",
    "- Returns documents most similar to query\n",
    "- Fast and straightforward\n",
    "- May return redundant results\n",
    "\n",
    "### MMR (Maximal Marginal Relevance)\n",
    "- Balances relevance with diversity\n",
    "- Reduces redundancy\n",
    "- Better for exploration and broad topics\n",
    "- Controlled by `lambda_mult`:\n",
    "  - 1.0 = pure relevance\n",
    "  - 0.0 = pure diversity  \n",
    "  - 0.5 = balanced (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.config import DEFAULT_MMR_FETCH_K, DEFAULT_MMR_LAMBDA\n",
    "\n",
    "print_section_header(\"Creating Retrievers\")\n",
    "\n",
    "# Similarity retriever\n",
    "similarity_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": DEFAULT_K}\n",
    ")\n",
    "print(f\"âœ“ Similarity retriever created (k={DEFAULT_K})\")\n",
    "\n",
    "# MMR retriever\n",
    "mmr_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": DEFAULT_K,\n",
    "        \"fetch_k\": DEFAULT_MMR_FETCH_K,\n",
    "        \"lambda_mult\": DEFAULT_MMR_LAMBDA\n",
    "    }\n",
    ")\n",
    "print(f\"âœ“ MMR retriever created (k={DEFAULT_K}, fetch_k={DEFAULT_MMR_FETCH_K}, Î»={DEFAULT_MMR_LAMBDA})\")\n",
    "\n",
    "print(\"\\nðŸ’¡ MMR fetches {DEFAULT_MMR_FETCH_K} candidates, then selects {DEFAULT_K} diverse documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Retrieval\n",
    "\n",
    "Compare both retrieval strategies with the same query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.utils import print_results\n",
    "\n",
    "query = \"What are the steps to build a RAG agent?\"\n",
    "\n",
    "print_section_header(f\"Retrieval Comparison\")\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "# Similarity search\n",
    "print(\"=== Similarity Search ===\")\n",
    "similarity_docs = similarity_retriever.invoke(query)\n",
    "print_results(similarity_docs, \"Retrieved Documents\", max_docs=3, preview_length=150)\n",
    "\n",
    "# MMR search\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n=== MMR Search ===\")\n",
    "mmr_docs = mmr_retriever.invoke(query)\n",
    "print_results(mmr_docs, \"Retrieved Documents\", max_docs=3, preview_length=150)\n",
    "\n",
    "print(\"\\nðŸ’¡ Notice: MMR may show more diverse sources and content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize LLM\n",
    "\n",
    "We use GPT-4o-mini for cost-effectiveness. Set temperature=0 for deterministic responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from shared.config import DEFAULT_MODEL, DEFAULT_TEMPERATURE\n",
    "\n",
    "print_section_header(\"Initializing LLM\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=DEFAULT_MODEL,\n",
    "    temperature=DEFAULT_TEMPERATURE\n",
    ")\n",
    "\n",
    "print(f\"âœ“ LLM initialized\")\n",
    "print(f\"  Model: {DEFAULT_MODEL}\")\n",
    "print(f\"  Temperature: {DEFAULT_TEMPERATURE} (deterministic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build RAG Chains\n",
    "\n",
    "Create complete RAG chains using LCEL (LangChain Expression Language):\n",
    "\n",
    "```python\n",
    "Chain = Retriever â†’ Format â†’ Prompt + LLM â†’ Parse\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from shared.prompts import RAG_PROMPT_TEMPLATE\n",
    "from shared.utils import format_docs\n",
    "\n",
    "print_section_header(\"Building RAG Chains\")\n",
    "\n",
    "# Similarity RAG chain\n",
    "similarity_chain = (\n",
    "    {\"context\": similarity_retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | RAG_PROMPT_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"âœ“ Similarity RAG chain created\")\n",
    "\n",
    "# MMR RAG chain\n",
    "mmr_chain = (\n",
    "    {\"context\": mmr_retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | RAG_PROMPT_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"âœ“ MMR RAG chain created\")\n",
    "\n",
    "print(\"\\nâœ… RAG chains ready for queries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test RAG Chains\n",
    "\n",
    "Compare answers from both retrieval strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "query = \"How to build a RAG agent with LangChain?\"\n",
    "\n",
    "print_section_header(\"RAG Chain Comparison\")\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "# Similarity chain\n",
    "print(\"=\" * 80)\n",
    "print(\"SIMILARITY RAG\")\n",
    "print(\"=\" * 80)\n",
    "start = time.time()\n",
    "response_sim = similarity_chain.invoke(query)\n",
    "time_sim = time.time() - start\n",
    "print(response_sim)\n",
    "print(f\"\\nâ±ï¸  Time: {time_sim:.2f}s\")\n",
    "\n",
    "# MMR chain\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MMR RAG\")\n",
    "print(\"=\" * 80)\n",
    "start = time.time()\n",
    "response_mmr = mmr_chain.invoke(query)\n",
    "time_mmr = time.time() - start\n",
    "print(response_mmr)\n",
    "print(f\"\\nâ±ï¸  Time: {time_mmr:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nðŸ“Š Metrics:\")\n",
    "print(f\"   Similarity: {len(response_sim)} chars, {time_sim:.2f}s\")\n",
    "print(f\"   MMR: {len(response_mmr)} chars, {time_mmr:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multiple Query Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"What is a retriever?\",\n",
    "    \"How do embeddings work?\",\n",
    "    \"What are the key components of RAG?\"\n",
    "]\n",
    "\n",
    "print_section_header(\"Multiple Query Testing\")\n",
    "\n",
    "for i, q in enumerate(test_queries, 1):\n",
    "    print(f\"\\nQuery {i}: '{q}'\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    response = similarity_chain.invoke(q)\n",
    "    print(response[:300] + \"...\" if len(response) > 300 else response)\n",
    "\n",
    "print(\"\\nâœ… All queries processed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices Summary\n",
    "\n",
    "### Retrieval Strategy Selection\n",
    "\n",
    "**Use Similarity Search when:**\n",
    "- Precise factual answers needed\n",
    "- Query is specific and well-defined\n",
    "- Speed is priority\n",
    "\n",
    "**Use MMR when:**\n",
    "- Exploring a topic broadly\n",
    "- Avoiding redundant information\n",
    "- Diverse perspectives valuable\n",
    "\n",
    "### Parameter Tuning\n",
    "\n",
    "```python\n",
    "# Similarity\n",
    "k=4  # Typically 3-5 documents\n",
    "\n",
    "# MMR\n",
    "k=4  # Final count\n",
    "fetch_k=20  # Candidates (3-5x of k)\n",
    "lambda_mult=0.5  # Balance (0.0-1.0)\n",
    "```\n",
    "\n",
    "### Prompt Engineering\n",
    "\n",
    "Good prompts:\n",
    "- Instruct model to cite sources\n",
    "- Tell model when to say \"I don't know\"\n",
    "- Be specific about format\n",
    "- Include context clearly\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "- Cache vector stores (done!)\n",
    "- Use batch operations for multiple docs\n",
    "- Consider async for concurrent queries\n",
    "- Monitor token usage and costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "âœ… Loaded pre-built vector stores  \n",
    "âœ… Created similarity and MMR retrievers  \n",
    "âœ… Built complete RAG chains with LCEL  \n",
    "âœ… Compared retrieval strategies  \n",
    "âœ… Established baseline performance  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Simple RAG** is the foundation for all advanced architectures\n",
    "- **Similarity search** is fast and precise\n",
    "- **MMR** provides diversity at slight cost of speed\n",
    "- **LCEL** enables composable, modular chains\n",
    "- **Vector store caching** saves time and money\n",
    "\n",
    "### Performance Baseline\n",
    "\n",
    "These metrics serve as baseline for comparing advanced architectures:\n",
    "- **Latency:** ~1-2s per query\n",
    "- **Quality:** Good answers for in-domain queries\n",
    "- **Limitations:** \n",
    "  - No conversation memory\n",
    "  - Fixed retrieval strategy\n",
    "  - No self-correction\n",
    "  - No web fallback\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Explore **Advanced Architectures** to address limitations:\n",
    "\n",
    "- **[04_rag_with_memory.ipynb](../advanced_architectures/04_rag_with_memory.ipynb)** - Conversational RAG\n",
    "- **[05_branched_rag.ipynb](../advanced_architectures/05_branched_rag.ipynb)** - Multi-query\n",
    "- **[06_hyde.ipynb](../advanced_architectures/06_hyde.ipynb)** - Hypothetical docs\n",
    "- **[07_adaptive_rag.ipynb](../advanced_architectures/07_adaptive_rag.ipynb)** - Intelligent routing\n",
    "- **[08_corrective_rag.ipynb](../advanced_architectures/08_corrective_rag.ipynb)** - Quality checking\n",
    "- **[09_self_rag.ipynb](../advanced_architectures/09_self_rag.ipynb)** - Self-reflective\n",
    "- **[10_agentic_rag.ipynb](../advanced_architectures/10_agentic_rag.ipynb)** - Autonomous agents\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ‰ Fundamentals Complete!** You now have a solid foundation in RAG systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
