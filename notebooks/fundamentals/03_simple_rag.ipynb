{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Simple RAG\n",
    "\n",
    "This notebook implements the complete Simple RAG architecture:\n",
    "\n",
    "```\n",
    "User Query â†’ Retriever â†’ LLM + Prompt â†’ Response\n",
    "```\n",
    "\n",
    "We'll compare two retrieval strategies:\n",
    "1. **Similarity Search** - Returns most similar documents\n",
    "2. **MMR** - Balances relevance with diversity\n",
    "\n",
    "**Prerequisites:**\n",
    "- 01_setup_and_basics.ipynb\n",
    "- 02_embeddings_comparison.ipynb\n",
    "- Vector stores created\n",
    "\n",
    "**Duration:** ~15 minutes\n",
    "\n",
    "**Outputs:**\n",
    "- Working RAG chains\n",
    "- Baseline performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Load Vector Stores\n",
    "\n",
    "Load the pre-built vector stores from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING VECTOR STORE\n",
      "================================================================================\n",
      "\n",
      "âœ“ Loaded vector store from /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/fundamentals/../../data/vector_stores/openai_embeddings\n",
      "\n",
      "âœ… Vector store loaded and ready!\n",
      "   Using k=4 documents per query\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from shared.config import OPENAI_VECTOR_STORE_PATH, DEFAULT_K\n",
    "from shared.utils import load_vector_store, print_section_header\n",
    "\n",
    "print_section_header(\"Loading Vector Store\")\n",
    "\n",
    "# Initialize embeddings\n",
    "openai_embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Load pre-built vector store\n",
    "vectorstore = load_vector_store(\n",
    "    OPENAI_VECTOR_STORE_PATH,\n",
    "    openai_embeddings,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Vector store loaded and ready!\")\n",
    "print(f\"   Using k={DEFAULT_K} documents per query\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieval Strategies\n",
    "\n",
    "### Similarity Search\n",
    "- Returns documents most similar to query\n",
    "- Fast and straightforward\n",
    "- May return redundant results\n",
    "\n",
    "### MMR (Maximal Marginal Relevance)\n",
    "- Balances relevance with diversity\n",
    "- Reduces redundancy\n",
    "- Better for exploration and broad topics\n",
    "- Controlled by `lambda_mult`:\n",
    "  - 1.0 = pure relevance\n",
    "  - 0.0 = pure diversity  \n",
    "  - 0.5 = balanced (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CREATING RETRIEVERS\n",
      "================================================================================\n",
      "\n",
      "âœ“ Similarity retriever created (k=4)\n",
      "âœ“ MMR retriever created (k=4, fetch_k=20, Î»=0.5)\n",
      "\n",
      "ðŸ’¡ MMR fetches {DEFAULT_MMR_FETCH_K} candidates, then selects {DEFAULT_K} diverse documents\n"
     ]
    }
   ],
   "source": [
    "from shared.config import DEFAULT_MMR_FETCH_K, DEFAULT_MMR_LAMBDA\n",
    "\n",
    "print_section_header(\"Creating Retrievers\")\n",
    "\n",
    "# Similarity retriever\n",
    "similarity_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": DEFAULT_K}\n",
    ")\n",
    "print(f\"âœ“ Similarity retriever created (k={DEFAULT_K})\")\n",
    "\n",
    "# MMR retriever\n",
    "mmr_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\n",
    "        \"k\": DEFAULT_K,\n",
    "        \"fetch_k\": DEFAULT_MMR_FETCH_K,\n",
    "        \"lambda_mult\": DEFAULT_MMR_LAMBDA\n",
    "    }\n",
    ")\n",
    "print(f\"âœ“ MMR retriever created (k={DEFAULT_K}, fetch_k={DEFAULT_MMR_FETCH_K}, Î»={DEFAULT_MMR_LAMBDA})\")\n",
    "\n",
    "print(\"\\nðŸ’¡ MMR fetches {DEFAULT_MMR_FETCH_K} candidates, then selects {DEFAULT_K} diverse documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Retrieval\n",
    "\n",
    "Compare both retrieval strategies with the same query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RETRIEVAL COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Query: 'What are the steps to build a RAG agent?'\n",
      "\n",
      "=== Similarity Search ===\n",
      "\n",
      "Retrieved Documents\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Rea...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Rea...\n",
      "\n",
      "3. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\n",
      "You can add a deeper level of control and custom...\n",
      "\n",
      "... and 1 more documents\n",
      "\n",
      "================================================================================\n",
      "\n",
      "=== MMR Search ===\n",
      "\n",
      "Retrieved Documents\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Rea...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\n",
      "Generate: A model produces an answer using a prompt that i...\n",
      "\n",
      "3. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: A RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\n",
      "A two-step RAG chain that uses just a single LLM...\n",
      "\n",
      "... and 1 more documents\n",
      "\n",
      "ðŸ’¡ Notice: MMR may show more diverse sources and content\n"
     ]
    }
   ],
   "source": [
    "from shared.utils import print_results\n",
    "\n",
    "query = \"What are the steps to build a RAG agent?\"\n",
    "\n",
    "print_section_header(\"Retrieval Comparison\")\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "# Similarity search\n",
    "print(\"=== Similarity Search ===\")\n",
    "similarity_docs = similarity_retriever.invoke(query)\n",
    "print_results(similarity_docs, \"Retrieved Documents\", max_docs=3, preview_length=150)\n",
    "\n",
    "# MMR search\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n=== MMR Search ===\")\n",
    "mmr_docs = mmr_retriever.invoke(query)\n",
    "print_results(mmr_docs, \"Retrieved Documents\", max_docs=3, preview_length=150)\n",
    "\n",
    "print(\"\\nðŸ’¡ Notice: MMR may show more diverse sources and content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize LLM\n",
    "\n",
    "We use GPT-4o-mini for cost-effectiveness. Set temperature=0 for deterministic responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INITIALIZING LLM\n",
      "================================================================================\n",
      "\n",
      "âœ“ LLM initialized\n",
      "  Model: gpt-4o-mini\n",
      "  Temperature: 0.0 (deterministic)\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from shared.config import DEFAULT_MODEL, DEFAULT_TEMPERATURE\n",
    "\n",
    "print_section_header(\"Initializing LLM\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=DEFAULT_MODEL,\n",
    "    temperature=DEFAULT_TEMPERATURE\n",
    ")\n",
    "\n",
    "print(\"âœ“ LLM initialized\")\n",
    "print(f\"  Model: {DEFAULT_MODEL}\")\n",
    "print(f\"  Temperature: {DEFAULT_TEMPERATURE} (deterministic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build RAG Chains\n",
    "\n",
    "Create complete RAG chains using LCEL (LangChain Expression Language):\n",
    "\n",
    "```python\n",
    "Chain = Retriever â†’ Format â†’ Prompt + LLM â†’ Parse\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BUILDING RAG CHAINS\n",
      "================================================================================\n",
      "\n",
      "âœ“ Similarity RAG chain created\n",
      "âœ“ MMR RAG chain created\n",
      "\n",
      "âœ… RAG chains ready for queries!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from shared.prompts import RAG_PROMPT_TEMPLATE\n",
    "from shared.utils import format_docs\n",
    "\n",
    "print_section_header(\"Building RAG Chains\")\n",
    "\n",
    "# Similarity RAG chain\n",
    "similarity_chain = (\n",
    "    {\"context\": similarity_retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | RAG_PROMPT_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"âœ“ Similarity RAG chain created\")\n",
    "\n",
    "# MMR RAG chain\n",
    "mmr_chain = (\n",
    "    {\"context\": mmr_retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | RAG_PROMPT_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "print(\"âœ“ MMR RAG chain created\")\n",
    "\n",
    "print(\"\\nâœ… RAG chains ready for queries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test RAG Chains\n",
    "\n",
    "Compare answers from both retrieval strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RAG CHAIN COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Query: 'How to build a RAG agent with LangChain?'\n",
      "\n",
      "================================================================================\n",
      "SIMILARITY RAG\n",
      "================================================================================\n",
      "The context provided does not contain specific step-by-step instructions on how to build a RAG agent with LangChain. It mentions components like indexing, loading documents, splitting documents, storing documents, and retrieval and generation, but does not elaborate on the actual process of building a RAG agent.\n",
      "\n",
      "For detailed instructions, you may need to refer to the LangChain documentation or tutorials directly.\n",
      "\n",
      "â±ï¸  Time: 2.95s\n",
      "\n",
      "================================================================================\n",
      "MMR RAG\n",
      "================================================================================\n",
      "To build a RAG (Retrieval-Augmented Generation) agent with LangChain, you can follow these general steps based on the provided context:\n",
      "\n",
      "1. **Setup and Installation**: First, ensure you have Python 3.10+ installed and then install LangChain using the command:\n",
      "   ```bash\n",
      "   pip install -U langchain\n",
      "   ```\n",
      "\n",
      "2. **Indexing Documents**: Load your documents, split them into manageable pieces, and store them in a vector store for retrieval.\n",
      "\n",
      "3. **Retrieval and Generation**:\n",
      "   - Implement a retrieval mechanism that takes user input and retrieves relevant document splits from storage using a Retriever.\n",
      "   - Use a model to generate an answer by passing the retrieved documents along with the user's question.\n",
      "\n",
      "4. **Building the Application Logic**: Create a simple application that:\n",
      "   - Takes a user question.\n",
      "   - Searches for relevant documents.\n",
      "   - Passes the retrieved documents and the initial question to a model.\n",
      "   - Returns the generated answer.\n",
      "\n",
      "5. **Implementing a RAG Agent**: You can create a minimal RAG agent by implementing a tool that wraps your vector store. This can be done using the `tool` from LangChain.\n",
      "\n",
      "6. **Example Implementations**: You can start with a simple RAG agent that executes searches with a basic tool or a two-step RAG chain that uses a single LLM call per query for efficiency.\n",
      "\n",
      "These steps provide a high-level overview of building a RAG agent with LangChain. For detailed code examples and specific implementations, you may refer to the LangChain documentation. \n",
      "\n",
      "This information is derived from the context provided, specifically from the sections discussing the setup, indexing, retrieval, and generation processes for building a RAG agent.\n",
      "\n",
      "â±ï¸  Time: 9.05s\n",
      "\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Metrics:\n",
      "   Similarity: 417 chars, 2.95s\n",
      "   MMR: 1685 chars, 9.05s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "query = \"How to build a RAG agent with LangChain?\"\n",
    "\n",
    "print_section_header(\"RAG Chain Comparison\")\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "# Similarity chain\n",
    "print(\"=\" * 80)\n",
    "print(\"SIMILARITY RAG\")\n",
    "print(\"=\" * 80)\n",
    "start = time.time()\n",
    "response_sim = similarity_chain.invoke(query)\n",
    "time_sim = time.time() - start\n",
    "print(response_sim)\n",
    "print(f\"\\nâ±ï¸  Time: {time_sim:.2f}s\")\n",
    "\n",
    "# MMR chain\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MMR RAG\")\n",
    "print(\"=\" * 80)\n",
    "start = time.time()\n",
    "response_mmr = mmr_chain.invoke(query)\n",
    "time_mmr = time.time() - start\n",
    "print(response_mmr)\n",
    "print(f\"\\nâ±ï¸  Time: {time_mmr:.2f}s\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nðŸ“Š Metrics:\")\n",
    "print(f\"   Similarity: {len(response_sim)} chars, {time_sim:.2f}s\")\n",
    "print(f\"   MMR: {len(response_mmr)} chars, {time_mmr:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Multiple Query Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MULTIPLE QUERY TESTING\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Query 1: 'What is a retriever?'\n",
      "--------------------------------------------------------------------------------\n",
      "A retriever is a component in a Retrieval Augmented Generation (RAG) application that retrieves relevant splits or documents from storage based on a user input. It plays a crucial role in the process by searching for information that is pertinent to the user's question before passing that informatio...\n",
      "\n",
      "Query 2: 'How do embeddings work?'\n",
      "--------------------------------------------------------------------------------\n",
      "The context provided does not contain specific information about how embeddings work. It mentions the use of embeddings in the context of the OpenAIEmbeddings model and their application in indexing and storing data for search purposes, but it does not explain the underlying mechanism or theory behi...\n",
      "\n",
      "Query 3: 'What are the key components of RAG?'\n",
      "--------------------------------------------------------------------------------\n",
      "The key components of RAG (Retrieval-Augmented Generation) include:\n",
      "\n",
      "1. **Indexing**: This is a pipeline for ingesting data from a source and indexing it, which usually happens in a separate process.\n",
      "\n",
      "2. **Retrieval and Generation**: This is the actual RAG process that takes the user query at run ti...\n",
      "\n",
      "âœ… All queries processed successfully!\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "    \"What is a retriever?\",\n",
    "    \"How do embeddings work?\",\n",
    "    \"What are the key components of RAG?\"\n",
    "]\n",
    "\n",
    "print_section_header(\"Multiple Query Testing\")\n",
    "\n",
    "for i, q in enumerate(test_queries, 1):\n",
    "    print(f\"\\nQuery {i}: '{q}'\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    response = similarity_chain.invoke(q)\n",
    "    print(response[:300] + \"...\" if len(response) > 300 else response)\n",
    "\n",
    "print(\"\\nâœ… All queries processed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices Summary\n",
    "\n",
    "### Retrieval Strategy Selection\n",
    "\n",
    "**Use Similarity Search when:**\n",
    "- Precise factual answers needed\n",
    "- Query is specific and well-defined\n",
    "- Speed is priority\n",
    "\n",
    "**Use MMR when:**\n",
    "- Exploring a topic broadly\n",
    "- Avoiding redundant information\n",
    "- Diverse perspectives valuable\n",
    "\n",
    "### Parameter Tuning\n",
    "\n",
    "```python\n",
    "# Similarity\n",
    "k=4  # Typically 3-5 documents\n",
    "\n",
    "# MMR\n",
    "k=4  # Final count\n",
    "fetch_k=20  # Candidates (3-5x of k)\n",
    "lambda_mult=0.5  # Balance (0.0-1.0)\n",
    "```\n",
    "\n",
    "### Prompt Engineering\n",
    "\n",
    "Good prompts:\n",
    "- Instruct model to cite sources\n",
    "- Tell model when to say \"I don't know\"\n",
    "- Be specific about format\n",
    "- Include context clearly\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "- Cache vector stores (done!)\n",
    "- Use batch operations for multiple docs\n",
    "- Consider async for concurrent queries\n",
    "- Monitor token usage and costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "âœ… Loaded pre-built vector stores  \n",
    "âœ… Created similarity and MMR retrievers  \n",
    "âœ… Built complete RAG chains with LCEL  \n",
    "âœ… Compared retrieval strategies  \n",
    "âœ… Established baseline performance  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **Simple RAG** is the foundation for all advanced architectures\n",
    "- **Similarity search** is fast and precise\n",
    "- **MMR** provides diversity at slight cost of speed\n",
    "- **LCEL** enables composable, modular chains\n",
    "- **Vector store caching** saves time and money\n",
    "\n",
    "### Performance Baseline\n",
    "\n",
    "These metrics serve as baseline for comparing advanced architectures:\n",
    "- **Latency:** ~1-2s per query\n",
    "- **Quality:** Good answers for in-domain queries\n",
    "- **Limitations:** \n",
    "  - No conversation memory\n",
    "  - Fixed retrieval strategy\n",
    "  - No self-correction\n",
    "  - No web fallback\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Explore **Advanced Architectures** to address limitations:\n",
    "\n",
    "- **[04_rag_with_memory.ipynb](../advanced_architectures/04_rag_with_memory.ipynb)** - Conversational RAG\n",
    "- **[05_branched_rag.ipynb](../advanced_architectures/05_branched_rag.ipynb)** - Multi-query\n",
    "- **[06_hyde.ipynb](../advanced_architectures/06_hyde.ipynb)** - Hypothetical docs\n",
    "- **[07_adaptive_rag.ipynb](../advanced_architectures/07_adaptive_rag.ipynb)** - Intelligent routing\n",
    "- **[08_corrective_rag.ipynb](../advanced_architectures/08_corrective_rag.ipynb)** - Quality checking\n",
    "- **[09_self_rag.ipynb](../advanced_architectures/09_self_rag.ipynb)** - Self-reflective\n",
    "- **[10_agentic_rag.ipynb](../advanced_architectures/10_agentic_rag.ipynb)** - Autonomous agents\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ‰ Fundamentals Complete!** You now have a solid foundation in RAG systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
