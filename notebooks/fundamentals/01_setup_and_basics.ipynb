{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Setup and Basics\n",
    "\n",
    "This notebook covers the fundamental setup for building RAG systems:\n",
    "\n",
    "1. **Environment Setup** - API keys and configuration\n",
    "2. **Document Loading** - Loading documents from web sources  \n",
    "3. **Text Splitting** - Breaking documents into chunks\n",
    "4. **Strategy Comparison** - Comparing different chunking approaches\n",
    "\n",
    "**Prerequisites:** None (start here!)\n",
    "\n",
    "**Duration:** ~10 minutes\n",
    "\n",
    "**Outputs:**\n",
    "- Loaded documents from LangChain documentation\n",
    "- Document chunks with optimized chunking strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "First, we'll set up our environment and verify that everything is configured correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ENVIRONMENT SETUP\n",
      "================================================================================\n",
      "\n",
      "âœ“ OpenAI API Key: LOADED\n",
      "  Preview: sk-proj...vIQA\n",
      "\n",
      "Project Configuration:\n",
      "--------------------------------------------------------------------------------\n",
      "environment................... dev\n",
      "debug_mode.................... False\n",
      "log_level..................... INFO\n",
      "project_root.................. /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/fundamentals/../..\n",
      "vector_store_dir.............. /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/fundamentals/../../data/vector_stores\n",
      "cache_dir..................... /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/fundamentals/../../data/cache\n",
      "openai_api_key_loaded......... True\n",
      "huggingface_api_key_loaded.... False\n",
      "langsmith_api_key_loaded...... True\n",
      "default_model................. gpt-4o-mini\n",
      "default_temperature........... 0.0\n",
      "openai_embedding_model........ text-embedding-3-small\n",
      "hf_embedding_model............ sentence-transformers/all-MiniLM-L6-v2\n",
      "chunk_size.................... 1000\n",
      "chunk_overlap................. 200\n",
      "k............................. 4\n",
      "mmr_fetch_k................... 20\n",
      "mmr_lambda.................... 0.5\n",
      "section_width................. 80\n",
      "preview_length................ 300\n",
      "\n",
      "âœ… Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Add parent directory to path to import shared module\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "# Import shared utilities\n",
    "from shared.config import verify_api_key, get_project_info, SECTION_WIDTH\n",
    "from shared.utils import print_section_header\n",
    "\n",
    "print_section_header(\"Environment Setup\")\n",
    "\n",
    "# Verify API key\n",
    "api_key_ok = verify_api_key()\n",
    "\n",
    "if not api_key_ok:\n",
    "    raise ValueError(\"OpenAI API key not configured. See README.md for setup instructions.\")\n",
    "\n",
    "# Show configuration\n",
    "print(\"\\nProject Configuration:\")\n",
    "print(\"-\" * SECTION_WIDTH)\n",
    "info = get_project_info()\n",
    "for key, value in info.items():\n",
    "    print(f\"{key:.<30} {value}\")\n",
    "\n",
    "print(\"\\nâœ… Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Loading\n",
    "\n",
    "We'll load documentation from LangChain's website using the `WebBaseLoader`. \n",
    "\n",
    "The shared module provides a convenient function that:\n",
    "- Loads documents from multiple URLs\n",
    "- Adds custom metadata (source_type, process_date, domain)\n",
    "- Returns Document objects ready for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DOCUMENT LOADING\n",
      "================================================================================\n",
      "\n",
      "Loading 4 documents from web...\n",
      "  - https://python.langchain.com/docs/use_cases/question_answering/\n",
      "  - https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
      "  - https://python.langchain.com/docs/modules/model_io/llms/\n",
      "  - https://python.langchain.com/docs/use_cases/chatbots/\n",
      "âœ“ Loaded 4 documents\n",
      "âœ“ Added custom metadata to all documents\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Sample Document:\n",
      "--------------------------------------------------------------------------------\n",
      "Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "Title: Build a RAG agent with LangChain - Docs by LangChain\n",
      "Source Type: web_documentation\n",
      "Process Date: 2025-11-12\n",
      "\n",
      "Content Preview (first 300 chars):\n",
      "Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...âŒ˜KGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with L...\n",
      "\n",
      "âœ… Successfully loaded 4 documents\n"
     ]
    }
   ],
   "source": [
    "from shared.loaders import load_langchain_docs\n",
    "from shared.utils import print_results\n",
    "\n",
    "print_section_header(\"Document Loading\")\n",
    "\n",
    "# Load documents using shared utility\n",
    "# This loads from DEFAULT_LANGCHAIN_URLS defined in shared/config.py\n",
    "docs = load_langchain_docs(verbose=True)\n",
    "\n",
    "# Show sample document\n",
    "if docs:\n",
    "    print(\"\\n\" + \"-\" * SECTION_WIDTH)\n",
    "    print(\"Sample Document:\")\n",
    "    print(\"-\" * SECTION_WIDTH)\n",
    "    sample_doc = docs[0]\n",
    "    print(f\"Source: {sample_doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"Title: {sample_doc.metadata.get('title', 'N/A')}\")\n",
    "    print(f\"Source Type: {sample_doc.metadata.get('source_type', 'N/A')}\")\n",
    "    print(f\"Process Date: {sample_doc.metadata.get('process_date', 'N/A')}\")\n",
    "    print(f\"\\nContent Preview (first 300 chars):\\n{sample_doc.page_content[:300]}...\")\n",
    "\n",
    "print(f\"\\nâœ… Successfully loaded {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Splitting\n",
    "\n",
    "Large documents must be split into smaller chunks for effective retrieval. Key parameters:\n",
    "\n",
    "- **chunk_size**: Maximum characters per chunk\n",
    "- **chunk_overlap**: Overlapping characters between chunks\n",
    "\n",
    "### Why Overlap Matters\n",
    "\n",
    "Overlap ensures context isn't lost at chunk boundaries. If a sentence is split, overlap helps preserve its meaning.\n",
    "\n",
    "### Default Strategy\n",
    "\n",
    "We use:\n",
    "- `chunk_size=1000` (good balance of context and precision)\n",
    "- `chunk_overlap=200` (preserves context at boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEXT SPLITTING\n",
      "================================================================================\n",
      "\n",
      "Splitting documents...\n",
      "  - Chunk size: 1000\n",
      "  - Chunk overlap: 200\n",
      "âœ“ Created 120 chunks\n",
      "\n",
      "  Sample chunk:\n",
      "    - Length: 839 chars\n",
      "    - Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "    - Preview: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Rea...\n",
      "\n",
      "âœ… Created 120 chunks\n",
      "   Using chunk_size=1000, overlap=200\n"
     ]
    }
   ],
   "source": [
    "from shared.loaders import split_documents\n",
    "from shared.config import DEFAULT_CHUNK_SIZE, DEFAULT_CHUNK_OVERLAP\n",
    "\n",
    "print_section_header(\"Text Splitting\")\n",
    "\n",
    "# Split documents using default settings\n",
    "chunks = split_documents(\n",
    "    docs,\n",
    "    chunk_size=DEFAULT_CHUNK_SIZE,\n",
    "    chunk_overlap=DEFAULT_CHUNK_OVERLAP,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Created {len(chunks)} chunks\")\n",
    "print(f\"   Using chunk_size={DEFAULT_CHUNK_SIZE}, overlap={DEFAULT_CHUNK_OVERLAP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Strategy Comparison\n",
    "\n",
    "Let's compare different splitting strategies to understand the trade-offs:\n",
    "\n",
    "- **Large chunks (1000/200)**: More context, fewer chunks\n",
    "- **Small chunks (500/100)**: More precision, more chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SPLITTING STRATEGY COMPARISON\n",
      "================================================================================\n",
      "\n",
      "\n",
      "=== Splitting Strategy Comparison ===\n",
      "\n",
      "Strategy        Chunk Size      Overlap         Chunks    \n",
      "------------------------------------------------------------\n",
      "1000/200        1000            200             120       \n",
      "500/100         500             100             256       \n",
      "\n",
      "ðŸ’¡ Larger chunks = more context, fewer chunks\n",
      "ðŸ’¡ Smaller chunks = more precise, more chunks\n",
      "\n",
      "ðŸ’¡ Recommendation:\n",
      "   - Use 1000/200 for context-heavy questions\n",
      "   - Use 500/100 for precise information retrieval\n",
      "   - We'll use 1000/200 (default) for this tutorial\n"
     ]
    }
   ],
   "source": [
    "from shared.loaders import compare_splitting_strategies\n",
    "\n",
    "print_section_header(\"Splitting Strategy Comparison\")\n",
    "\n",
    "# Compare two strategies\n",
    "strategies = [\n",
    "    (1000, 200),  # Default strategy - good balance\n",
    "    (500, 100),   # Smaller chunks - more precise\n",
    "]\n",
    "\n",
    "results = compare_splitting_strategies(docs, strategies, verbose=True)\n",
    "\n",
    "print(\"\\nðŸ’¡ Recommendation:\")\n",
    "print(\"   - Use 1000/200 for context-heavy questions\")\n",
    "print(\"   - Use 500/100 for precise information retrieval\")\n",
    "print(\"   - We'll use 1000/200 (default) for this tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inspect Chunks\n",
    "\n",
    "Let's examine a few chunks to understand the structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SAMPLE CHUNKS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Chunk 1:\n",
      "--------------------------------------------------------------------------------\n",
      "Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "Length: 839 characters\n",
      "Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n",
      "\n",
      "Chunk 2:\n",
      "--------------------------------------------------------------------------------\n",
      "Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "Length: 395 characters\n",
      "Content: One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These appl...\n",
      "\n",
      "Chunk 3:\n",
      "--------------------------------------------------------------------------------\n",
      "Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "Length: 558 characters\n",
      "Content: A RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\n",
      "A two-step RAG chain that uses just a single LLM call per query. This is a fast and effective meth...\n",
      "\n",
      "ðŸ“Š Summary:\n",
      "   Total chunks: 120\n",
      "   Average chunk size: 750 chars\n",
      "   Shortest chunk: 52 chars\n",
      "   Longest chunk: 1000 chars\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Sample Chunks\")\n",
    "\n",
    "# Show first 3 chunks\n",
    "for i, chunk in enumerate(chunks[:3], 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(\"-\" * SECTION_WIDTH)\n",
    "    print(f\"Source: {chunk.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"Length: {len(chunk.page_content)} characters\")\n",
    "    print(f\"Content: {chunk.page_content[:200]}...\")\n",
    "\n",
    "print(\"\\nðŸ“Š Summary:\")\n",
    "print(f\"   Total chunks: {len(chunks)}\")\n",
    "print(f\"   Average chunk size: {sum(len(c.page_content) for c in chunks) // len(chunks)} chars\")\n",
    "print(f\"   Shortest chunk: {min(len(c.page_content) for c in chunks)} chars\")\n",
    "print(f\"   Longest chunk: {max(len(c.page_content) for c in chunks)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "âœ… Set up the environment and verified API keys  \n",
    "âœ… Loaded documents from LangChain documentation  \n",
    "âœ… Split documents into chunks using optimized strategy  \n",
    "âœ… Compared different splitting strategies  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **chunk_size** controls how much context each chunk contains\n",
    "- **chunk_overlap** prevents losing information at boundaries\n",
    "- Larger chunks = more context, smaller chunks = more precision\n",
    "- Default strategy (1000/200) provides good balance\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Continue to **[02_embeddings_comparison.ipynb](02_embeddings_comparison.ipynb)** to:\n",
    "- Create embeddings with OpenAI and HuggingFace\n",
    "- Build FAISS vector stores\n",
    "- Compare performance and quality\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¾ Important:** The `chunks` variable created here will be used in the next notebook. Keep this kernel running or re-run this notebook before proceeding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
