{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Setup and Basics\n",
    "\n",
    "This notebook covers the fundamental setup for building RAG systems:\n",
    "\n",
    "1. **Environment Setup** - API keys and configuration\n",
    "2. **Document Loading** - Loading documents from web sources  \n",
    "3. **Text Splitting** - Breaking documents into chunks\n",
    "4. **Strategy Comparison** - Comparing different chunking approaches\n",
    "\n",
    "**Prerequisites:** None (start here!)\n",
    "\n",
    "**Duration:** ~10 minutes\n",
    "\n",
    "**Outputs:**\n",
    "- Loaded documents from LangChain documentation\n",
    "- Document chunks with optimized chunking strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "First, we'll set up our environment and verify that everything is configured correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add parent directory to path to import shared module\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "# Import shared utilities\n",
    "from shared.config import verify_api_key, get_project_info, SECTION_WIDTH\n",
    "from shared.utils import print_section_header\n",
    "\n",
    "print_section_header(\"Environment Setup\")\n",
    "\n",
    "# Verify API key\n",
    "api_key_ok = verify_api_key()\n",
    "\n",
    "if not api_key_ok:\n",
    "    raise ValueError(\"OpenAI API key not configured. See README.md for setup instructions.\")\n",
    "\n",
    "# Show configuration\n",
    "print(\"\\nProject Configuration:\")\n",
    "print(\"-\" * SECTION_WIDTH)\n",
    "info = get_project_info()\n",
    "for key, value in info.items():\n",
    "    print(f\"{key:.<30} {value}\")\n",
    "\n",
    "print(\"\\nâœ… Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Document Loading\n",
    "\n",
    "We'll load documentation from LangChain's website using the `WebBaseLoader`. \n",
    "\n",
    "The shared module provides a convenient function that:\n",
    "- Loads documents from multiple URLs\n",
    "- Adds custom metadata (source_type, process_date, domain)\n",
    "- Returns Document objects ready for processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.loaders import load_langchain_docs\n",
    "from shared.utils import print_results\n",
    "\n",
    "print_section_header(\"Document Loading\")\n",
    "\n",
    "# Load documents using shared utility\n",
    "# This loads from DEFAULT_LANGCHAIN_URLS defined in shared/config.py\n",
    "docs = load_langchain_docs(verbose=True)\n",
    "\n",
    "# Show sample document\n",
    "if docs:\n",
    "    print(\"\\n\" + \"-\" * SECTION_WIDTH)\n",
    "    print(\"Sample Document:\")\n",
    "    print(\"-\" * SECTION_WIDTH)\n",
    "    sample_doc = docs[0]\n",
    "    print(f\"Source: {sample_doc.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"Title: {sample_doc.metadata.get('title', 'N/A')}\")\n",
    "    print(f\"Source Type: {sample_doc.metadata.get('source_type', 'N/A')}\")\n",
    "    print(f\"Process Date: {sample_doc.metadata.get('process_date', 'N/A')}\")\n",
    "    print(f\"\\nContent Preview (first 300 chars):\\n{sample_doc.page_content[:300]}...\")\n",
    "\n",
    "print(f\"\\nâœ… Successfully loaded {len(docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Splitting\n",
    "\n",
    "Large documents must be split into smaller chunks for effective retrieval. Key parameters:\n",
    "\n",
    "- **chunk_size**: Maximum characters per chunk\n",
    "- **chunk_overlap**: Overlapping characters between chunks\n",
    "\n",
    "### Why Overlap Matters\n",
    "\n",
    "Overlap ensures context isn't lost at chunk boundaries. If a sentence is split, overlap helps preserve its meaning.\n",
    "\n",
    "### Default Strategy\n",
    "\n",
    "We use:\n",
    "- `chunk_size=1000` (good balance of context and precision)\n",
    "- `chunk_overlap=200` (preserves context at boundaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.loaders import split_documents\n",
    "from shared.config import DEFAULT_CHUNK_SIZE, DEFAULT_CHUNK_OVERLAP\n",
    "\n",
    "print_section_header(\"Text Splitting\")\n",
    "\n",
    "# Split documents using default settings\n",
    "chunks = split_documents(\n",
    "    docs,\n",
    "    chunk_size=DEFAULT_CHUNK_SIZE,\n",
    "    chunk_overlap=DEFAULT_CHUNK_OVERLAP,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Created {len(chunks)} chunks\")\n",
    "print(f\"   Using chunk_size={DEFAULT_CHUNK_SIZE}, overlap={DEFAULT_CHUNK_OVERLAP}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Strategy Comparison\n",
    "\n",
    "Let's compare different splitting strategies to understand the trade-offs:\n",
    "\n",
    "- **Large chunks (1000/200)**: More context, fewer chunks\n",
    "- **Small chunks (500/100)**: More precision, more chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.loaders import compare_splitting_strategies\n",
    "\n",
    "print_section_header(\"Splitting Strategy Comparison\")\n",
    "\n",
    "# Compare two strategies\n",
    "strategies = [\n",
    "    (1000, 200),  # Default strategy - good balance\n",
    "    (500, 100),   # Smaller chunks - more precise\n",
    "]\n",
    "\n",
    "results = compare_splitting_strategies(docs, strategies, verbose=True)\n",
    "\n",
    "print(\"\\nðŸ’¡ Recommendation:\")\n",
    "print(\"   - Use 1000/200 for context-heavy questions\")\n",
    "print(\"   - Use 500/100 for precise information retrieval\")\n",
    "print(\"   - We'll use 1000/200 (default) for this tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Inspect Chunks\n",
    "\n",
    "Let's examine a few chunks to understand the structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Sample Chunks\")\n",
    "\n",
    "# Show first 3 chunks\n",
    "for i, chunk in enumerate(chunks[:3], 1):\n",
    "    print(f\"\\nChunk {i}:\")\n",
    "    print(\"-\" * SECTION_WIDTH)\n",
    "    print(f\"Source: {chunk.metadata.get('source', 'N/A')}\")\n",
    "    print(f\"Length: {len(chunk.page_content)} characters\")\n",
    "    print(f\"Content: {chunk.page_content[:200]}...\")\n",
    "\n",
    "print(\"\\nðŸ“Š Summary:\")\n",
    "print(f\"   Total chunks: {len(chunks)}\")\n",
    "print(f\"   Average chunk size: {sum(len(c.page_content) for c in chunks) // len(chunks)} chars\")\n",
    "print(f\"   Shortest chunk: {min(len(c.page_content) for c in chunks)} chars\")\n",
    "print(f\"   Longest chunk: {max(len(c.page_content) for c in chunks)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "âœ… Set up the environment and verified API keys  \n",
    "âœ… Loaded documents from LangChain documentation  \n",
    "âœ… Split documents into chunks using optimized strategy  \n",
    "âœ… Compared different splitting strategies  \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- **chunk_size** controls how much context each chunk contains\n",
    "- **chunk_overlap** prevents losing information at boundaries\n",
    "- Larger chunks = more context, smaller chunks = more precision\n",
    "- Default strategy (1000/200) provides good balance\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Continue to **[02_embeddings_comparison.ipynb](02_embeddings_comparison.ipynb)** to:\n",
    "- Create embeddings with OpenAI and HuggingFace\n",
    "- Build FAISS vector stores\n",
    "- Compare performance and quality\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸ’¾ Important:** The `chunks` variable created here will be used in the next notebook. Keep this kernel running or re-run this notebook before proceeding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
