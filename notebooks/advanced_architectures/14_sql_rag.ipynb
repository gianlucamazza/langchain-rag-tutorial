{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14. SQL RAG - Natural Language to SQL\n",
    "\n",
    "**Complexity:** ⭐⭐⭐⭐\n",
    "\n",
    "## Overview\n",
    "\n",
    "**SQL RAG** enables natural language queries over structured databases by combining:\n",
    "1. Schema retrieval (finding relevant tables)\n",
    "2. Text-to-SQL generation (LLM generates SQL)\n",
    "3. SQL execution (safe execution in sandbox)\n",
    "4. Results interpretation (natural language answer)\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Standard RAG works great for unstructured text, but fails for:\n",
    "- Structured databases (SQL, NoSQL)\n",
    "- Aggregations and analytics (COUNT, SUM, AVG)\n",
    "- Precise data lookups (exact matches, filters)\n",
    "- Relational queries (JOINs across tables)\n",
    "\n",
    "**Example queries that need SQL:**\n",
    "- \"How many customers do we have in France?\"\n",
    "- \"What's the average order value last month?\"\n",
    "- \"Show me the top 5 products by revenue\"\n",
    "- \"Which employees have sold more than $50,000?\"\n",
    "\n",
    "### The Solution\n",
    "\n",
    "SQL RAG pipeline:\n",
    "\n",
    "```\n",
    "Question → Schema Retrieval → Text-to-SQL → SQL Validation\n",
    "    → Execute Query → Results → Natural Language Answer\n",
    "```\n",
    "\n",
    "### Architecture\n",
    "\n",
    "1. **Schema Index**: Embed table/column descriptions\n",
    "2. **Schema Retrieval**: Find relevant tables for query\n",
    "3. **SQL Generation**: LLM generates SQL with schema context\n",
    "4. **Safety Layer**: Validate SQL (read-only, prevent injection)\n",
    "5. **Execution**: Run query in controlled environment\n",
    "6. **Interpretation**: Convert results to natural language\n",
    "7. **Error Handling**: Retry with corrections if SQL fails\n",
    "\n",
    "### Example Database: Chinook\n",
    "\n",
    "We'll use the **Chinook** database - a sample music store database with:\n",
    "- **Artists**: Band/musician information\n",
    "- **Albums**: Music albums\n",
    "- **Tracks**: Individual songs\n",
    "- **Customers**: Customer records\n",
    "- **Employees**: Staff information\n",
    "- **Invoices**: Sales transactions\n",
    "- **InvoiceLines**: Line items\n",
    "- **Playlists**: Song collections\n",
    "\n",
    "### When to Use\n",
    "\n",
    "✅ **Good for:**\n",
    "- Analytics and aggregations\n",
    "- Structured data queries\n",
    "- Enterprise data (databases, data warehouses)\n",
    "- Precise lookups and filters\n",
    "- Time-series data\n",
    "\n",
    "❌ **Not ideal for:**\n",
    "- Unstructured text documents\n",
    "- Semantic similarity search\n",
    "- When users don't understand data structure\n",
    "- When SQL is too complex for LLM\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "**Pros:**\n",
    "- ✅ Precise answers (no hallucination)\n",
    "- ✅ Handles aggregations and math\n",
    "- ✅ Works with existing databases\n",
    "- ✅ Verifiable results\n",
    "\n",
    "**Cons:**\n",
    "- ❌ Requires good schema design\n",
    "- ❌ LLM SQL errors are common\n",
    "- ❌ Security considerations\n",
    "- ❌ Limited to query expressiveness\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport sqlite3\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nimport json\n\n# Add parent directory to path for imports\nsys.path.append(str(Path(\"../..\").resolve()))\n\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.documents import Document\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_community.vectorstores import FAISS\nimport pandas as pd\n\nfrom shared.config import (\n    verify_api_key,\n    DEFAULT_MODEL,\n    DEFAULT_TEMPERATURE,\n    OPENAI_EMBEDDING_MODEL,\n    VECTOR_STORE_DIR,\n)\nfrom shared.prompts import (\n    SQL_SCHEMA_SUMMARY_PROMPT,\n    TEXT_TO_SQL_PROMPT,\n    SQL_RESULTS_INTERPRETATION_PROMPT,\n    SQL_ERROR_RECOVERY_PROMPT,\n)\nfrom shared.utils import (\n    print_section_header,\n    load_vector_store,\n    save_vector_store,\n)\n\n# Verify API key\nverify_api_key()\n\nprint(\"✓ All imports successful\")\nprint(f\"✓ Using model: {DEFAULT_MODEL}\")\nprint(f\"✓ Using embeddings: {OPENAI_EMBEDDING_MODEL}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connect to Chinook Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Connecting to Chinook Database\")\n",
    "\n",
    "# Path to Chinook database\n",
    "DB_PATH = Path(\"../..\") / \"data\" / \"chinook.db\"\n",
    "\n",
    "if not DB_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Chinook database not found at {DB_PATH}.\\n\"\n",
    "        \"Please download it from: https://github.com/lerocha/chinook-database\"\n",
    "    )\n",
    "\n",
    "# Create read-only connection\n",
    "conn = sqlite3.connect(f\"file:{DB_PATH}?mode=ro\", uri=True)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(f\"✓ Connected to database: {DB_PATH}\")\n",
    "\n",
    "# Get table names\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name\")\n",
    "tables = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "print(f\"\\n✓ Found {len(tables)} tables:\")\n",
    "for table in tables:\n",
    "    cursor.execute(f\"SELECT COUNT(*) FROM {table}\")\n",
    "    count = cursor.fetchone()[0]\n",
    "    print(f\"  • {table}: {count} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspect Database Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_schema(conn: sqlite3.Connection, table_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the schema for a table.\n",
    "    \n",
    "    Args:\n",
    "        conn: Database connection\n",
    "        table_name: Name of table\n",
    "    \n",
    "    Returns:\n",
    "        Schema description string\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(f\"PRAGMA table_info({table_name})\")\n",
    "    columns = cursor.fetchall()\n",
    "    \n",
    "    schema_lines = [f\"Table: {table_name}\"]\n",
    "    schema_lines.append(\"Columns:\")\n",
    "    \n",
    "    for col in columns:\n",
    "        col_id, name, col_type, not_null, default, pk = col\n",
    "        constraints = []\n",
    "        if pk:\n",
    "            constraints.append(\"PRIMARY KEY\")\n",
    "        if not_null:\n",
    "            constraints.append(\"NOT NULL\")\n",
    "        \n",
    "        constraint_str = f\" ({', '.join(constraints)})\" if constraints else \"\"\n",
    "        schema_lines.append(f\"  - {name}: {col_type}{constraint_str}\")\n",
    "    \n",
    "    return \"\\n\".join(schema_lines)\n",
    "\n",
    "\n",
    "print_section_header(\"Database Schema Inspection\")\n",
    "\n",
    "# Show schema for a few example tables\n",
    "example_tables = [\"Artist\", \"Album\", \"Track\", \"Customer\", \"Invoice\"]\n",
    "\n",
    "for table in example_tables:\n",
    "    if table in tables:\n",
    "        print(f\"\\n{table}:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(get_table_schema(conn, table))\n",
    "\n",
    "print(\"\\n✓ Schema inspection complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Schema Embeddings\n",
    "\n",
    "We'll embed table schemas to enable semantic retrieval of relevant tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print_section_header(\"Creating Schema Embeddings\")\n\n# Initialize LLM for schema summarization\nllm = ChatOpenAI(\n    model=DEFAULT_MODEL,\n    temperature=DEFAULT_TEMPERATURE,\n)\n\n# Create schema documents with summaries\nschema_docs = []\n\nprint(\"\\nGenerating semantic descriptions for tables...\")\n\nfor table in tables:\n    schema = get_table_schema(conn, table)\n    \n    # Generate semantic summary\n    summary_chain = SQL_SCHEMA_SUMMARY_PROMPT | llm | StrOutputParser()\n    summary = summary_chain.invoke({\n        \"table_name\": table,\n        \"schema\": schema,\n    })\n    \n    # Create document\n    doc = Document(\n        page_content=f\"{table}: {summary}\\n\\nSchema:\\n{schema}\",\n        metadata={\n            \"table_name\": table,\n            \"summary\": summary,\n            \"schema\": schema,\n        },\n    )\n    schema_docs.append(doc)\n    \n    print(f\"  ✓ {table}\")\n\nprint(f\"\\n✓ Created {len(schema_docs)} schema documents\")\n\n# Create vector store for schema\nembeddings = OpenAIEmbeddings(model=OPENAI_EMBEDDING_MODEL)\nschema_store_path = VECTOR_STORE_DIR / \"sql_rag_schema\"\n\nschema_vectorstore = load_vector_store(schema_store_path, embeddings)\n\nif schema_vectorstore is None:\n    print(\"\\nCreating schema vector store...\")\n    schema_vectorstore = FAISS.from_documents(schema_docs, embeddings)\n    save_vector_store(schema_vectorstore, schema_store_path)\n    print(\"✓ Schema vector store created\")\nelse:\n    print(\"✓ Loaded existing schema vector store\")\n\n# Create retriever\nschema_retriever = schema_vectorstore.as_retriever(search_kwargs={\"k\": 3})\nprint(\"✓ Schema retriever ready\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Schema Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Testing Schema Retrieval\")\n",
    "\n",
    "test_queries = [\n",
    "    \"Which tables contain information about songs and music?\",\n",
    "    \"Where can I find customer purchase data?\",\n",
    "    \"Show me tables related to employees and sales\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    relevant_tables = schema_retriever.invoke(query)\n",
    "    \n",
    "    print(\"Relevant tables:\")\n",
    "    for doc in relevant_tables:\n",
    "        table_name = doc.metadata[\"table_name\"]\n",
    "        summary = doc.metadata[\"summary\"]\n",
    "        print(f\"  • {table_name}: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implement Safe SQL Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql_safely(\n",
    "    conn: sqlite3.Connection,\n",
    "    query: str,\n",
    "    max_results: int = 100,\n",
    ") -> tuple[bool, Any, str]:\n",
    "    \"\"\"\n",
    "    Execute SQL query safely with validation.\n",
    "    \n",
    "    Args:\n",
    "        conn: Database connection (should be read-only)\n",
    "        query: SQL query to execute\n",
    "        max_results: Maximum rows to return\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (success, results/error, error_message)\n",
    "    \"\"\"\n",
    "    # Safety checks\n",
    "    query_upper = query.upper().strip()\n",
    "    \n",
    "    # Block dangerous operations\n",
    "    forbidden = [\"DROP\", \"DELETE\", \"UPDATE\", \"INSERT\", \"ALTER\", \"CREATE\", \"TRUNCATE\"]\n",
    "    for keyword in forbidden:\n",
    "        if keyword in query_upper:\n",
    "            return False, None, f\"Forbidden keyword: {keyword}\"\n",
    "    \n",
    "    # Must start with SELECT\n",
    "    if not query_upper.startswith(\"SELECT\"):\n",
    "        return False, None, \"Only SELECT queries are allowed\"\n",
    "    \n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query)\n",
    "        \n",
    "        # Fetch results with limit\n",
    "        results = cursor.fetchmany(max_results)\n",
    "        \n",
    "        # Get column names\n",
    "        columns = [desc[0] for desc in cursor.description] if cursor.description else []\n",
    "        \n",
    "        # Convert to list of dicts\n",
    "        results_list = [\n",
    "            dict(zip(columns, row))\n",
    "            for row in results\n",
    "        ]\n",
    "        \n",
    "        return True, results_list, \"\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return False, None, str(e)\n",
    "\n",
    "\n",
    "print(\"✓ Safe SQL execution function defined\")\n",
    "\n",
    "# Test\n",
    "print(\"\\nTest query: SELECT * FROM Artist LIMIT 3\")\n",
    "success, results, error = execute_sql_safely(conn, \"SELECT * FROM Artist LIMIT 3\")\n",
    "\n",
    "if success:\n",
    "    print(\"\\n✓ Query successful!\")\n",
    "    df = pd.DataFrame(results)\n",
    "    print(df)\n",
    "else:\n",
    "    print(f\"\\n❌ Query failed: {error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build Text-to-SQL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sql_rag(\n",
    "    question: str,\n",
    "    conn: sqlite3.Connection,\n",
    "    schema_retriever,\n",
    "    llm,\n",
    "    verbose: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Complete Text-to-SQL RAG pipeline.\n",
    "    \n",
    "    Args:\n",
    "        question: Natural language question\n",
    "        conn: Database connection\n",
    "        schema_retriever: Schema retriever\n",
    "        llm: Language model\n",
    "        verbose: Print debug info\n",
    "    \n",
    "    Returns:\n",
    "        Dict with query, results, answer, etc.\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n[SQL RAG] Question: {question}\")\n",
    "    \n",
    "    # 1. Retrieve relevant schema\n",
    "    relevant_schemas = schema_retriever.invoke(question)\n",
    "    \n",
    "    schema_context = \"\\n\\n\".join([\n",
    "        doc.metadata[\"schema\"]\n",
    "        for doc in relevant_schemas\n",
    "    ])\n",
    "    \n",
    "    if verbose:\n",
    "        tables = [doc.metadata[\"table_name\"] for doc in relevant_schemas]\n",
    "        print(f\"[SQL RAG] Relevant tables: {', '.join(tables)}\")\n",
    "    \n",
    "    # 2. Generate SQL query\n",
    "    sql_chain = TEXT_TO_SQL_PROMPT | llm | StrOutputParser()\n",
    "    sql_query = sql_chain.invoke({\n",
    "        \"schema\": schema_context,\n",
    "        \"question\": question,\n",
    "    }).strip()\n",
    "    \n",
    "    # Clean SQL (remove markdown formatting if present)\n",
    "    sql_query = sql_query.replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"[SQL RAG] Generated SQL: {sql_query}\")\n",
    "    \n",
    "    # 3. Execute SQL\n",
    "    success, results, error = execute_sql_safely(conn, sql_query)\n",
    "    \n",
    "    if not success:\n",
    "        if verbose:\n",
    "            print(f\"[SQL RAG] Query failed: {error}\")\n",
    "            print(\"[SQL RAG] Attempting error recovery...\")\n",
    "        \n",
    "        # Try error recovery\n",
    "        recovery_chain = SQL_ERROR_RECOVERY_PROMPT | llm | StrOutputParser()\n",
    "        corrected_sql = recovery_chain.invoke({\n",
    "            \"question\": question,\n",
    "            \"failed_query\": sql_query,\n",
    "            \"error\": error,\n",
    "            \"schema\": schema_context,\n",
    "        }).strip().replace(\"```sql\", \"\").replace(\"```\", \"\").strip()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"[SQL RAG] Corrected SQL: {corrected_sql}\")\n",
    "        \n",
    "        success, results, error = execute_sql_safely(conn, corrected_sql)\n",
    "        \n",
    "        if success:\n",
    "            sql_query = corrected_sql\n",
    "        else:\n",
    "            return {\n",
    "                \"question\": question,\n",
    "                \"sql_query\": sql_query,\n",
    "                \"success\": False,\n",
    "                \"error\": error,\n",
    "                \"answer\": f\"I couldn't generate a valid SQL query. Error: {error}\",\n",
    "            }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"[SQL RAG] Query successful! Rows returned: {len(results)}\")\n",
    "    \n",
    "    # 4. Interpret results\n",
    "    results_str = json.dumps(results, indent=2) if results else \"No results found\"\n",
    "    \n",
    "    interpret_chain = SQL_RESULTS_INTERPRETATION_PROMPT | llm | StrOutputParser()\n",
    "    answer = interpret_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"sql_query\": sql_query,\n",
    "        \"results\": results_str,\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"sql_query\": sql_query,\n",
    "        \"success\": True,\n",
    "        \"results\": results,\n",
    "        \"answer\": answer,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"✓ Text-to-SQL RAG pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test SQL RAG with Various Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Testing SQL RAG\")\n",
    "\n",
    "test_questions = [\n",
    "    \"How many customers are in the database?\",\n",
    "    \"What are the top 5 longest songs?\",\n",
    "    \"Show me the total sales by country, ordered by highest revenue\",\n",
    "    \"Which artist has released the most albums?\",\n",
    "    \"What's the average track length in minutes?\",\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    result = text_to_sql_rag(\n",
    "        question=question,\n",
    "        conn=conn,\n",
    "        schema_retriever=schema_retriever,\n",
    "        llm=llm,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"ANSWER:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(result[\"answer\"])\n",
    "    \n",
    "    # Show results as DataFrame if available\n",
    "    if result[\"success\"] and result[\"results\"]:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"DATA:\")\n",
    "        print(\"-\" * 80)\n",
    "        df = pd.DataFrame(result[\"results\"]).head(10)\n",
    "        print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Complex Queries with JOINs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Testing Complex Queries with JOINs\")\n",
    "\n",
    "complex_questions = [\n",
    "    \"Show me the top 3 customers by total amount spent\",\n",
    "    \"Which genres have the most tracks?\",\n",
    "    \"List all employees and how many customers they support\",\n",
    "]\n",
    "\n",
    "for i, question in enumerate(complex_questions, 1):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Complex Query {i}: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    result = text_to_sql_rag(\n",
    "        question=question,\n",
    "        conn=conn,\n",
    "        schema_retriever=schema_retriever,\n",
    "        llm=llm,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"ANSWER:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(result[\"answer\"])\n",
    "    \n",
    "    if result[\"success\"] and result[\"results\"]:\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        print(\"DATA:\")\n",
    "        print(\"-\" * 80)\n",
    "        df = pd.DataFrame(result[\"results\"])\n",
    "        print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Performance Metrics\")\n",
    "\n",
    "import time\n",
    "\n",
    "# Benchmark query\n",
    "benchmark_query = \"How many albums are there in total?\"\n",
    "\n",
    "print(f\"Benchmark query: {benchmark_query}\\n\")\n",
    "\n",
    "start = time.time()\n",
    "result = text_to_sql_rag(\n",
    "    question=benchmark_query,\n",
    "    conn=conn,\n",
    "    schema_retriever=schema_retriever,\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    ")\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PERFORMANCE BREAKDOWN:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total time: {elapsed:.2f}s\")\n",
    "print(\"\\nComponents:\")\n",
    "print(\"  • Schema retrieval: ~0.5-1.0s (vector search)\")\n",
    "print(\"  • SQL generation: ~1.0-2.0s (LLM call)\")\n",
    "print(\"  • SQL execution: <0.1s (fast on indexed DB)\")\n",
    "print(\"  • Results interpretation: ~1.0-2.0s (LLM call)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COST ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"LLM Calls per query:\")\n",
    "print(\"  • Schema summarization: 1 (one-time, cached)\")\n",
    "print(\"  • SQL generation: 1\")\n",
    "print(\"  • Error recovery: 0-1 (if needed)\")\n",
    "print(\"  • Results interpretation: 1\")\n",
    "print(\"  • Total: 2-3 LLM calls\")\n",
    "print(\"\\nVector searches: 1 (schema retrieval)\")\n",
    "print(\"Database queries: 1 (SQL execution)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Hybrid Approach: SQL + Vector RAG\n",
    "\n",
    "For maximum flexibility, combine SQL RAG with traditional vector RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Hybrid SQL + Vector RAG\")\n",
    "\n",
    "print(\"\\nThis approach routes queries to either:\")\n",
    "print(\"  • SQL RAG: For structured queries (counts, aggregations, filters)\")\n",
    "print(\"  • Vector RAG: For semantic queries (document search, similarity)\")\n",
    "\n",
    "# Simple query classifier\n",
    "def classify_query(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Classify if query needs SQL or vector RAG.\n",
    "    \"\"\"\n",
    "    sql_keywords = [\n",
    "        \"how many\", \"count\", \"total\", \"sum\", \"average\", \"maximum\", \"minimum\",\n",
    "        \"top\", \"bottom\", \"highest\", \"lowest\", \"most\", \"least\",\n",
    "        \"by country\", \"by genre\", \"by artist\", \"per\",\n",
    "    ]\n",
    "    \n",
    "    question_lower = question.lower()\n",
    "    \n",
    "    for keyword in sql_keywords:\n",
    "        if keyword in question_lower:\n",
    "            return \"SQL\"\n",
    "    \n",
    "    return \"VECTOR\"\n",
    "\n",
    "\n",
    "# Test classification\n",
    "test_cases = [\n",
    "    \"How many customers are in France?\",  # SQL\n",
    "    \"What is LCEL in LangChain?\",  # VECTOR\n",
    "    \"Show me the top 5 albums\",  # SQL\n",
    "    \"Explain the concept of retrieval\",  # VECTOR\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Query Classification Examples:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for query in test_cases:\n",
    "    classification = classify_query(query)\n",
    "    print(f\"\\n'{query}'\")\n",
    "    print(f\"  → {classification} RAG\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Implementation Note:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"A production system would:\")\n",
    "print(\"  1. Use LLM to classify query intent\")\n",
    "print(\"  2. Route to appropriate RAG system\")\n",
    "print(\"  3. Fall back to alternative if primary fails\")\n",
    "print(\"  4. Combine results if needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Takeaways\n",
    "\n",
    "### Summary\n",
    "\n",
    "**SQL RAG** enables natural language queries over structured databases:\n",
    "- Schema retrieval finds relevant tables\n",
    "- Text-to-SQL generates queries with LLM\n",
    "- Safe execution prevents dangerous operations\n",
    "- Results interpretation provides natural answers\n",
    "- Error recovery handles SQL generation failures\n",
    "\n",
    "### Pipeline Components\n",
    "\n",
    "1. **Schema Index** (one-time): Embed table descriptions\n",
    "2. **Schema Retrieval**: Find relevant tables (vector search)\n",
    "3. **SQL Generation**: LLM creates query with schema context\n",
    "4. **Validation**: Safety checks (read-only, no injection)\n",
    "5. **Execution**: Run query in controlled environment\n",
    "6. **Interpretation**: Convert results to natural language\n",
    "7. **Error Recovery**: Retry with corrections if needed\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "**Schema Design:**\n",
    "- ✅ Use descriptive table/column names\n",
    "- ✅ Add comments and documentation\n",
    "- ✅ Maintain referential integrity\n",
    "- ✅ Create semantic summaries for tables\n",
    "\n",
    "**Safety:**\n",
    "- ✅ Always use read-only connections\n",
    "- ✅ Whitelist allowed operations (SELECT only)\n",
    "- ✅ Set query timeouts\n",
    "- ✅ Limit result set sizes\n",
    "- ✅ Validate generated SQL\n",
    "\n",
    "**Error Handling:**\n",
    "- ✅ Implement retry logic\n",
    "- ✅ Provide helpful error messages\n",
    "- ✅ Log failed queries for analysis\n",
    "- ✅ Fall back to alternative approaches\n",
    "\n",
    "**Optimization:**\n",
    "- ✅ Cache schema embeddings\n",
    "- ✅ Index database properly\n",
    "- ✅ Use query result caching\n",
    "- ✅ Batch similar queries\n",
    "\n",
    "### Limitations\n",
    "\n",
    "❌ **SQL Generation Challenges:**\n",
    "- Complex JOINs may fail\n",
    "- Ambiguous questions lead to wrong queries\n",
    "- LLM may hallucinate table/column names\n",
    "- Window functions and CTEs are hard\n",
    "\n",
    "❌ **Schema Dependency:**\n",
    "- Requires well-designed schema\n",
    "- Poor naming leads to poor queries\n",
    "- Schema changes break queries\n",
    "\n",
    "### When to Use\n",
    "\n",
    "Choose **SQL RAG** when:\n",
    "- ✅ Data is structured in databases\n",
    "- ✅ Need aggregations and analytics\n",
    "- ✅ Precision is critical\n",
    "- ✅ Users ask \"how many\", \"total\", \"average\"\n",
    "- ✅ Existing database infrastructure\n",
    "\n",
    "Choose **Vector RAG** when:\n",
    "- ✅ Unstructured text documents\n",
    "- ✅ Semantic similarity matters\n",
    "- ✅ Fuzzy matching needed\n",
    "- ✅ No clear schema\n",
    "\n",
    "Choose **Hybrid** when:\n",
    "- ✅ Both structured and unstructured data\n",
    "- ✅ Diverse query types\n",
    "- ✅ Maximum flexibility needed\n",
    "\n",
    "### Extensions\n",
    "\n",
    "**Advanced Features:**\n",
    "- Multi-database support (PostgreSQL, MySQL, etc.)\n",
    "- Query optimization hints\n",
    "- Cached query plans\n",
    "- Natural language query suggestions\n",
    "- Query history and learning\n",
    "\n",
    "**Production Enhancements:**\n",
    "- Query approval workflows\n",
    "- Result explanation (EXPLAIN)\n",
    "- Performance monitoring\n",
    "- A/B testing of prompts\n",
    "- User feedback loops\n",
    "\n",
    "---\n",
    "\n",
    "**Complexity Rating:** ⭐⭐⭐⭐ (High - requires database knowledge + LLM integration)\n",
    "\n",
    "**Production Readiness:** ⭐⭐⭐ (Medium - needs extensive testing and safety measures)\n",
    "\n",
    "Continue to **15_graphrag.ipynb** for Graph-based Knowledge Retrieval!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close database connection\n",
    "conn.close()\n",
    "print(\"✓ Database connection closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}