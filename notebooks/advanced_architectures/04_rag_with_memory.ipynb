{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - RAG with Memory (Conversational RAG)\n",
    "\n",
    "**Architecture:** RAG with Conversational Memory\n",
    "\n",
    "**Complexity:** â­â­\n",
    "\n",
    "**Use Cases:**\n",
    "- Chatbots and conversational AI\n",
    "- Customer support systems  \n",
    "- Interactive Q&A sessions\n",
    "\n",
    "**Key Feature:** Maintains chat history to handle follow-up questions and anaphoric references.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "User: \"What is RAG?\"\n",
    "Bot: \"RAG is Retrieval-Augmented Generation...\"\n",
    "User: \"What are its main components?\"  â† References \"RAG\" from context\n",
    "Bot: \"The main components of RAG are...\"  â† Understands reference\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SETUP: RAG WITH MEMORY\n",
      "================================================================================\n",
      "\n",
      "âœ“ Loaded vector store from /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/advanced_architectures/../../data/vector_stores/openai_embeddings\n",
      "\n",
      "âœ… Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from shared.config import OPENAI_VECTOR_STORE_PATH, DEFAULT_MODEL, DEFAULT_K\n",
    "from shared.utils import load_vector_store, print_section_header, format_docs\n",
    "from shared.prompts import MEMORY_RAG_PROMPT\n",
    "\n",
    "print_section_header(\"Setup: RAG with Memory\")\n",
    "\n",
    "# Load vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = load_vector_store(OPENAI_VECTOR_STORE_PATH, embeddings)\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": DEFAULT_K})\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=DEFAULT_MODEL, temperature=0)\n",
    "\n",
    "print(\"\\nâœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory Setup\n",
    "\n",
    "We'll use `RunnableWithMessageHistory` to add conversational memory to our RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MEMORY CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      "âœ“ Memory store configured\n",
      "âœ“ Conversational chain created with memory\n",
      "\n",
      "ðŸ’¡ Chat history is maintained per session_id\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print_section_header(\"Memory Configuration\")\n",
    "\n",
    "# Session store (in-memory for demo)\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "print(\"âœ“ Memory store configured\")\n",
    "\n",
    "# Build base chain\n",
    "# The retriever needs just the \"input\" string, not the whole dict\n",
    "base_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=lambda x: format_docs(retriever.invoke(x[\"input\"]))\n",
    "    )\n",
    "    | MEMORY_RAG_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Wrap with memory\n",
    "conversational_chain = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "print(\"âœ“ Conversational chain created with memory\")\n",
    "print(\"\\nðŸ’¡ Chat history is maintained per session_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Conversational Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONVERSATIONAL TEST\n",
      "================================================================================\n",
      "\n",
      "User: What is RAG?\n",
      "\n",
      "Bot: RAG stands for Retrieval-Augmented Generation. It is a process that combines retrieval and generation techniques to enhance the performance of language models. In a RAG system, a user query is processed in two main steps: first, relevant data is retrieved from an indexed source, and then this data is passed to a language model (LLM) to generate a response. This approach allows for more accurate and contextually relevant answers by leveraging external information.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "User: What are its main components?\n",
      "\n",
      "Bot: The main components of Retrieval-Augmented Generation (RAG) typically include:\n",
      "\n",
      "1. **Retrieval Component**: This part is responsible for fetching relevant documents or pieces of information from a knowledge base or database based on the user's query. It often uses techniques like vector search or traditional keyword-based search.\n",
      "\n",
      "2. **Generation Component**: After retrieving the relevant information, this component uses a language model to generate a coherent and contextually appropriate response based on the retrieved data.\n",
      "\n",
      "3. **Indexing**: A system for organizing and storing the documents or data that can be retrieved efficiently. This may involve creating embeddings or other structures to facilitate quick access.\n",
      "\n",
      "4. **Query Processing**: This involves understanding and processing the user's input to formulate effective queries for the retrieval component.\n",
      "\n",
      "5. **Integration Layer**: This component connects the retrieval and generation components, ensuring that the output from the retrieval phase is effectively utilized in the generation phase.\n",
      "\n",
      "These components work together to enhance the quality and relevance of the generated responses by leveraging external knowledge.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "User: How do I implement it?\n",
      "\n",
      "Bot: To implement a Retrieval-Augmented Generation (RAG) system, you can follow these general steps:\n",
      "\n",
      "1. **Set Up Your Environment**:\n",
      "   - Choose a programming language and framework (e.g., Python with libraries like Hugging Face Transformers, LangChain, etc.).\n",
      "   - Install necessary libraries for both retrieval and generation.\n",
      "\n",
      "2. **Data Collection**:\n",
      "   - Gather a dataset or knowledge base that your RAG system will use for retrieval. This could be documents, articles, or any relevant information.\n",
      "\n",
      "3. **Indexing**:\n",
      "   - Create an index of your data to facilitate efficient retrieval. You can use tools like Elasticsearch, FAISS, or create embeddings using models like Sentence Transformers.\n",
      "\n",
      "4. **Implement the Retrieval Component**:\n",
      "   - Develop a retrieval mechanism that takes a user query and fetches relevant documents from your indexed data. This can involve using vector similarity search or keyword-based search.\n",
      "\n",
      "5. **Implement the Generation Component**:\n",
      "   - Use a pre-trained language model (like GPT, BERT, etc.) to generate responses. You can fine-tune the model if necessary, depending on your specific use case.\n",
      "\n",
      "6. **Connect Retrieval and Generation**:\n",
      "   - Create a pipeline where the output from the retrieval component is fed into the generation component. This typically involves formatting the retrieved information in a way that the language model can understand.\n",
      "\n",
      "7. **Testing and Evaluation**:\n",
      "   - Test your RAG system with various queries to evaluate its performance. Adjust the retrieval and generation components as needed based on the results.\n",
      "\n",
      "8. **Deployment**:\n",
      "   - Once satisfied with the implementation, deploy your RAG application. You can use cloud services or local servers depending on your requirements.\n",
      "\n",
      "9. **Iterate and Improve**:\n",
      "   - Continuously monitor the performance of your RAG system and make improvements based on user feedback and new data.\n",
      "\n",
      "By following these steps, you can implement a RAG system tailored to your specific needs.\n",
      "\n",
      "âœ… Conversation maintained context successfully!\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Conversational Test\")\n",
    "\n",
    "session_id = \"user_123\"\n",
    "\n",
    "# First question\n",
    "print(\"User: What is RAG?\\n\")\n",
    "response1 = conversational_chain.invoke(\n",
    "    {\"input\": \"What is RAG?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(f\"Bot: {response1}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Follow-up with anaphoric reference\n",
    "print(\"\\nUser: What are its main components?\\n\")\n",
    "response2 = conversational_chain.invoke(\n",
    "    {\"input\": \"What are its main components?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(f\"Bot: {response2}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Another follow-up\n",
    "print(\"\\nUser: How do I implement it?\\n\")\n",
    "response3 = conversational_chain.invoke(\n",
    "    {\"input\": \"How do I implement it?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(f\"Bot: {response3}\")\n",
    "\n",
    "print(\"\\nâœ… Conversation maintained context successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. View Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CHAT HISTORY\n",
      "================================================================================\n",
      "\n",
      "Messages in session 'user_123': 6\n",
      "\n",
      "1. User: What is RAG?\n",
      "\n",
      "2. Bot: RAG stands for Retrieval-Augmented Generation. It is a process that combines retrieval and generation techniques to enhance the performance of languag...\n",
      "\n",
      "3. User: What are its main components?\n",
      "\n",
      "4. Bot: The main components of Retrieval-Augmented Generation (RAG) typically include:\n",
      "\n",
      "1. **Retrieval Component**: This part is responsible for fetching rele...\n",
      "\n",
      "5. User: How do I implement it?\n",
      "\n",
      "6. Bot: To implement a Retrieval-Augmented Generation (RAG) system, you can follow these general steps:\n",
      "\n",
      "1. **Set Up Your Environment**:\n",
      "   - Choose a program...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Chat History\")\n",
    "\n",
    "history = store[session_id]\n",
    "print(f\"Messages in session '{session_id}': {len(history.messages)}\\n\")\n",
    "\n",
    "for i, msg in enumerate(history.messages, 1):\n",
    "    role = \"User\" if msg.type == \"human\" else \"Bot\"\n",
    "    content = msg.content[:150] + \"...\" if len(msg.content) > 150 else msg.content\n",
    "    print(f\"{i}. {role}: {content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison: Memory vs No Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARISON: MEMORY VS NO MEMORY\n",
      "================================================================================\n",
      "\n",
      "Query (after discussing RAG): 'What are the advantages of using it?'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Simple RAG - NO MEMORY]\n",
      "The advantages of using the LLM (Language Model) as described in the context include:\n",
      "\n",
      "1. **Search only when needed**: The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches.\n",
      "2. **Contextual search queries**: The LLM crafts its own queries that incorpora\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Memory RAG - WITH MEMORY]\n",
      "The advantages of using Retrieval-Augmented Generation (RAG) include:\n",
      "\n",
      "1. **Enhanced Accuracy**: By retrieving relevant information from a knowledge base, RAG can provide more accurate and contextually relevant responses compared to models that rely solely on pre-trained knowledge.\n",
      "\n",
      "2. **Up-to-Date \n",
      "\n",
      "================================================================================\n",
      "\n",
      "ðŸ’¡ Memory RAG understands 'it' refers to RAG from conversation context!\n"
     ]
    }
   ],
   "source": [
    "from shared.prompts import RAG_PROMPT_TEMPLATE\n",
    "\n",
    "print_section_header(\"Comparison: Memory vs No Memory\")\n",
    "\n",
    "# Simple RAG (no memory)\n",
    "simple_chain = (\n",
    "    {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | RAG_PROMPT_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test with anaphoric query\n",
    "query = \"What are the advantages of using it?\"\n",
    "\n",
    "print(f\"Query (after discussing RAG): '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n[Simple RAG - NO MEMORY]\")\n",
    "try:\n",
    "    simple_response = simple_chain.invoke(query)\n",
    "    print(simple_response[:300])\n",
    "except Exception as e:\n",
    "    print(f\"Cannot answer: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n[Memory RAG - WITH MEMORY]\")\n",
    "memory_response = conversational_chain.invoke(\n",
    "    {\"input\": query},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(memory_response[:300])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nðŸ’¡ Memory RAG understands 'it' refers to RAG from conversation context!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Architecture: RAG with Memory\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "User Query + Chat History â†’ Retriever â†’ LLM + Prompt â†’ Response\n",
    "                                            â†“\n",
    "                                    Update History\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "- `RunnableWithMessageHistory`: LCEL wrapper for memory\n",
    "- `ChatMessageHistory`: Stores conversation\n",
    "- `MessagesPlaceholder`: Injects history into prompt\n",
    "\n",
    "**Advantages:**\n",
    "âœ… Handles follow-up questions  \n",
    "âœ… Understands anaphoric references (\"it\", \"that\", \"them\")  \n",
    "âœ… More natural conversations  \n",
    "âœ… Context accumulates over session  \n",
    "\n",
    "**Limitations:**\n",
    "- Higher cost (more tokens in context)\n",
    "- Memory can grow large\n",
    "- Privacy considerations (stores conversations)\n",
    "\n",
    "**Production Tips:**\n",
    "- Use `ConversationBufferWindowMemory` to limit history size\n",
    "- Implement conversation summarization for long sessions\n",
    "- Store sessions in database (Redis, PostgreSQL)\n",
    "- Add conversation timeout/expiry\n",
    "\n",
    "**Next:** [05_branched_rag.ipynb](05_branched_rag.ipynb) - Multi-query parallel retrieval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
