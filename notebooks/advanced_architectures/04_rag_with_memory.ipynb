{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - RAG with Memory (Conversational RAG)\n",
    "\n",
    "**Architecture:** RAG with Conversational Memory\n",
    "\n",
    "**Complexity:** ‚≠ê‚≠ê\n",
    "\n",
    "**Use Cases:**\n",
    "- Chatbots and conversational AI\n",
    "- Customer support systems  \n",
    "- Interactive Q&A sessions\n",
    "\n",
    "**Key Feature:** Maintains chat history to handle follow-up questions and anaphoric references.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "User: \"What is RAG?\"\n",
    "Bot: \"RAG is Retrieval-Augmented Generation...\"\n",
    "User: \"What are its main components?\"  ‚Üê References \"RAG\" from context\n",
    "Bot: \"The main components of RAG are...\"  ‚Üê Understands reference\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SETUP: RAG WITH MEMORY\n",
      "================================================================================\n",
      "\n",
      "‚úì Loaded vector store from /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/advanced_architectures/../../data/vector_stores/openai_embeddings\n",
      "\n",
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from shared.config import OPENAI_VECTOR_STORE_PATH, DEFAULT_MODEL, DEFAULT_K\n",
    "from shared.utils import load_vector_store, print_section_header, format_docs\n",
    "from shared.prompts import MEMORY_RAG_PROMPT\n",
    "\n",
    "print_section_header(\"Setup: RAG with Memory\")\n",
    "\n",
    "# Load vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = load_vector_store(OPENAI_VECTOR_STORE_PATH, embeddings)\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": DEFAULT_K})\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=DEFAULT_MODEL, temperature=0)\n",
    "\n",
    "print(\"\\n‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory Setup\n",
    "\n",
    "We'll use `RunnableWithMessageHistory` to add conversational memory to our RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MEMORY CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      "‚úì Memory store configured\n",
      "‚úì Conversational chain created with memory\n",
      "\n",
      "üí° Chat history is maintained per session_id\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print_section_header(\"Memory Configuration\")\n",
    "\n",
    "# Session store (in-memory for demo)\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "print(\"‚úì Memory store configured\")\n",
    "\n",
    "# Build base chain\n",
    "# The retriever needs just the \"input\" string, not the whole dict\n",
    "base_chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        context=lambda x: format_docs(retriever.invoke(x[\"input\"]))\n",
    "    )\n",
    "    | MEMORY_RAG_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Wrap with memory\n",
    "conversational_chain = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "print(\"‚úì Conversational chain created with memory\")\n",
    "print(\"\\nüí° Chat history is maintained per session_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Conversational Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONVERSATIONAL TEST\n",
      "================================================================================\n",
      "\n",
      "User: What is RAG?\n",
      "\n",
      "Bot: RAG (Retrieval-Augmented Generation) is a pattern where a model retrieves relevant information from an external index at query time and uses it to generate a grounded answer.\n",
      "\n",
      "Key pieces:\n",
      "- Indexing: a separate pipeline that ingests data from sources and builds an index.\n",
      "- Retrieval and generation: at run time, fetch the most relevant chunks from the index and pass them to the model to produce the response.\n",
      "\n",
      "Common implementations:\n",
      "- Agentic RAG: the LLM decides when/how to call a simple search tool‚Äîgood general-purpose approach.\n",
      "- Two-step RAG chain: retrieve first, then a single LLM call‚Äîfast and effective for simple queries.\n",
      "\n",
      "Tooling:\n",
      "- You can view step-by-step execution, latency, and metadata in a LangSmith trace.\n",
      "- For more control, use LangGraph to add steps like grading document relevance or rewriting queries (see the Agentic RAG tutorial).\n",
      "\n",
      "================================================================================\n",
      "\n",
      "User: What are its main components?\n",
      "\n",
      "Bot: Core components of RAG\n",
      "\n",
      "- Ingestion and indexing (offline)\n",
      "  - Connectors to data sources\n",
      "  - Text cleaning, chunking/splitting\n",
      "  - Embedding model to vectorize chunks\n",
      "  - Index/store (vector, keyword, or hybrid) with metadata and permissions\n",
      "\n",
      "- Retrieval (online)\n",
      "  - Query processing (rewrite/expansion, filters)\n",
      "  - Retriever (similarity search, BM25, hybrid)\n",
      "  - Re-ranking and deduplication of results\n",
      "\n",
      "- Generation (online)\n",
      "  - Prompt construction that injects retrieved context\n",
      "  - LLM to synthesize the answer\n",
      "  - Optional citations/source grounding\n",
      "\n",
      "- Quality and governance (optional)\n",
      "  - Hallucination/grounding checks, relevance grading\n",
      "  - Observability, evaluation, and feedback loops\n",
      "  - Caching, routing, and access control\n",
      "\n",
      "Some systems add orchestration/agent logic so the model can decide when/how to retrieve, decompose queries, or iterate.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "User: How do I implement it?\n",
      "\n",
      "Bot: Here‚Äôs a practical way to implement RAG, from simplest to more advanced, using the pieces we discussed.\n",
      "\n",
      "Baseline (two-step RAG)\n",
      "- Choose a chat model (cloud or local; Ollama is an easy local option). If supported, set the model‚Äôs ‚Äúreasoning effort‚Äù tier as needed.\n",
      "- Ingest and index (offline):\n",
      "  - Load data from your sources.\n",
      "  - Clean and chunk text.\n",
      "  - Embed chunks with an embedding model.\n",
      "  - Store in an index (vector, keyword, or hybrid) with metadata/permissions.\n",
      "- Retrieve (online):\n",
      "  - On each query, optionally rewrite/expand it and apply filters.\n",
      "  - Retrieve top-k chunks (similarity/BM25/hybrid). Optionally re-rank and deduplicate.\n",
      "- Generate (online):\n",
      "  - Build a prompt that injects the retrieved chunks plus instructions (e.g., cite sources).\n",
      "  - Call the model and return the answer with source metadata.\n",
      "\n",
      "Agentic RAG (create_agent)\n",
      "- Expose your retriever as a tool.\n",
      "- Use create_agent so the model decides when/how to call the retriever (and possibly iterate or decompose the query).\n",
      "- This is easy to extend with additional tools (e.g., SQL, web search).\n",
      "\n",
      "Add-on features (from the docs you saw)\n",
      "- Streaming: stream tokens for responsiveness; ensure every step in the pipeline can handle streams.\n",
      "- Conversational memory: keep message history for multi-turn grounding.\n",
      "- Long-term memory: persist facts across threads/sessions.\n",
      "- Structured responses: ask the model to return a typed schema for easier downstream use.\n",
      "- Prompt caching: reduce latency/cost on repeat prompts.\n",
      "- Local models: run fully on-device for privacy/cost.\n",
      "- Deploy and observe: use LangSmith Deployments and tracing to monitor latency, relevance, and grounding.\n",
      "\n",
      "If you share your stack (Python/JS, LangChain vs raw SDK, local vs cloud, chosen vector store), I can give you a tailored starter template.\n",
      "\n",
      "‚úÖ Conversation maintained context successfully!\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Conversational Test\")\n",
    "\n",
    "session_id = \"user_123\"\n",
    "\n",
    "# First question\n",
    "print(\"User: What is RAG?\\n\")\n",
    "response1 = conversational_chain.invoke(\n",
    "    {\"input\": \"What is RAG?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(f\"Bot: {response1}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Follow-up with anaphoric reference\n",
    "print(\"\\nUser: What are its main components?\\n\")\n",
    "response2 = conversational_chain.invoke(\n",
    "    {\"input\": \"What are its main components?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(f\"Bot: {response2}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Another follow-up\n",
    "print(\"\\nUser: How do I implement it?\\n\")\n",
    "response3 = conversational_chain.invoke(\n",
    "    {\"input\": \"How do I implement it?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(f\"Bot: {response3}\")\n",
    "\n",
    "print(\"\\n‚úÖ Conversation maintained context successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. View Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CHAT HISTORY\n",
      "================================================================================\n",
      "\n",
      "Messages in session 'user_123': 6\n",
      "\n",
      "1. User: What is RAG?\n",
      "\n",
      "2. Bot: RAG (Retrieval-Augmented Generation) is a pattern where a model retrieves relevant information from an external index at query time and uses it to gen...\n",
      "\n",
      "3. User: What are its main components?\n",
      "\n",
      "4. Bot: Core components of RAG\n",
      "\n",
      "- Ingestion and indexing (offline)\n",
      "  - Connectors to data sources\n",
      "  - Text cleaning, chunking/splitting\n",
      "  - Embedding model to...\n",
      "\n",
      "5. User: How do I implement it?\n",
      "\n",
      "6. Bot: Here‚Äôs a practical way to implement RAG, from simplest to more advanced, using the pieces we discussed.\n",
      "\n",
      "Baseline (two-step RAG)\n",
      "- Choose a chat model...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Chat History\")\n",
    "\n",
    "history = store[session_id]\n",
    "print(f\"Messages in session '{session_id}': {len(history.messages)}\\n\")\n",
    "\n",
    "for i, msg in enumerate(history.messages, 1):\n",
    "    role = \"User\" if msg.type == \"human\" else \"Bot\"\n",
    "    content = msg.content[:150] + \"...\" if len(msg.content) > 150 else msg.content\n",
    "    print(f\"{i}. {role}: {content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison: Memory vs No Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARISON: MEMORY VS NO MEMORY\n",
      "================================================================================\n",
      "\n",
      "Query (after discussing RAG): 'What are the advantages of using it?'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Simple RAG - NO MEMORY]\n",
      "It‚Äôs not clear what ‚Äúit‚Äù refers to. Based on the context, there are two relevant sets of advantages:\n",
      "\n",
      "Advantages of treating search as a tool the LLM uses\n",
      "- Search only when needed ‚Äì The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches. [From ‚ÄúBenefits\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[Memory RAG - WITH MEMORY]\n",
      "Key advantages of RAG\n",
      "\n",
      "- Higher factual accuracy: grounds answers in retrieved sources, reducing hallucinations.\n",
      "- Up-to-date knowledge: you can update the index without retraining the model.\n",
      "- Domain adaptation without fine-tuning: plug in proprietary docs/policies and get expert answers fast.\n",
      "- Ex\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üí° Memory RAG understands 'it' refers to RAG from conversation context!\n"
     ]
    }
   ],
   "source": [
    "from shared.prompts import RAG_PROMPT_TEMPLATE\n",
    "\n",
    "print_section_header(\"Comparison: Memory vs No Memory\")\n",
    "\n",
    "# Simple RAG (no memory)\n",
    "simple_chain = (\n",
    "    {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | RAG_PROMPT_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test with anaphoric query\n",
    "query = \"What are the advantages of using it?\"\n",
    "\n",
    "print(f\"Query (after discussing RAG): '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n[Simple RAG - NO MEMORY]\")\n",
    "try:\n",
    "    simple_response = simple_chain.invoke(query)\n",
    "    print(simple_response[:300])\n",
    "except Exception as e:\n",
    "    print(f\"Cannot answer: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n[Memory RAG - WITH MEMORY]\")\n",
    "memory_response = conversational_chain.invoke(\n",
    "    {\"input\": query},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(memory_response[:300])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nüí° Memory RAG understands 'it' refers to RAG from conversation context!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Architecture: RAG with Memory\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "User Query + Chat History ‚Üí Retriever ‚Üí LLM + Prompt ‚Üí Response\n",
    "                                            ‚Üì\n",
    "                                    Update History\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "- `RunnableWithMessageHistory`: LCEL wrapper for memory\n",
    "- `ChatMessageHistory`: Stores conversation\n",
    "- `MessagesPlaceholder`: Injects history into prompt\n",
    "\n",
    "**Advantages:**\n",
    "‚úÖ Handles follow-up questions  \n",
    "‚úÖ Understands anaphoric references (\"it\", \"that\", \"them\")  \n",
    "‚úÖ More natural conversations  \n",
    "‚úÖ Context accumulates over session  \n",
    "\n",
    "**Limitations:**\n",
    "- Higher cost (more tokens in context)\n",
    "- Memory can grow large\n",
    "- Privacy considerations (stores conversations)\n",
    "\n",
    "**Production Tips:**\n",
    "- Use `ConversationBufferWindowMemory` to limit history size\n",
    "- Implement conversation summarization for long sessions\n",
    "- Store sessions in database (Redis, PostgreSQL)\n",
    "- Add conversation timeout/expiry\n",
    "\n",
    "**Next:** [05_branched_rag.ipynb](05_branched_rag.ipynb) - Multi-query parallel retrieval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
