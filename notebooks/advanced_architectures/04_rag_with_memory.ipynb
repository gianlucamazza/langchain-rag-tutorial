{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - RAG with Memory (Conversational RAG)\n",
    "\n",
    "**Architecture:** RAG with Conversational Memory\n",
    "\n",
    "**Complexity:** â­â­\n",
    "\n",
    "**Use Cases:**\n",
    "- Chatbots and conversational AI\n",
    "- Customer support systems  \n",
    "- Interactive Q&A sessions\n",
    "\n",
    "**Key Feature:** Maintains chat history to handle follow-up questions and anaphoric references.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "User: \"What is RAG?\"\n",
    "Bot: \"RAG is Retrieval-Augmented Generation...\"\n",
    "User: \"What are its main components?\"  â† References \"RAG\" from context\n",
    "Bot: \"The main components of RAG are...\"  â† Understands reference\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from shared.config import OPENAI_VECTOR_STORE_PATH, DEFAULT_MODEL, DEFAULT_K\n",
    "from shared.utils import load_vector_store, print_section_header, format_docs\n",
    "from shared.prompts import MEMORY_RAG_PROMPT\n",
    "\n",
    "print_section_header(\"Setup: RAG with Memory\")\n",
    "\n",
    "# Load vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = load_vector_store(OPENAI_VECTOR_STORE_PATH, embeddings)\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": DEFAULT_K})\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=DEFAULT_MODEL, temperature=0)\n",
    "\n",
    "print(\"\\nâœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory Setup\n",
    "\n",
    "We'll use `RunnableWithMessageHistory` to add conversational memory to our RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "print_section_header(\"Memory Configuration\")\n",
    "\n",
    "# Session store (in-memory for demo)\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "print(\"âœ“ Memory store configured\")\n",
    "\n",
    "# Build base chain\n",
    "base_chain = (\n",
    "    {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | MEMORY_RAG_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Wrap with memory\n",
    "conversational_chain = RunnableWithMessageHistory(\n",
    "    base_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")\n",
    "\n",
    "print(\"âœ“ Conversational chain created with memory\")\n",
    "print(\"\\nðŸ’¡ Chat history is maintained per session_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Conversational Flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Conversational Test\")\n",
    "\n",
    "session_id = \"user_123\"\n",
    "\n",
    "# First question\n",
    "print(\"User: What is RAG?\\n\")\n",
    "response1 = conversational_chain.invoke(\n",
    "    {\"input\": \"What is RAG?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(f\"Bot: {response1}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Follow-up with anaphoric reference\n",
    "print(\"\\nUser: What are its main components?\\n\")\n",
    "response2 = conversational_chain.invoke(\n",
    "    {\"input\": \"What are its main components?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(f\"Bot: {response2}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Another follow-up\n",
    "print(\"\\nUser: How do I implement it?\\n\")\n",
    "response3 = conversational_chain.invoke(\n",
    "    {\"input\": \"How do I implement it?\"},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(f\"Bot: {response3}\")\n",
    "\n",
    "print(\"\\nâœ… Conversation maintained context successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. View Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Chat History\")\n",
    "\n",
    "history = store[session_id]\n",
    "print(f\"Messages in session '{session_id}': {len(history.messages)}\\n\")\n",
    "\n",
    "for i, msg in enumerate(history.messages, 1):\n",
    "    role = \"User\" if msg.type == \"human\" else \"Bot\"\n",
    "    content = msg.content[:150] + \"...\" if len(msg.content) > 150 else msg.content\n",
    "    print(f\"{i}. {role}: {content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison: Memory vs No Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.prompts import RAG_PROMPT_TEMPLATE\n",
    "\n",
    "print_section_header(\"Comparison: Memory vs No Memory\")\n",
    "\n",
    "# Simple RAG (no memory)\n",
    "simple_chain = (\n",
    "    {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | RAG_PROMPT_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test with anaphoric query\n",
    "query = \"What are the advantages of using it?\"\n",
    "\n",
    "print(f\"Query (after discussing RAG): '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n[Simple RAG - NO MEMORY]\")\n",
    "try:\n",
    "    simple_response = simple_chain.invoke(query)\n",
    "    print(simple_response[:300])\n",
    "except Exception as e:\n",
    "    print(f\"Cannot answer: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n[Memory RAG - WITH MEMORY]\")\n",
    "memory_response = conversational_chain.invoke(\n",
    "    {\"input\": query},\n",
    "    config={\"configurable\": {\"session_id\": session_id}}\n",
    ")\n",
    "print(memory_response[:300])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nðŸ’¡ Memory RAG understands 'it' refers to RAG from conversation context!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Architecture: RAG with Memory\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "User Query + Chat History â†’ Retriever â†’ LLM + Prompt â†’ Response\n",
    "                                            â†“\n",
    "                                    Update History\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "- `RunnableWithMessageHistory`: LCEL wrapper for memory\n",
    "- `ChatMessageHistory`: Stores conversation\n",
    "- `MessagesPlaceholder`: Injects history into prompt\n",
    "\n",
    "**Advantages:**\n",
    "âœ… Handles follow-up questions  \n",
    "âœ… Understands anaphoric references (\"it\", \"that\", \"them\")  \n",
    "âœ… More natural conversations  \n",
    "âœ… Context accumulates over session  \n",
    "\n",
    "**Limitations:**\n",
    "- Higher cost (more tokens in context)\n",
    "- Memory can grow large\n",
    "- Privacy considerations (stores conversations)\n",
    "\n",
    "**Production Tips:**\n",
    "- Use `ConversationBufferWindowMemory` to limit history size\n",
    "- Implement conversation summarization for long sessions\n",
    "- Store sessions in database (Redis, PostgreSQL)\n",
    "- Add conversation timeout/expiry\n",
    "\n",
    "**Next:** [05_branched_rag.ipynb](05_branched_rag.ipynb) - Multi-query parallel retrieval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
