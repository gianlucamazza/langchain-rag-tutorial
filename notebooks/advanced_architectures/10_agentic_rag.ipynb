{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Agentic RAG (Autonomous Agents)\n",
    "\n",
    "**Complexity:** â­â­â­â­â­\n",
    "\n",
    "**Use Cases:** Multi-step reasoning, BI dashboards, complex decision-making\n",
    "\n",
    "**Key Features:**\n",
    "- ReAct agent (Reasoning + Acting)\n",
    "- Multiple tools (retriever, calculator, web search)\n",
    "- Autonomous planning and execution\n",
    "- Multi-step reasoning\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Query: \"If I have 10K docs and process 1M tokens/day, should I use OpenAI or HF?\"\n",
    "\n",
    "Agent:\n",
    "1. Thought: Need cost calculation\n",
    "   Action: Calculator â†’ Estimate costs\n",
    "2. Thought: Need comparison info\n",
    "   Action: Knowledge Base â†’ Retrieve comparison\n",
    "3. Thought: Can now recommend\n",
    "   Final Answer: \"HuggingFace is better because...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gianlucamazza/Workspace/notebooks/llm_rag/venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SETUP: AGENTIC RAG\n",
      "================================================================================\n",
      "\n",
      "âœ“ Loaded vector store from /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/advanced_architectures/../../data/vector_stores/openai_embeddings\n",
      "âœ… Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from shared.config import OPENAI_VECTOR_STORE_PATH, DEFAULT_MODEL\n",
    "from shared.utils import load_vector_store, print_section_header, format_docs\n",
    "from shared.prompts import REACT_AGENT_PROMPT\n",
    "\n",
    "print_section_header(\"Setup: Agentic RAG\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = load_vector_store(OPENAI_VECTOR_STORE_PATH, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "llm = ChatOpenAI(model=DEFAULT_MODEL, temperature=0)\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Tools for Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_core.tools import tool\nimport numexpr\n\nprint_section_header(\"Agent Tools\")\n\n# Tool 1: Knowledge Base Retriever\n@tool\ndef knowledge_base(query: str) -> str:\n    \"\"\"Search LangChain RAG knowledge base.\n    Use for questions about RAG, embeddings, retrievers, LangChain.\"\"\"\n    docs = retriever.invoke(query)\n    return format_docs(docs[:3])\n\n# Tool 2: Calculator\n@tool\ndef calculator(expression: str) -> str:\n    \"\"\"Calculate mathematical expressions safely.\n    Input should be a valid math expression like '2 + 2' or '100000 * 0.00002'.\n    Examples: '10 + 5', '(100 * 2) / 3', '2 ** 8'\"\"\"\n    try:\n        # Use numexpr for safe evaluation\n        result = numexpr.evaluate(expression).item()\n        return str(result)\n    except Exception as e:\n        return f\"Error calculating '{expression}': {str(e)}\"\n\n# Tool 3: Web Search (if available)\ntools = [knowledge_base, calculator]\n\ntry:\n    from langchain_community.tools.tavily_search import TavilySearchResults\n    \n    @tool\n    def web_search(query: str) -> str:\n        \"\"\"Search the web for current information.\n        Use for queries outside the knowledge base.\"\"\"\n        search = TavilySearchResults(max_results=2)\n        return search.invoke(query)\n    \n    tools.append(web_search)\n    print(\"âœ“ Created 3 tools (retriever, calculator, web)\")\nexcept:\n    print(\"âœ“ Created 2 tools (retriever, calculator)\")\n\nprint(\"  Tools:\")\nfor tool in tools:\n    print(f\"    - {tool.name}: {tool.description[:60]}...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "print_section_header(\"Modern ReAct Agent (LangChain 1.0)\")\n",
    "\n",
    "# Create modern agent (built on LangGraph)\n",
    "agent_executor = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    system_prompt=\"\"\"You are a helpful AI assistant with expertise in RAG systems and embeddings.\n",
    "\n",
    "You have access to these tools:\n",
    "- knowledge_base: Search documentation about RAG, embeddings, and LangChain\n",
    "- calculator: Perform mathematical calculations\n",
    "- web_search: Search the web for current information (if available)\n",
    "\n",
    "Guidelines:\n",
    "1. Think step by step about what information you need\n",
    "2. Use the appropriate tool(s) to gather information\n",
    "3. Synthesize the results into a clear, concise answer\n",
    "4. Always cite your sources when using the knowledge base\n",
    "\n",
    "Be helpful and accurate!\"\"\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Modern ReAct agent created\")\n",
    "print(\"  - Built on LangGraph internally\")\n",
    "print(\"  - Automatic state management\")\n",
    "print(\"  - Message-based interface\")\n",
    "print(\"  - Streaming support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Agent - Simple Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Test 1: Simple Query\")\n",
    "\n",
    "query = \"What is a vector store?\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Modern message-based invocation\n",
    "response = agent_executor.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Extract final answer from messages\n",
    "final_answer = response[\"messages\"][-1].content\n",
    "print(f\"\\nFinal Answer: {final_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Agent - Multi-Step Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Test 2: Multi-Step\")\n",
    "\n",
    "query = \"\"\"If embedding 100,000 documents costs $2 with OpenAI,\n",
    "what's the cost per document? Then tell me if HuggingFace is cheaper.\"\"\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Modern message-based invocation\n",
    "response = agent_executor.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Extract final answer\n",
    "final_answer = response[\"messages\"][-1].content\n",
    "print(f\"\\nFinal Answer: {final_answer}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Agent used multiple tools to solve this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Agent with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Conversational Agent\")\n",
    "\n",
    "print(\"âœ“ Using message-based conversation state\\n\")\n",
    "print(\"  Modern LangChain 1.0 approach:\")\n",
    "print(\"  - No separate memory object needed\")\n",
    "print(\"  - State managed through message history\")\n",
    "print(\"  - Agent automatically maintains context\\n\")\n",
    "\n",
    "# Maintain conversation history as a list of messages\n",
    "conversation = []\n",
    "\n",
    "# Multi-turn conversation\n",
    "# Turn 1\n",
    "query1 = \"What are the embedding dimensions?\"\n",
    "print(f\"User: {query1}\\n\")\n",
    "\n",
    "conversation.append({\"role\": \"user\", \"content\": query1})\n",
    "response1 = agent_executor.invoke({\"messages\": conversation})\n",
    "conversation = response1[\"messages\"]  # Update with full conversation\n",
    "\n",
    "answer1 = conversation[-1].content\n",
    "print(f\"Agent: {answer1[:150]}...\\n\")\n",
    "\n",
    "# Turn 2 (agent has context from turn 1)\n",
    "query2 = \"Which one is cheaper?\"\n",
    "print(f\"\\nUser: {query2}\\n\")\n",
    "\n",
    "conversation.append({\"role\": \"user\", \"content\": query2})\n",
    "response2 = agent_executor.invoke({\"messages\": conversation})\n",
    "conversation = response2[\"messages\"]  # Update with full conversation\n",
    "\n",
    "answer2 = conversation[-1].content\n",
    "print(f\"Agent: {answer2[:150]}...\")\n",
    "\n",
    "print(\"\\nâœ… Agent maintained conversation context!\")\n",
    "print(f\"   (Total messages in conversation: {len(conversation)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "Query â†’ Agent Reasoning Loop:\n",
    "  1. Think about what to do\n",
    "  2. Select tool to use\n",
    "  3. Execute tool\n",
    "  4. Observe result\n",
    "  5. Repeat until answer found\n",
    "â†’ Final Answer\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "âœ… Autonomous decision-making  \n",
    "âœ… Multi-step reasoning  \n",
    "âœ… Tool composition  \n",
    "âœ… Handles complex queries  \n",
    "âœ… Extensible (add more tools)  \n",
    "\n",
    "**Limitations:**\n",
    "- Very slow (many LLM calls)\n",
    "- Expensive (reasoning tokens)\n",
    "- Can get stuck in loops\n",
    "- Unpredictable behavior\n",
    "- Hard to debug\n",
    "\n",
    "**When to Use:**\n",
    "- Complex multi-step tasks\n",
    "- Requires multiple data sources\n",
    "- Research/analysis workflows\n",
    "- BI dashboards\n",
    "\n",
    "**Production Tips:**\n",
    "- Set max_iterations carefully\n",
    "- Monitor token usage\n",
    "- Add timeout mechanisms\n",
    "- Implement fallbacks\n",
    "- Use LangSmith for tracing\n",
    "- Test edge cases thoroughly\n",
    "\n",
    "**Next:** [11_comparison.ipynb](11_comparison.ipynb) - Benchmark all architectures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}