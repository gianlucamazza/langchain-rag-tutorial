{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Agentic RAG (Autonomous Agents)\n",
    "\n",
    "**Complexity:** â­â­â­â­â­\n",
    "\n",
    "**Use Cases:** Multi-step reasoning, BI dashboards, complex decision-making\n",
    "\n",
    "**Key Features:**\n",
    "- ReAct agent (Reasoning + Acting)\n",
    "- Multiple tools (retriever, calculator, web search)\n",
    "- Autonomous planning and execution\n",
    "- Multi-step reasoning\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Query: \"If I have 10K docs and process 1M tokens/day, should I use OpenAI or HF?\"\n",
    "\n",
    "Agent:\n",
    "1. Thought: Need cost calculation\n",
    "   Action: Calculator â†’ Estimate costs\n",
    "2. Thought: Need comparison info\n",
    "   Action: Knowledge Base â†’ Retrieve comparison\n",
    "3. Thought: Can now recommend\n",
    "   Final Answer: \"HuggingFace is better because...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from shared.config import OPENAI_VECTOR_STORE_PATH, DEFAULT_MODEL\n",
    "from shared.utils import load_vector_store, print_section_header, format_docs\n",
    "from shared.prompts import REACT_AGENT_PROMPT\n",
    "\n",
    "print_section_header(\"Setup: Agentic RAG\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = load_vector_store(OPENAI_VECTOR_STORE_PATH, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "llm = ChatOpenAI(model=DEFAULT_MODEL, temperature=0)\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Tools for Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "print_section_header(\"Agent Tools\")\n",
    "\n",
    "# Tool 1: Retriever\n",
    "def retriever_tool_func(query: str) -> str:\n",
    "    \"\"\"Search LangChain RAG knowledge base.\"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    return format_docs(docs[:3])\n",
    "\n",
    "retriever_tool = Tool(\n",
    "    name=\"Knowledge_Base\",\n",
    "    func=retriever_tool_func,\n",
    "    description=\"\"\"Search LangChain RAG knowledge base. \n",
    "    Use for questions about RAG, embeddings, retrievers, LangChain.\"\"\"\n",
    ")\n",
    "\n",
    "# Tool 2: Calculator\n",
    "llm_math = LLMMathChain.from_llm(llm)\n",
    "calculator_tool = Tool(\n",
    "    name=\"Calculator\",\n",
    "    func=llm_math.run,\n",
    "    description=\"\"\"Useful for math calculations. \n",
    "    Input should be a math expression.\"\"\"\n",
    ")\n",
    "\n",
    "# Tool 3: Web Search (if available)\n",
    "try:\n",
    "    from langchain_community.tools import DuckDuckGoSearchResults\n",
    "    web_search_tool = Tool(\n",
    "        name=\"Web_Search\",\n",
    "        func=DuckDuckGoSearchResults(num_results=2).invoke,\n",
    "        description=\"\"\"Search the web for current info. \n",
    "        Use for queries outside knowledge base.\"\"\"\n",
    "    )\n",
    "    tools = [retriever_tool, calculator_tool, web_search_tool]\n",
    "    print(\"âœ“ Created 3 tools (retriever, calculator, web)\")\n",
    "except:\n",
    "    tools = [retriever_tool, calculator_tool]\n",
    "    print(\"âœ“ Created 2 tools (retriever, calculator)\")\n",
    "\n",
    "for tool in tools:\n",
    "    print(f\"  - {tool.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "print_section_header(\"ReAct Agent\")\n",
    "\n",
    "# Create agent\n",
    "agent = create_react_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=REACT_AGENT_PROMPT\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    max_iterations=5,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "print(\"âœ“ ReAct agent created\")\n",
    "print(\"  - Max iterations: 5\")\n",
    "print(\"  - Verbose mode enabled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Agent - Simple Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Test 1: Simple Query\")\n",
    "\n",
    "query = \"What is a vector store?\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "response = agent_executor.invoke({\"input\": query})\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nFinal Answer: {response['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Agent - Multi-Step Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Test 2: Multi-Step\")\n",
    "\n",
    "query = \"\"\"If embedding 100,000 documents costs $2 with OpenAI,\n",
    "what's the cost per document? Then tell me if HuggingFace is cheaper.\"\"\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "response = agent_executor.invoke({\"input\": query})\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"\\nFinal Answer: {response['output']}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Agent used multiple tools to solve this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Agent with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "print_section_header(\"Conversational Agent\")\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "conversational_agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    memory=memory,\n",
    "    verbose=False,  # Less verbose for readability\n",
    "    max_iterations=5\n",
    ")\n",
    "\n",
    "print(\"âœ“ Conversational agent created\\n\")\n",
    "\n",
    "# Multi-turn conversation\n",
    "print(\"User: What are the embedding dimensions?\\n\")\n",
    "r1 = conversational_agent.invoke({\"input\": \"What are the embedding dimensions?\"})\n",
    "print(f\"Agent: {r1['output'][:150]}...\\n\")\n",
    "\n",
    "print(\"\\nUser: Which one is cheaper?\\n\")\n",
    "r2 = conversational_agent.invoke({\"input\": \"Which one is cheaper?\"})\n",
    "print(f\"Agent: {r2['output'][:150]}...\")\n",
    "\n",
    "print(\"\\nâœ… Agent maintained conversation context!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "Query â†’ Agent Reasoning Loop:\n",
    "  1. Think about what to do\n",
    "  2. Select tool to use\n",
    "  3. Execute tool\n",
    "  4. Observe result\n",
    "  5. Repeat until answer found\n",
    "â†’ Final Answer\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "âœ… Autonomous decision-making  \n",
    "âœ… Multi-step reasoning  \n",
    "âœ… Tool composition  \n",
    "âœ… Handles complex queries  \n",
    "âœ… Extensible (add more tools)  \n",
    "\n",
    "**Limitations:**\n",
    "- Very slow (many LLM calls)\n",
    "- Expensive (reasoning tokens)\n",
    "- Can get stuck in loops\n",
    "- Unpredictable behavior\n",
    "- Hard to debug\n",
    "\n",
    "**When to Use:**\n",
    "- Complex multi-step tasks\n",
    "- Requires multiple data sources\n",
    "- Research/analysis workflows\n",
    "- BI dashboards\n",
    "\n",
    "**Production Tips:**\n",
    "- Set max_iterations carefully\n",
    "- Monitor token usage\n",
    "- Add timeout mechanisms\n",
    "- Implement fallbacks\n",
    "- Use LangSmith for tracing\n",
    "- Test edge cases thoroughly\n",
    "\n",
    "**Next:** [11_comparison.ipynb](11_comparison.ipynb) - Benchmark all architectures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
