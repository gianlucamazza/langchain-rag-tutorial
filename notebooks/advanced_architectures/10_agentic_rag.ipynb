{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Agentic RAG (Autonomous Agents)\n",
    "\n",
    "**Complexity:** â­â­â­â­â­\n",
    "\n",
    "**Use Cases:** Multi-step reasoning, BI dashboards, complex decision-making\n",
    "\n",
    "**Key Features:**\n",
    "- ReAct agent (Reasoning + Acting)\n",
    "- Multiple tools (retriever, calculator, web search)\n",
    "- Autonomous planning and execution\n",
    "- Multi-step reasoning\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Query: \"If I have 10K docs and process 1M tokens/day, should I use OpenAI or HF?\"\n",
    "\n",
    "Agent:\n",
    "1. Thought: Need cost calculation\n",
    "   Action: Calculator â†’ Estimate costs\n",
    "2. Thought: Need comparison info\n",
    "   Action: Knowledge Base â†’ Retrieve comparison\n",
    "3. Thought: Can now recommend\n",
    "   Final Answer: \"HuggingFace is better because...\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SETUP: AGENTIC RAG\n",
      "================================================================================\n",
      "\n",
      "âœ“ Loaded vector store from /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/advanced_architectures/../../data/vector_stores/openai_embeddings\n",
      "âœ… Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from shared.config import OPENAI_VECTOR_STORE_PATH, DEFAULT_MODEL\n",
    "from shared.utils import load_vector_store, print_section_header, format_docs\n",
    "from shared.prompts import REACT_AGENT_PROMPT\n",
    "\n",
    "print_section_header(\"Setup: Agentic RAG\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = load_vector_store(OPENAI_VECTOR_STORE_PATH, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "llm = ChatOpenAI(model=DEFAULT_MODEL, temperature=0)\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Tools for Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "AGENT TOOLS\n",
      "================================================================================\n",
      "\n",
      "âœ“ Created 3 tools (retriever, calculator, web)\n",
      "  Tools:\n",
      "    - knowledge_base: Search LangChain RAG knowledge base.\n",
      "Use for questions about...\n",
      "    - calculator: Calculate mathematical expressions safely.\n",
      "Input should be a...\n",
      "    - web_search: Search the web for current information.\n",
      "Use for queries outs...\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "import numexpr\n",
    "\n",
    "print_section_header(\"Agent Tools\")\n",
    "\n",
    "# Tool 1: Knowledge Base Retriever\n",
    "@tool\n",
    "def knowledge_base(query: str) -> str:\n",
    "    \"\"\"Search LangChain RAG knowledge base.\n",
    "    Use for questions about RAG, embeddings, retrievers, LangChain.\"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    return format_docs(docs[:3])\n",
    "\n",
    "# Tool 2: Calculator\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Calculate mathematical expressions safely.\n",
    "    Input should be a valid math expression like '2 + 2' or '100000 * 0.00002'.\n",
    "    Examples: '10 + 5', '(100 * 2) / 3', '2 ** 8'\"\"\"\n",
    "    try:\n",
    "        # Use numexpr for safe evaluation\n",
    "        result = numexpr.evaluate(expression).item()\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"Error calculating '{expression}': {str(e)}\"\n",
    "\n",
    "# Tool 3: Web Search (if available)\n",
    "tools = [knowledge_base, calculator]\n",
    "\n",
    "try:\n",
    "    from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "    \n",
    "    @tool\n",
    "    def web_search(query: str) -> str:\n",
    "        \"\"\"Search the web for current information.\n",
    "        Use for queries outside the knowledge base.\"\"\"\n",
    "        search = TavilySearchResults(max_results=2)\n",
    "        return search.invoke(query)\n",
    "    \n",
    "    tools.append(web_search)\n",
    "    print(\"âœ“ Created 3 tools (retriever, calculator, web)\")\n",
    "except ImportError:\n",
    "    print(\"âœ“ Created 2 tools (retriever, calculator)\")\n",
    "\n",
    "print(\"  Tools:\")\n",
    "for t in tools:\n",
    "    print(f\"    - {t.name}: {t.description[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MODERN REACT AGENT (LANGCHAIN 1.0)\n",
      "================================================================================\n",
      "\n",
      "âœ“ Modern ReAct agent created\n",
      "  - Built on LangGraph internally\n",
      "  - Automatic state management\n",
      "  - Message-based interface\n",
      "  - Streaming support\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "print_section_header(\"Modern ReAct Agent (LangChain 1.0)\")\n",
    "\n",
    "# Create modern agent (built on LangGraph)\n",
    "agent_executor = create_agent(\n",
    "    model=llm,\n",
    "    tools=tools,\n",
    "    system_prompt=\"\"\"You are a helpful AI assistant with expertise in RAG systems and embeddings.\n",
    "\n",
    "You have access to these tools:\n",
    "- knowledge_base: Search documentation about RAG, embeddings, and LangChain\n",
    "- calculator: Perform mathematical calculations\n",
    "- web_search: Search the web for current information (if available)\n",
    "\n",
    "Guidelines:\n",
    "1. Think step by step about what information you need\n",
    "2. Use the appropriate tool(s) to gather information\n",
    "3. Synthesize the results into a clear, concise answer\n",
    "4. Always cite your sources when using the knowledge base\n",
    "\n",
    "Be helpful and accurate!\"\"\"\n",
    ")\n",
    "\n",
    "print(\"âœ“ Modern ReAct agent created\")\n",
    "print(\"  - Built on LangGraph internally\")\n",
    "print(\"  - Automatic state management\")\n",
    "print(\"  - Message-based interface\")\n",
    "print(\"  - Streaming support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Agent - Simple Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST 1: SIMPLE QUERY\n",
      "================================================================================\n",
      "\n",
      "Query: 'What is a vector store?'\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Final Answer: A vector store is a specialized database designed to store and index vector representations of data, typically generated through embeddings. In the context of machine learning and natural language processing, embeddings are numerical representations of text or other data types that capture semantic meaning.\n",
      "\n",
      "Here's how a vector store typically fits into the workflow:\n",
      "\n",
      "1. **Load**: Data is loaded using document loaders.\n",
      "2. **Split**: The data is divided into smaller chunks using text splitters, which makes it easier to manage and search.\n",
      "3. **Store**: The smaller chunks are then stored in a vector store, where they can be indexed and searched efficiently.\n",
      "\n",
      "Vector stores are essential for applications like information retrieval, where you need to quickly find relevant data based on similarity in the vector space. They enable efficient querying and retrieval of information based on the semantic content of the data rather than just keyword matching.\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Test 1: Simple Query\")\n",
    "\n",
    "query = \"What is a vector store?\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Modern message-based invocation\n",
    "response = agent_executor.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Extract final answer from messages\n",
    "final_answer = response[\"messages\"][-1].content\n",
    "print(f\"\\nFinal Answer: {final_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Agent - Multi-Step Reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TEST 2: MULTI-STEP\n",
      "================================================================================\n",
      "\n",
      "Query: If embedding 100,000 documents costs $2 with OpenAI,\n",
      "what's the cost per document? Then tell me if HuggingFace is cheaper.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Final Answer: The cost per document when embedding 100,000 documents with OpenAI is calculated as follows:\n",
      "\n",
      "\\[\n",
      "\\text{Cost per document} = \\frac{\\text{Total cost}}{\\text{Number of documents}} = \\frac{2}{100000} = 0.00002 \\text{ USD} \\text{ or } 0.02 \\text{ cents per document.}\n",
      "\\]\n",
      "\n",
      "Regarding Hugging Face, the pricing information indicates that they charge for inference endpoints starting from approximately $0.033 per CPU-hour. However, the exact cost per document can vary significantly based on the model used and the size of the documents being processed. \n",
      "\n",
      "In general, OpenAI's cost of $0.02 per document is lower than the starting cost of Hugging Face's inference endpoints, but the final comparison would depend on the specific usage and model requirements for Hugging Face.\n",
      "\n",
      "For more detailed pricing, you can refer to the sources:\n",
      "- [Embedding Models in 2025](https://www.linkedin.com/pulse/embedding-models-2025-technology-pricing-practical-advice-azimbaev-694qe)\n",
      "- [The True Cost of Hugging Face](https://www.metacto.com/blogs/the-true-cost-of-hugging-face-a-guide-to-pricing-and-integration)\n",
      "\n",
      "ðŸ’¡ Agent used multiple tools to solve this!\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Test 2: Multi-Step\")\n",
    "\n",
    "query = \"\"\"If embedding 100,000 documents costs $2 with OpenAI,\n",
    "what's the cost per document? Then tell me if HuggingFace is cheaper.\"\"\"\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Modern message-based invocation\n",
    "response = agent_executor.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Extract final answer\n",
    "final_answer = response[\"messages\"][-1].content\n",
    "print(f\"\\nFinal Answer: {final_answer}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Agent used multiple tools to solve this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Agent with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONVERSATIONAL AGENT\n",
      "================================================================================\n",
      "\n",
      "âœ“ Using message-based conversation state\n",
      "\n",
      "  Modern LangChain 1.0 approach:\n",
      "  - No separate memory object needed\n",
      "  - State managed through message history\n",
      "  - Agent automatically maintains context\n",
      "\n",
      "User: What are the embedding dimensions?\n",
      "\n",
      "Agent: Embedding dimensions refer to the size of the vector representation used to encode data, such as text, images, or other types of information. In the c...\n",
      "\n",
      "\n",
      "User: Which one is cheaper?\n",
      "\n",
      "Agent: When considering the cost of embedding dimensions, \"cheaper\" can refer to several factors, including computational cost, memory usage, and training ti...\n",
      "\n",
      "âœ… Agent maintained conversation context!\n",
      "   (Total messages in conversation: 8)\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Conversational Agent\")\n",
    "\n",
    "print(\"âœ“ Using message-based conversation state\\n\")\n",
    "print(\"  Modern LangChain 1.0 approach:\")\n",
    "print(\"  - No separate memory object needed\")\n",
    "print(\"  - State managed through message history\")\n",
    "print(\"  - Agent automatically maintains context\\n\")\n",
    "\n",
    "# Maintain conversation history as a list of messages\n",
    "conversation = []\n",
    "\n",
    "# Multi-turn conversation\n",
    "# Turn 1\n",
    "query1 = \"What are the embedding dimensions?\"\n",
    "print(f\"User: {query1}\\n\")\n",
    "\n",
    "conversation.append({\"role\": \"user\", \"content\": query1})\n",
    "response1 = agent_executor.invoke({\"messages\": conversation})\n",
    "conversation = response1[\"messages\"]  # Update with full conversation\n",
    "\n",
    "answer1 = conversation[-1].content\n",
    "print(f\"Agent: {answer1[:150]}...\\n\")\n",
    "\n",
    "# Turn 2 (agent has context from turn 1)\n",
    "query2 = \"Which one is cheaper?\"\n",
    "print(f\"\\nUser: {query2}\\n\")\n",
    "\n",
    "conversation.append({\"role\": \"user\", \"content\": query2})\n",
    "response2 = agent_executor.invoke({\"messages\": conversation})\n",
    "conversation = response2[\"messages\"]  # Update with full conversation\n",
    "\n",
    "answer2 = conversation[-1].content\n",
    "print(f\"Agent: {answer2[:150]}...\")\n",
    "\n",
    "print(\"\\nâœ… Agent maintained conversation context!\")\n",
    "print(f\"   (Total messages in conversation: {len(conversation)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "Query â†’ Agent Reasoning Loop:\n",
    "  1. Think about what to do\n",
    "  2. Select tool to use\n",
    "  3. Execute tool\n",
    "  4. Observe result\n",
    "  5. Repeat until answer found\n",
    "â†’ Final Answer\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "âœ… Autonomous decision-making  \n",
    "âœ… Multi-step reasoning  \n",
    "âœ… Tool composition  \n",
    "âœ… Handles complex queries  \n",
    "âœ… Extensible (add more tools)  \n",
    "\n",
    "**Limitations:**\n",
    "- Very slow (many LLM calls)\n",
    "- Expensive (reasoning tokens)\n",
    "- Can get stuck in loops\n",
    "- Unpredictable behavior\n",
    "- Hard to debug\n",
    "\n",
    "**When to Use:**\n",
    "- Complex multi-step tasks\n",
    "- Requires multiple data sources\n",
    "- Research/analysis workflows\n",
    "- BI dashboards\n",
    "\n",
    "**Production Tips:**\n",
    "- Set max_iterations carefully\n",
    "- Monitor token usage\n",
    "- Add timeout mechanisms\n",
    "- Implement fallbacks\n",
    "- Use LangSmith for tracing\n",
    "- Test edge cases thoroughly\n",
    "\n",
    "**Next:** [11_comparison.ipynb](11_comparison.ipynb) - Benchmark all architectures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
