{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 - Self-RAG (Self-Reflective RAG)\n",
    "\n",
    "**Complexity:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "**Use Cases:** Exploratory research, quality-critical applications, self-correcting systems\n",
    "\n",
    "**Key Features:**\n",
    "- LLM decides autonomously when to retrieve\n",
    "- Self-critique mechanism\n",
    "- Iterative refinement\n",
    "- Citation validation\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "Query ‚Üí Need Retrieval? ‚Üí \n",
    "  Yes: Retrieve + Generate ‚Üí Self-Critique ‚Üí Retry if poor\n",
    "  No: Direct Generate ‚Üí Self-Critique\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from shared.config import OPENAI_VECTOR_STORE_PATH, DEFAULT_MODEL\n",
    "from shared.utils import load_vector_store, print_section_header, format_docs\n",
    "from shared.prompts import (\n",
    "    RETRIEVAL_NEED_PROMPT, SELF_CRITIQUE_PROMPT, \n",
    "    CITATION_CHECK_PROMPT, RAG_PROMPT_TEMPLATE\n",
    ")\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "print_section_header(\"Setup: Self-RAG\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = load_vector_store(OPENAI_VECTOR_STORE_PATH, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "llm = ChatOpenAI(model=DEFAULT_MODEL, temperature=0)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Self-RAG Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Self-RAG Components\")\n",
    "\n",
    "# Retrieval need decider\n",
    "retrieval_decider = RETRIEVAL_NEED_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "# Response critic\n",
    "response_critic = SELF_CRITIQUE_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "# Citation checker\n",
    "citation_checker = CITATION_CHECK_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "print(\"‚úì Self-RAG components initialized:\")\n",
    "print(\"  - Retrieval need decider\")\n",
    "print(\"  - Response critic\")\n",
    "print(\"  - Citation checker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Testing Self-RAG Components\")\n",
    "\n",
    "# Test retrieval decision\n",
    "queries = [\n",
    "    \"What is 2+2?\",  # NO retrieval\n",
    "    \"What is RAG in LangChain?\",  # YES retrieval\n",
    "]\n",
    "\n",
    "print(\"Retrieval Need Decisions:\\n\")\n",
    "for q in queries:\n",
    "    decision = retrieval_decider.invoke({\"query\": q}).strip()\n",
    "    print(f\"{decision:3} | {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Self-RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Self-RAG Pipeline\")\n",
    "\n",
    "def self_rag_pipeline(query: str, max_iterations: int = 2):\n",
    "    \"\"\"Self-RAG with iterative refinement.\"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    while iteration < max_iterations:\n",
    "        iteration += 1\n",
    "        print(f\"\\n--- Iteration {iteration} ---\")\n",
    "        \n",
    "        # Decide if retrieval needed\n",
    "        need_retrieval = retrieval_decider.invoke({\"query\": query})\n",
    "        print(f\"Retrieval needed: {need_retrieval.strip()}\")\n",
    "        \n",
    "        # Retrieve or use general knowledge\n",
    "        if \"YES\" in need_retrieval.upper():\n",
    "            docs = retriever.invoke(query)\n",
    "            context = format_docs(docs)\n",
    "            print(f\"Retrieved {len(docs)} documents\")\n",
    "        else:\n",
    "            context = \"Using general knowledge.\"\n",
    "            print(\"Using general knowledge only\")\n",
    "        \n",
    "        # Generate response\n",
    "        gen_prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", f\"Context: {context}\"),\n",
    "            (\"user\", \"{query}\")\n",
    "        ])\n",
    "        response = (gen_prompt | llm | StrOutputParser()).invoke({\"query\": query})\n",
    "        print(f\"\\nGenerated ({len(response)} chars)\")\n",
    "        \n",
    "        # Self-critique\n",
    "        critique = response_critic.invoke({\n",
    "            \"query\": query,\n",
    "            \"context\": context[:1000],\n",
    "            \"response\": response\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nüîç Critique:\\n{critique}\")\n",
    "        \n",
    "        # Check if retry needed\n",
    "        if \"SHOULD_RETRY: yes\" not in critique.lower():\n",
    "            print(\"\\n‚úì Response approved!\")\n",
    "            return response, iteration, critique\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  Retrying with refinement...\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  Max iterations reached\")\n",
    "    return response, iteration, critique\n",
    "\n",
    "print(\"‚úì Self-RAG pipeline ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Self-RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Self-RAG Test\")\n",
    "\n",
    "# Test 1: Query needing retrieval\n",
    "query1 = \"What is the difference between similarity and MMR retrieval?\"\n",
    "response1, iters1, _ = self_rag_pipeline(query1)\n",
    "\n",
    "print(f\"\\n\\nFinal Response ({iters1} iteration{'s' if iters1 > 1 else ''}):\")\n",
    "print(response1[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: Simple query (no retrieval)\n",
    "query2 = \"What is 5 + 7?\"\n",
    "response2, iters2, _ = self_rag_pipeline(query2)\n",
    "\n",
    "print(f\"\\n\\nFinal Response ({iters2} iteration{'s' if iters2 > 1 else ''}):\")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Advantages:**\n",
    "‚úÖ Autonomous decision-making  \n",
    "‚úÖ Self-correction capability  \n",
    "‚úÖ Only retrieves when needed (efficient)  \n",
    "‚úÖ Quality assurance built-in  \n",
    "\n",
    "**Limitations:**\n",
    "- Very slow (multiple LLM calls)\n",
    "- Expensive (iterations + critique)\n",
    "- Complex to tune\n",
    "- May over-correct\n",
    "\n",
    "**When to Use:**\n",
    "- Quality is paramount\n",
    "- Research applications\n",
    "- Self-correcting systems\n",
    "- Budget allows higher cost\n",
    "\n",
    "**Production Tips:**\n",
    "- Limit max iterations\n",
    "- Cache critique results\n",
    "- Monitor iteration distribution\n",
    "- Set quality thresholds\n",
    "\n",
    "**Next:** [10_agentic_rag.ipynb](10_agentic_rag.ipynb) - Autonomous agents with tools"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
