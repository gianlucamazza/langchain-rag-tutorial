{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 - Corrective RAG (CRAG)\n",
    "\n",
    "**Complexity:** ⭐⭐⭐⭐\n",
    "\n",
    "**Use Cases:** High-stakes domains (legal, medical), out-of-domain queries, fact-checking\n",
    "\n",
    "**Key Feature:** Grades document relevance, triggers web search if quality is low.\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "Query → Retrieve → Grade Relevance → \n",
    "  If good: Use retrieved docs\n",
    "  If poor: Web search + combine sources\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SETUP: CRAG\n",
      "================================================================================\n",
      "\n",
      "✓ Loaded vector store from /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/advanced_architectures/../../data/vector_stores/openai_embeddings\n",
      "✅ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from shared.config import OPENAI_VECTOR_STORE_PATH, DEFAULT_MODEL\n",
    "from shared.utils import load_vector_store, print_section_header, format_docs\n",
    "from shared.prompts import RELEVANCE_GRADER_PROMPT, CRAG_PROMPT\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "print_section_header(\"Setup: CRAG\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = load_vector_store(OPENAI_VECTOR_STORE_PATH, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "llm = ChatOpenAI(model=DEFAULT_MODEL, temperature=0)\n",
    "\n",
    "print(\"✅ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Relevance Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RELEVANCE GRADER\n",
      "================================================================================\n",
      "\n",
      "Query: 'What is RAG?'\n",
      "\n",
      "Testing relevance grader:\n",
      "  Relevant doc: yes\n",
      "  Irrelevant doc: no\n",
      "\n",
      "✓ Relevance grader working!\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Relevance Grader\")\n",
    "\n",
    "relevance_grader = RELEVANCE_GRADER_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "# Test grader\n",
    "query = \"What is RAG?\"\n",
    "relevant_doc = \"RAG stands for Retrieval-Augmented Generation, a technique that...\"\n",
    "irrelevant_doc = \"Python is a programming language used for...\"\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Testing relevance grader:\")\n",
    "print(f\"  Relevant doc: {relevance_grader.invoke({'question': query, 'document': relevant_doc}).strip()}\")\n",
    "print(f\"  Irrelevant doc: {relevance_grader.invoke({'question': query, 'document': irrelevant_doc}).strip()}\")\n",
    "\n",
    "print(\"\\n✓ Relevance grader working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "WEB SEARCH TOOL\n",
      "================================================================================\n",
      "\n",
      "⚠️  DuckDuckGo search not available\n",
      "   Install with: pip install duckduckgo-search\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Web Search Tool\")\n",
    "\n",
    "try:\n",
    "    from langchain_community.tools import DuckDuckGoSearchResults\n",
    "    \n",
    "    web_search = DuckDuckGoSearchResults(num_results=3)\n",
    "    print(\"✓ DuckDuckGo search tool initialized\")\n",
    "    \n",
    "    # Test\n",
    "    test_results = web_search.invoke(\"LangChain RAG tutorial\")\n",
    "    print(f\"\\nTest search results: {test_results[:200]}...\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠️  DuckDuckGo search not available\")\n",
    "    print(\"   Install with: pip install duckduckgo-search\")\n",
    "    web_search = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CRAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CRAG PIPELINE\n",
      "================================================================================\n",
      "\n",
      "✓ CRAG pipeline configured\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"CRAG Pipeline\")\n",
    "\n",
    "def crag_retrieve(query: str, threshold: float = 0.5):\n",
    "    \"\"\"CRAG retrieval with relevance grading and web fallback.\"\"\"\n",
    "    # Retrieve\n",
    "    docs = retriever.invoke(query)\n",
    "    \n",
    "    # Grade relevance\n",
    "    relevant_docs = []\n",
    "    for doc in docs:\n",
    "        grade = relevance_grader.invoke({\n",
    "            \"question\": query,\n",
    "            \"document\": doc.page_content[:1000]\n",
    "        })\n",
    "        if \"yes\" in grade.lower():\n",
    "            relevant_docs.append(doc)\n",
    "    \n",
    "    relevance_ratio = len(relevant_docs) / len(docs) if docs else 0\n",
    "    used_web = False\n",
    "    web_results = \"\"\n",
    "    \n",
    "    # Web search if poor relevance\n",
    "    if relevance_ratio < threshold and web_search:\n",
    "        print(f\"⚠️  Low relevance ({relevance_ratio:.0%}), using web search...\")\n",
    "        web_results = web_search.invoke(query)\n",
    "        used_web = True\n",
    "    \n",
    "    context = format_docs(relevant_docs)\n",
    "    if web_results:\n",
    "        context += f\"\\n\\n[WEB SEARCH]\\n{web_results}\"\n",
    "    \n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"input\": query,\n",
    "        \"used_web\": used_web,\n",
    "        \"relevance_ratio\": relevance_ratio\n",
    "    }\n",
    "\n",
    "print(\"✓ CRAG pipeline configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CRAG Chain & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CRAG TESTING\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Test 1 (in-domain): 'What are vector stores in RAG?'\n",
      "================================================================================\n",
      "In the context of Retrieval-Augmented Generation (RAG), vector stores refer to specialized data structures or databases that store embeddings of documents or pieces of information in a high-dimensional vector space. These embeddings are typically gen\n",
      "\n",
      "\n",
      "✅ CRAG adapts to document quality!\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"CRAG Testing\")\n",
    "\n",
    "crag_chain = (\n",
    "    RunnableLambda(crag_retrieve)\n",
    "    | CRAG_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test 1: In-domain (should NOT trigger web)\n",
    "query1 = \"What are vector stores in RAG?\"\n",
    "print(f\"\\nTest 1 (in-domain): '{query1}'\")\n",
    "print(\"=\" * 80)\n",
    "response1 = crag_chain.invoke(query1)\n",
    "print(response1[:250])\n",
    "\n",
    "# Test 2: Out-of-domain (should trigger web)\n",
    "if web_search:\n",
    "    query2 = \"What is the latest Python version?\"\n",
    "    print(f\"\\n\\nTest 2 (out-of-domain): '{query2}'\")\n",
    "    print(\"=\" * 80)\n",
    "    response2 = crag_chain.invoke(query2)\n",
    "    print(response2[:250])\n",
    "\n",
    "print(\"\\n\\n✅ CRAG adapts to document quality!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Advantages:**\n",
    "✅ High accuracy through quality checking  \n",
    "✅ Web fallback for missing info  \n",
    "✅ Robust to out-of-domain queries  \n",
    "✅ Transparent (shows when web used)  \n",
    "\n",
    "**Limitations:**\n",
    "- Slow (grading + potential web search)\n",
    "- High cost (multiple LLM calls)\n",
    "- Depends on web search quality\n",
    "\n",
    "**When to Use:**\n",
    "- High-accuracy requirements\n",
    "- Out-of-domain queries expected\n",
    "- Legal, medical, financial domains\n",
    "\n",
    "**Next:** [09_self_rag.ipynb](09_self_rag.ipynb) - Self-reflective RAG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
