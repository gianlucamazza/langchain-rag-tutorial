{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 - Corrective RAG (CRAG)\n",
    "\n",
    "**Complexity:** ⭐⭐⭐⭐\n",
    "\n",
    "**Use Cases:** High-stakes domains (legal, medical), out-of-domain queries, fact-checking\n",
    "\n",
    "**Key Feature:** Grades document relevance, triggers web search if quality is low.\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "Query → Retrieve → Grade Relevance → \n",
    "  If good: Use retrieved docs\n",
    "  If poor: Web search + combine sources\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from shared.config import OPENAI_VECTOR_STORE_PATH, DEFAULT_MODEL\n",
    "from shared.utils import load_vector_store, print_section_header, format_docs\n",
    "from shared.prompts import RELEVANCE_GRADER_PROMPT, CRAG_PROMPT\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "print_section_header(\"Setup: CRAG\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = load_vector_store(OPENAI_VECTOR_STORE_PATH, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "llm = ChatOpenAI(model=DEFAULT_MODEL, temperature=0)\n",
    "\n",
    "print(\"✅ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Relevance Grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Relevance Grader\")\n",
    "\n",
    "relevance_grader = RELEVANCE_GRADER_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "# Test grader\n",
    "query = \"What is RAG?\"\n",
    "relevant_doc = \"RAG stands for Retrieval-Augmented Generation, a technique that...\"\n",
    "irrelevant_doc = \"Python is a programming language used for...\"\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Testing relevance grader:\")\n",
    "print(f\"  Relevant doc: {relevance_grader.invoke({'question': query, 'document': relevant_doc}).strip()}\")\n",
    "print(f\"  Irrelevant doc: {relevance_grader.invoke({'question': query, 'document': irrelevant_doc}).strip()}\")\n",
    "\n",
    "print(\"\\n✓ Relevance grader working!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Web Search Tool\")\n",
    "\n",
    "try:\n",
    "    from langchain_community.tools import DuckDuckGoSearchResults\n",
    "    \n",
    "    web_search = DuckDuckGoSearchResults(num_results=3)\n",
    "    print(\"✓ DuckDuckGo search tool initialized\")\n",
    "    \n",
    "    # Test\n",
    "    test_results = web_search.invoke(\"LangChain RAG tutorial\")\n",
    "    print(f\"\\nTest search results: {test_results[:200]}...\")\n",
    "    \nexcept ImportError:\n",
    "    print(\"⚠️  DuckDuckGo search not available\")\n",
    "    print(\"   Install with: pip install duckduckgo-search\")\n",
    "    web_search = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. CRAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"CRAG Pipeline\")\n",
    "\n",
    "def crag_retrieve(query: str, threshold: float = 0.5):\n",
    "    \"\"\"CRAG retrieval with relevance grading and web fallback.\"\"\"\n",
    "    # Retrieve\n",
    "    docs = retriever.invoke(query)\n",
    "    \n",
    "    # Grade relevance\n",
    "    relevant_docs = []\n",
    "    for doc in docs:\n",
    "        grade = relevance_grader.invoke({\n",
    "            \"question\": query,\n",
    "            \"document\": doc.page_content[:1000]\n",
    "        })\n",
    "        if \"yes\" in grade.lower():\n",
    "            relevant_docs.append(doc)\n",
    "    \n",
    "    relevance_ratio = len(relevant_docs) / len(docs) if docs else 0\n",
    "    used_web = False\n",
    "    web_results = \"\"\n",
    "    \n",
    "    # Web search if poor relevance\n",
    "    if relevance_ratio < threshold and web_search:\n",
    "        print(f\"⚠️  Low relevance ({relevance_ratio:.0%}), using web search...\")\n",
    "        web_results = web_search.invoke(query)\n",
    "        used_web = True\n",
    "    \n",
    "    context = format_docs(relevant_docs)\n",
    "    if web_results:\n",
    "        context += f\"\\n\\n[WEB SEARCH]\\n{web_results}\"\n",
    "    \n",
    "    return {\n",
    "        \"context\": context,\n",
    "        \"input\": query,\n",
    "        \"used_web\": used_web,\n",
    "        \"relevance_ratio\": relevance_ratio\n",
    "    }\n",
    "\n",
    "print(\"✓ CRAG pipeline configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. CRAG Chain & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"CRAG Testing\")\n",
    "\n",
    "crag_chain = (\n",
    "    RunnableLambda(crag_retrieve)\n",
    "    | CRAG_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test 1: In-domain (should NOT trigger web)\n",
    "query1 = \"What are vector stores in RAG?\"\n",
    "print(f\"\\nTest 1 (in-domain): '{query1}'\")\n",
    "print(\"=\" * 80)\n",
    "response1 = crag_chain.invoke(query1)\n",
    "print(response1[:250])\n",
    "\n",
    "# Test 2: Out-of-domain (should trigger web)\n",
    "if web_search:\n",
    "    query2 = \"What is the latest Python version?\"\n",
    "    print(f\"\\n\\nTest 2 (out-of-domain): '{query2}'\")\n",
    "    print(\"=\" * 80)\n",
    "    response2 = crag_chain.invoke(query2)\n",
    "    print(response2[:250])\n",
    "\n",
    "print(\"\\n\\n✅ CRAG adapts to document quality!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Advantages:**\n",
    "✅ High accuracy through quality checking  \n",
    "✅ Web fallback for missing info  \n",
    "✅ Robust to out-of-domain queries  \n",
    "✅ Transparent (shows when web used)  \n",
    "\n",
    "**Limitations:**\n",
    "- Slow (grading + potential web search)\n",
    "- High cost (multiple LLM calls)\n",
    "- Depends on web search quality\n",
    "\n",
    "**When to Use:**\n",
    "- High-accuracy requirements\n",
    "- Out-of-domain queries expected\n",
    "- Legal, medical, financial domains\n",
    "\n",
    "**Next:** [09_self_rag.ipynb](09_self_rag.ipynb) - Self-reflective RAG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
