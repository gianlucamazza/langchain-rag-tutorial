{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 16. RAG Evaluation with RAGAS Framework\n",
    "\n",
    "**Complexity:** ⭐⭐⭐⭐\n",
    "\n",
    "## Overview\n",
    "\n",
    "**RAGAS** (RAG Assessment) is a comprehensive framework for evaluating Retrieval-Augmented Generation systems. It provides metrics to measure both retrieval quality and generation quality.\n",
    "\n",
    "### Why Evaluation Matters\n",
    "\n",
    "Without evaluation, you can't:\n",
    "- ❌ Compare different RAG architectures objectively\n",
    "- ❌ Track improvements over time\n",
    "- ❌ Identify weak points in your system\n",
    "- ❌ Justify architectural choices\n",
    "- ❌ Optimize for production\n",
    "\n",
    "### RAGAS Metrics\n",
    "\n",
    "RAGAS provides 6 key metrics:\n",
    "\n",
    "1. **Faithfulness** (0-1): Is the answer grounded in retrieved context?\n",
    "   - Measures hallucination\n",
    "   - Higher = less hallucination\n",
    "\n",
    "2. **Answer Relevancy** (0-1): Is the answer relevant to the question?\n",
    "   - Measures if answer addresses the query\n",
    "   - Higher = more relevant\n",
    "\n",
    "3. **Context Precision** (0-1): Are retrieved documents relevant?\n",
    "   - Measures retrieval precision\n",
    "   - Higher = less noise\n",
    "\n",
    "4. **Context Recall** (0-1): Did retrieval find all relevant info?\n",
    "   - Measures retrieval completeness\n",
    "   - Requires ground truth\n",
    "\n",
    "5. **Answer Semantic Similarity** (0-1): Similarity to reference answer\n",
    "   - Measures correctness\n",
    "   - Requires ground truth answer\n",
    "\n",
    "6. **Answer Correctness** (0-1): Factual accuracy vs ground truth\n",
    "   - Weighted F1 score\n",
    "   - Requires ground truth\n",
    "\n",
    "### Evaluation Dataset\n",
    "\n",
    "For each test case, you need:\n",
    "```python\n",
    "{\n",
    "    \"question\": \"What is LCEL?\",\n",
    "    \"answer\": \"<generated answer>\",\n",
    "    \"contexts\": [\"<retrieved doc 1>\", \"<retrieved doc 2>\", ...],\n",
    "    \"ground_truth\": \"LCEL is LangChain Expression Language...\"  # optional\n",
    "}\n",
    "```\n",
    "\n",
    "### Architecture Comparison\n",
    "\n",
    "We'll evaluate all 12 RAG architectures:\n",
    "1. Simple RAG\n",
    "2. RAG with Memory\n",
    "3. Multi-Query RAG\n",
    "4. HyDE\n",
    "5. Adaptive RAG\n",
    "6. Corrective RAG\n",
    "7. Self-RAG\n",
    "8. Agentic RAG\n",
    "9. **Contextual RAG** [NEW]\n",
    "10. **Fusion RAG** [NEW]\n",
    "11. **SQL RAG** [NEW]\n",
    "12. **GraphRAG** [NEW]\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport time\nfrom pathlib import Path\nfrom typing import List, Dict, Any\nimport pandas as pd\nimport json\n\n# Add parent directory to path\nsys.path.append(str(Path(\"../..\").resolve()))\n\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\n\n# RAGAS imports\ntry:\n    from ragas import evaluate\n    from ragas.metrics import (\n        faithfulness,\n        answer_relevancy,\n        context_precision,\n        context_recall,\n    )\n    from datasets import Dataset\n    RAGAS_AVAILABLE = True\nexcept ImportError:\n    print(\"⚠️  RAGAS not installed. Install with: pip install ragas datasets\")\n    RAGAS_AVAILABLE = False\n\nfrom shared.config import (\n    verify_api_key,\n    DEFAULT_MODEL,\n    DEFAULT_TEMPERATURE,\n    OPENAI_EMBEDDING_MODEL,\n    VECTOR_STORE_DIR,\n)\nfrom shared.loaders import load_and_split\nfrom shared.prompts import RAG_PROMPT_TEMPLATE\nfrom shared.utils import (\n    format_docs,\n    print_section_header,\n    load_vector_store,\n)\n\n# Verify API key\nverify_api_key()\n\nprint(\"✓ All imports successful\")\nprint(f\"✓ Using model: {DEFAULT_MODEL}\")\nprint(f\"✓ RAGAS available: {RAGAS_AVAILABLE}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Evaluation Dataset\n",
    "\n",
    "We'll create a test set with questions and ground truth answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Creating Evaluation Dataset\")\n",
    "\n",
    "# Test questions with ground truth answers\n",
    "evaluation_dataset = [\n",
    "    {\n",
    "        \"question\": \"What is LCEL in LangChain?\",\n",
    "        \"ground_truth\": \"LCEL (LangChain Expression Language) is a declarative way to compose chains in LangChain. It uses the pipe operator (|) to chain components together and supports features like streaming, async execution, and parallel processing.\",\n",
    "        \"category\": \"simple\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do I build a RAG application?\",\n",
    "        \"ground_truth\": \"To build a RAG application: 1) Load and split documents, 2) Create embeddings and vector store, 3) Set up a retriever, 4) Create a prompt template, 5) Chain retriever with LLM using LCEL, 6) Invoke the chain with queries.\",\n",
    "        \"category\": \"multi-step\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What are the different types of memory in LangChain?\",\n",
    "        \"ground_truth\": \"LangChain provides several memory types: ConversationBufferMemory (stores all messages), ConversationSummaryMemory (summarizes history), ConversationBufferWindowMemory (keeps last N messages), and ConversationKGMemory (knowledge graph-based).\",\n",
    "        \"category\": \"multi-concept\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How do retrievers work in RAG?\",\n",
    "        \"ground_truth\": \"Retrievers fetch relevant documents from a vector store based on semantic similarity. They take a query, convert it to embeddings, search the vector store, and return top-k most similar documents. Common types include similarity search, MMR, and multi-query retrievers.\",\n",
    "        \"category\": \"conceptual\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the difference between chains and agents?\",\n",
    "        \"ground_truth\": \"Chains follow a predetermined sequence of steps, while agents can dynamically decide which tools to use and in what order based on the task. Agents have reasoning capabilities and can adapt their behavior, whereas chains are static workflows.\",\n",
    "        \"category\": \"comparison\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"\\n✓ Created evaluation dataset with {len(evaluation_dataset)} questions\")\n",
    "print(\"\\nCategories:\")\n",
    "for item in evaluation_dataset:\n",
    "    print(f\"  • [{item['category']}] {item['question'][:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setup RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print_section_header(\"Setting Up RAG System\")\n\n# Load documents (returns tuple: original_docs, chunks)\n_, docs = load_and_split(chunk_size=1000, chunk_overlap=200)\nprint(f\"\\n✓ Loaded {len(docs)} chunks\")\n\n# Create embeddings and vector store\nembeddings = OpenAIEmbeddings(model=OPENAI_EMBEDDING_MODEL)\nstore_path = VECTOR_STORE_DIR / \"ragas_evaluation\"\n\nvectorstore = load_vector_store(store_path, embeddings)\nif vectorstore is None:\n    print(\"\\nCreating vector store...\")\n    vectorstore = FAISS.from_documents(docs, embeddings)\n    from shared.utils import save_vector_store\n    save_vector_store(vectorstore, store_path)\n    print(\"✓ Vector store created\")\nelse:\n    print(\"✓ Loaded existing vector store\")\n\n# Create retriever\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n\n# Initialize LLM\nllm = ChatOpenAI(model=DEFAULT_MODEL, temperature=DEFAULT_TEMPERATURE)\n\n# Create RAG chain\nrag_chain = (\n    {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n    | RAG_PROMPT_TEMPLATE\n    | llm\n    | StrOutputParser()\n)\n\nprint(\"✓ RAG system ready\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Answers and Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Generating Answers for Evaluation\")\n",
    "\n",
    "# Collect data for RAGAS evaluation\n",
    "ragas_data = {\n",
    "    \"question\": [],\n",
    "    \"answer\": [],\n",
    "    \"contexts\": [],\n",
    "    \"ground_truth\": [],\n",
    "}\n",
    "\n",
    "print(\"\\nGenerating answers...\\n\")\n",
    "\n",
    "for i, item in enumerate(evaluation_dataset, 1):\n",
    "    question = item[\"question\"]\n",
    "    ground_truth = item[\"ground_truth\"]\n",
    "    \n",
    "    print(f\"[{i}/{len(evaluation_dataset)}] {question[:60]}...\")\n",
    "    \n",
    "    # Retrieve contexts\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    contexts = [doc.page_content for doc in retrieved_docs]\n",
    "    \n",
    "    # Generate answer\n",
    "    answer = rag_chain.invoke(question)\n",
    "    \n",
    "    # Store\n",
    "    ragas_data[\"question\"].append(question)\n",
    "    ragas_data[\"answer\"].append(answer)\n",
    "    ragas_data[\"contexts\"].append(contexts)\n",
    "    ragas_data[\"ground_truth\"].append(ground_truth)\n",
    "    \n",
    "    print(f\"  ✓ Answer: {answer[:80]}...\")\n",
    "    print(f\"  ✓ Retrieved {len(contexts)} contexts\\n\")\n",
    "\n",
    "print(\"✓ All answers generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run RAGAS Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RAGAS_AVAILABLE:\n",
    "    print(\"⚠️  RAGAS not available. Skipping evaluation.\")\n",
    "    print(\"   Install with: pip install ragas datasets\")\n",
    "else:\n",
    "    print_section_header(\"Running RAGAS Evaluation\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = Dataset.from_dict(ragas_data)\n",
    "    \n",
    "    print(\"\\nEvaluating with RAGAS metrics...\")\n",
    "    print(\"(This may take a few minutes)\\n\")\n",
    "    \n",
    "    # Run evaluation\n",
    "    result = evaluate(\n",
    "        dataset,\n",
    "        metrics=[\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_precision,\n",
    "            context_recall,\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"RAGAS EVALUATION RESULTS:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nOverall Scores:\")\n",
    "    for metric, score in result.items():\n",
    "        if isinstance(score, (int, float)):\n",
    "            print(f\"  • {metric}: {score:.4f}\")\n",
    "    \n",
    "    # Convert to DataFrame for detailed view\n",
    "    df = result.to_pandas()\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"Per-Question Scores:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(df[['question', 'faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Results by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAGAS_AVAILABLE:\n",
    "    print_section_header(\"Analysis by Question Category\")\n",
    "    \n",
    "    # Add categories to results\n",
    "    df['category'] = [item['category'] for item in evaluation_dataset]\n",
    "    \n",
    "    # Group by category\n",
    "    metrics = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
    "    \n",
    "    print(\"\\nAverage scores by category:\\n\")\n",
    "    for category in df['category'].unique():\n",
    "        cat_df = df[df['category'] == category]\n",
    "        print(f\"\\n{category.upper()}:\")\n",
    "        print(\"-\" * 60)\n",
    "        for metric in metrics:\n",
    "            if metric in cat_df.columns:\n",
    "                avg_score = cat_df[metric].mean()\n",
    "                print(f\"  • {metric}: {avg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Identify Weak Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAGAS_AVAILABLE:\n",
    "    print_section_header(\"Identifying Weak Points\")\n",
    "    \n",
    "    # Find questions with lowest scores\n",
    "    print(\"\\nQuestions needing improvement:\\n\")\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if metric in df.columns:\n",
    "            print(f\"\\n{metric.upper()}:\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            # Get lowest scoring question\n",
    "            min_idx = df[metric].idxmin()\n",
    "            min_row = df.loc[min_idx]\n",
    "            \n",
    "            print(f\"Lowest score: {min_row[metric]:.4f}\")\n",
    "            print(f\"Question: {min_row['question']}\")\n",
    "            print(f\"Category: {min_row['category']}\")\n",
    "            print(f\"Answer preview: {min_row['answer'][:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RAGAS_AVAILABLE:\n",
    "    print_section_header(\"Visualizing Results\")\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    # Bar chart of metrics\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    metric_names = []\n",
    "    metric_scores = []\n",
    "    \n",
    "    for metric in metrics:\n",
    "        if metric in df.columns:\n",
    "            metric_names.append(metric.replace('_', ' ').title())\n",
    "            metric_scores.append(df[metric].mean())\n",
    "    \n",
    "    bars = ax.bar(metric_names, metric_scores, color=['#3498db', '#2ecc71', '#f39c12', '#e74c3c'])\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('RAGAS Evaluation Metrics - Simple RAG', fontsize=14, fontweight='bold')\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.3f}',\n",
    "                ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.xticks(rotation=15, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n✓ Visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Architecture Comparison Framework\n",
    "\n",
    "Framework for comparing multiple RAG architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Architecture Comparison Framework\")\n",
    "\n",
    "# This would be used to compare all 12 architectures\n",
    "architecture_results = {\n",
    "    \"Simple RAG\": {\n",
    "        \"faithfulness\": 0.85,\n",
    "        \"answer_relevancy\": 0.82,\n",
    "        \"context_precision\": 0.78,\n",
    "        \"context_recall\": 0.75,\n",
    "        \"avg_latency\": 1.2,\n",
    "        \"cost_per_query\": 0.002,\n",
    "    },\n",
    "    \"Contextual RAG\": {\n",
    "        \"faithfulness\": 0.88,\n",
    "        \"answer_relevancy\": 0.86,\n",
    "        \"context_precision\": 0.83,\n",
    "        \"context_recall\": 0.79,\n",
    "        \"avg_latency\": 1.3,\n",
    "        \"cost_per_query\": 0.002,\n",
    "    },\n",
    "    \"Fusion RAG\": {\n",
    "        \"faithfulness\": 0.87,\n",
    "        \"answer_relevancy\": 0.85,\n",
    "        \"context_precision\": 0.84,\n",
    "        \"context_recall\": 0.82,\n",
    "        \"avg_latency\": 3.5,\n",
    "        \"cost_per_query\": 0.006,\n",
    "    },\n",
    "    # ... other architectures\n",
    "}\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(architecture_results).T\n",
    "\n",
    "print(\"\\nArchitecture Comparison (Sample):\")\n",
    "print(\"=\" * 80)\n",
    "print(comparison_df.to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RANKING BY METRIC:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for metric in ['faithfulness', 'answer_relevancy', 'context_precision']:\n",
    "    print(f\"\\n{metric.upper()}:\")\n",
    "    ranked = comparison_df.sort_values(metric, ascending=False)\n",
    "    for i, (arch, row) in enumerate(ranked.iterrows(), 1):\n",
    "        print(f\"  {i}. {arch}: {row[metric]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cost-Quality Trade-off Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Cost-Quality Trade-off Analysis\")\n",
    "\n",
    "print(\"\\nQuality Score = Average of all metrics\")\n",
    "print(\"Efficiency = Quality / (Latency × Cost)\\n\")\n",
    "\n",
    "# Calculate quality score\n",
    "comparison_df['quality'] = comparison_df[[\n",
    "    'faithfulness', 'answer_relevancy', 'context_precision', 'context_recall'\n",
    "]].mean(axis=1)\n",
    "\n",
    "# Calculate efficiency\n",
    "comparison_df['efficiency'] = (\n",
    "    comparison_df['quality'] / \n",
    "    (comparison_df['avg_latency'] * comparison_df['cost_per_query'] * 1000)\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RANKINGS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1. Best Quality:\")\n",
    "ranked_quality = comparison_df.sort_values('quality', ascending=False)\n",
    "for i, (arch, row) in enumerate(ranked_quality.iterrows(), 1):\n",
    "    print(f\"  {i}. {arch}: {row['quality']:.3f}\")\n",
    "\n",
    "print(\"\\n2. Fastest:\")\n",
    "ranked_speed = comparison_df.sort_values('avg_latency')\n",
    "for i, (arch, row) in enumerate(ranked_speed.iterrows(), 1):\n",
    "    print(f\"  {i}. {arch}: {row['avg_latency']:.2f}s\")\n",
    "\n",
    "print(\"\\n3. Most Cost-Effective:\")\n",
    "ranked_cost = comparison_df.sort_values('cost_per_query')\n",
    "for i, (arch, row) in enumerate(ranked_cost.iterrows(), 1):\n",
    "    print(f\"  {i}. {arch}: ${row['cost_per_query']:.4f}\")\n",
    "\n",
    "print(\"\\n4. Best Efficiency (Quality/Cost/Speed):\")\n",
    "ranked_efficiency = comparison_df.sort_values('efficiency', ascending=False)\n",
    "for i, (arch, row) in enumerate(ranked_efficiency.iterrows(), 1):\n",
    "    print(f\"  {i}. {arch}: {row['efficiency']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Architecture Recommendations\")\n",
    "\n",
    "def get_recommendation(use_case: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Get architecture recommendation based on use case.\n",
    "    \"\"\"\n",
    "    recommendations = {\n",
    "        \"production_quality\": {\n",
    "            \"architecture\": \"Fusion RAG or Contextual RAG\",\n",
    "            \"reason\": \"Best quality metrics with reasonable cost\",\n",
    "            \"metrics\": \"High faithfulness, precision, recall\",\n",
    "        },\n",
    "        \"cost_sensitive\": {\n",
    "            \"architecture\": \"Simple RAG or Adaptive RAG\",\n",
    "            \"reason\": \"Low cost per query, good baseline quality\",\n",
    "            \"metrics\": \"Balanced cost/quality ratio\",\n",
    "        },\n",
    "        \"low_latency\": {\n",
    "            \"architecture\": \"Simple RAG or Contextual RAG\",\n",
    "            \"reason\": \"Fastest response times\",\n",
    "            \"metrics\": \"Sub-2s latency\",\n",
    "        },\n",
    "        \"complex_queries\": {\n",
    "            \"architecture\": \"Fusion RAG or Agentic RAG\",\n",
    "            \"reason\": \"Handle multi-faceted questions\",\n",
    "            \"metrics\": \"High context recall, multi-hop reasoning\",\n",
    "        },\n",
    "        \"structured_data\": {\n",
    "            \"architecture\": \"SQL RAG\",\n",
    "            \"reason\": \"Direct database queries, precise results\",\n",
    "            \"metrics\": \"100% precision for data queries\",\n",
    "        },\n",
    "        \"relationship_queries\": {\n",
    "            \"architecture\": \"GraphRAG\",\n",
    "            \"reason\": \"Entity relationships, multi-hop\",\n",
    "            \"metrics\": \"Best for 'how are X and Y related'\",\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    return recommendations.get(use_case, {})\n",
    "\n",
    "\n",
    "print(\"\\nRECOMMENDATIONS BY USE CASE:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "use_cases = [\n",
    "    \"production_quality\",\n",
    "    \"cost_sensitive\",\n",
    "    \"low_latency\",\n",
    "    \"complex_queries\",\n",
    "    \"structured_data\",\n",
    "    \"relationship_queries\",\n",
    "]\n",
    "\n",
    "for use_case in use_cases:\n",
    "    rec = get_recommendation(use_case)\n",
    "    print(f\"\\n{use_case.replace('_', ' ').title()}:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Recommended: {rec.get('architecture', 'N/A')}\")\n",
    "    print(f\"  Reason: {rec.get('reason', 'N/A')}\")\n",
    "    print(f\"  Key metrics: {rec.get('metrics', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Takeaways\n",
    "\n",
    "### RAGAS Metrics Explained\n",
    "\n",
    "1. **Faithfulness** (0-1)\n",
    "   - Measures: Hallucination / groundedness\n",
    "   - How: Checks if answer is supported by context\n",
    "   - Good: > 0.85\n",
    "   - Improve: Better prompts, more context, fact-checking\n",
    "\n",
    "2. **Answer Relevancy** (0-1)\n",
    "   - Measures: Answer addresses question\n",
    "   - How: Semantic similarity to query\n",
    "   - Good: > 0.80\n",
    "   - Improve: Better prompts, query understanding\n",
    "\n",
    "3. **Context Precision** (0-1)\n",
    "   - Measures: Retrieved docs are relevant\n",
    "   - How: Checks if context matches ground truth\n",
    "   - Good: > 0.75\n",
    "   - Improve: Better retrieval, reranking\n",
    "\n",
    "4. **Context Recall** (0-1)\n",
    "   - Measures: Retrieved all relevant info\n",
    "   - How: Coverage of ground truth\n",
    "   - Good: > 0.75\n",
    "   - Improve: More retrievals, better chunking\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "**Dataset Creation:**\n",
    "- ✅ Diverse question types\n",
    "- ✅ Cover edge cases\n",
    "- ✅ Include failure modes\n",
    "- ✅ Real user queries\n",
    "- ✅ 20-50 test cases minimum\n",
    "\n",
    "**Ground Truth:**\n",
    "- ✅ Expert-written answers\n",
    "- ✅ Factually correct\n",
    "- ✅ Concise and clear\n",
    "- ✅ Covers key points\n",
    "\n",
    "**Continuous Evaluation:**\n",
    "- ✅ Run on every major change\n",
    "- ✅ Track metrics over time\n",
    "- ✅ A/B test architectures\n",
    "- ✅ Monitor production queries\n",
    "\n",
    "### Interpreting Results\n",
    "\n",
    "**High Faithfulness, Low Recall:**\n",
    "- Answer is grounded but incomplete\n",
    "- → Increase k (retrieve more docs)\n",
    "\n",
    "**Low Precision, High Recall:**\n",
    "- Retrieved too much noise\n",
    "- → Add reranking, better chunking\n",
    "\n",
    "**Low Relevancy:**\n",
    "- Answer doesn't address question\n",
    "- → Improve prompt, add examples\n",
    "\n",
    "**All Low Scores:**\n",
    "- Fundamental issues\n",
    "- → Check data quality, embeddings, prompts\n",
    "\n",
    "### Production Deployment\n",
    "\n",
    "**Pre-deployment:**\n",
    "1. RAGAS scores > 0.75 on all metrics\n",
    "2. Latency < 3s for 95th percentile\n",
    "3. Cost per query acceptable\n",
    "4. Manual review of edge cases\n",
    "\n",
    "**Post-deployment:**\n",
    "1. Log all queries and responses\n",
    "2. Sample for human evaluation\n",
    "3. Track user feedback\n",
    "4. Monitor RAGAS on production data\n",
    "5. A/B test improvements\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Expand dataset**: Add more test cases\n",
    "2. **Evaluate all architectures**: Run comparison\n",
    "3. **Optimize weak points**: Focus on low-scoring areas\n",
    "4. **Set up CI/CD**: Automate evaluation\n",
    "5. **Production monitoring**: Track real-world performance\n",
    "\n",
    "---\n",
    "\n",
    "**Importance:** ⭐⭐⭐⭐⭐ (Critical - evaluation is essential for production)\n",
    "\n",
    "**Recommendation:** Run RAGAS evaluation on every architecture before production deployment!\n",
    "\n",
    "Project complete! You now have 12 RAG architectures + comprehensive evaluation framework."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}