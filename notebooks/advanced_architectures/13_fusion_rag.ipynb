{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Fusion RAG - Reciprocal Rank Fusion\n",
    "\n",
    "**Complexity:** ⭐⭐⭐\n",
    "\n",
    "## Overview\n",
    "\n",
    "**RAG-Fusion** combines multiple query perspectives using Reciprocal Rank Fusion (RRF) to improve retrieval quality. It's an enhancement over Multi-Query RAG (notebook 05) that uses a more sophisticated fusion algorithm.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Single queries have limitations:\n",
    "- May miss relevant documents due to wording\n",
    "- Can't capture multiple aspects of complex questions\n",
    "- Vector similarity is sensitive to phrasing\n",
    "\n",
    "Multi-Query RAG helps, but simple deduplication loses ranking information.\n",
    "\n",
    "### The Solution\n",
    "\n",
    "RAG-Fusion improves on Multi-Query RAG by:\n",
    "1. Generating multiple query variations (like Multi-Query)\n",
    "2. Retrieving documents for each query\n",
    "3. **Using Reciprocal Rank Fusion** to combine results intelligently\n",
    "4. Re-ranking documents based on combined scores\n",
    "\n",
    "### Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "RRF is a powerful rank aggregation method:\n",
    "\n",
    "```\n",
    "RRF_score(doc) = Σ (1 / (k + rank_i))\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `k` = constant (typically 60)\n",
    "- `rank_i` = rank of document in i-th query results\n",
    "- Sum across all queries where document appears\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Query 1 ranks: [A, B, C, D]\n",
    "Query 2 ranks: [C, A, E, F]\n",
    "Query 3 ranks: [B, C, A, G]\n",
    "\n",
    "Document A:\n",
    "  - Appears at rank 1 in Q1: 1/(60+1) = 0.0164\n",
    "  - Appears at rank 2 in Q2: 1/(60+2) = 0.0161\n",
    "  - Appears at rank 3 in Q3: 1/(60+3) = 0.0159\n",
    "  - Total RRF score: 0.0484\n",
    "\n",
    "Document C:\n",
    "  - Appears at rank 3 in Q1: 1/(60+3) = 0.0159\n",
    "  - Appears at rank 1 in Q2: 1/(60+1) = 0.0164\n",
    "  - Appears at rank 2 in Q3: 1/(60+2) = 0.0161\n",
    "  - Total RRF score: 0.0484\n",
    "```\n",
    "\n",
    "**Why RRF works:**\n",
    "- ✅ Favors documents that appear in multiple query results\n",
    "- ✅ Considers both frequency and rank position\n",
    "- ✅ No need for score normalization\n",
    "- ✅ Robust to outliers\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "```\n",
    "Query → Generate N alternative queries → Retrieve for each query\n",
    "    → Apply RRF algorithm → Re-rank documents → Generate answer\n",
    "```\n",
    "\n",
    "### Fusion RAG vs Multi-Query RAG\n",
    "\n",
    "| Aspect | Multi-Query RAG | Fusion RAG |\n",
    "|--------|-----------------|------------|\n",
    "| Query generation | ✅ Multiple queries | ✅ Multiple queries |\n",
    "| Retrieval | ✅ Parallel | ✅ Parallel |\n",
    "| Fusion method | Simple deduplication | **RRF algorithm** |\n",
    "| Ranking | Lost | **Preserved and combined** |\n",
    "| Quality | Good | **Better** |\n",
    "| Complexity | Low | Medium |\n",
    "\n",
    "### When to Use\n",
    "\n",
    "✅ **Good for:**\n",
    "- Complex, multi-faceted questions\n",
    "- Queries with multiple valid interpretations\n",
    "- When ranking quality is important\n",
    "- When you want robust retrieval\n",
    "\n",
    "❌ **Not ideal for:**\n",
    "- Simple factual lookups\n",
    "- Latency-critical applications\n",
    "- Limited API budgets\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "**Pros:**\n",
    "- ✅ Better retrieval than single-query RAG\n",
    "- ✅ More sophisticated than Multi-Query RAG\n",
    "- ✅ Robust ranking algorithm\n",
    "- ✅ Captures diverse perspectives\n",
    "\n",
    "**Cons:**\n",
    "- ❌ Higher latency (multiple retrievals)\n",
    "- ❌ More API calls (query generation)\n",
    "- ❌ More complex implementation\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Let's build RAG-Fusion step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OpenAI API Key: LOADED\n",
      "  Preview: sk-proj...vIQA\n",
      "✓ All imports successful\n",
      "✓ Using model: gpt-4o-mini\n",
      "✓ Using embeddings: text-embedding-3-small\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(str(Path(\"../..\").resolve()))\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from shared.config import (\n",
    "    verify_api_key,\n",
    "    DEFAULT_MODEL,\n",
    "    DEFAULT_TEMPERATURE,\n",
    "    OPENAI_EMBEDDING_MODEL,\n",
    "    VECTOR_STORE_DIR,\n",
    ")\n",
    "from shared.loaders import load_and_split\n",
    "from shared.prompts import (\n",
    "    FUSION_QUERY_GENERATION_PROMPT,\n",
    "    FUSION_RAG_ANSWER_PROMPT,\n",
    ")\n",
    "from shared.utils import (\n",
    "    format_docs,\n",
    "    print_section_header,\n",
    "    load_vector_store,\n",
    "    save_vector_store,\n",
    ")\n",
    "\n",
    "# Verify API key\n",
    "verify_api_key()\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "print(f\"✓ Using model: {DEFAULT_MODEL}\")\n",
    "print(f\"✓ Using embeddings: {OPENAI_EMBEDDING_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Documents and Create Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING DOCUMENTS AND VECTOR STORE\n",
      "================================================================================\n",
      "\n",
      "Loading 4 documents from web...\n",
      "  - https://python.langchain.com/docs/use_cases/question_answering/\n",
      "  - https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
      "  - https://python.langchain.com/docs/modules/model_io/llms/\n",
      "  - https://python.langchain.com/docs/use_cases/chatbots/\n",
      "✓ Loaded 4 documents\n",
      "✓ Added custom metadata to all documents\n",
      "Splitting documents...\n",
      "  - Chunk size: 1000\n",
      "  - Chunk overlap: 200\n",
      "✓ Created 122 chunks\n",
      "\n",
      "  Sample chunk:\n",
      "    - Length: 991 chars\n",
      "    - Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "    - Preview: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Rea...\n",
      "\n",
      "✓ Loaded 122 chunks\n",
      "✗ Error loading vector store from /Users/gianlucamazza/Workspace/notebooks/llm_rag/data/vector_stores/fusion_rag: Error in faiss::FileIOReader::FileIOReader(const char *) at /Users/runner/work/faiss-wheels/faiss-wheels/third-party/faiss/faiss/impl/io.cpp:70: Error: 'f' failed: could not open /Users/gianlucamazza/Workspace/notebooks/llm_rag/data/vector_stores/fusion_rag/index.faiss for reading: No such file or directory\n",
      "\n",
      "Creating vector store...\n",
      "✓ Saved vector store to /Users/gianlucamazza/Workspace/notebooks/llm_rag/data/vector_stores/fusion_rag\n",
      "✓ Vector store created and saved\n",
      "✓ Retriever ready\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print_section_header(\"Loading Documents and Vector Store\")\n",
    "\n",
    "# Load and split documents (returns tuple: original_docs, chunks)\n",
    "_, docs = load_and_split(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(docs)} chunks\")\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(model=OPENAI_EMBEDDING_MODEL)\n",
    "\n",
    "# Load or create vector store\n",
    "store_path = VECTOR_STORE_DIR / \"fusion_rag\"\n",
    "vectorstore = load_vector_store(store_path, embeddings)\n",
    "\n",
    "if vectorstore is None:\n",
    "    print(\"\\nCreating vector store...\")\n",
    "    vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "    save_vector_store(vectorstore, store_path)\n",
    "    print(\"✓ Vector store created and saved\")\n",
    "else:\n",
    "    print(\"✓ Loaded existing vector store\")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "print(\"✓ Retriever ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement Reciprocal Rank Fusion\n",
    "\n",
    "The core algorithm of RAG-Fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Reciprocal Rank Fusion function defined\n",
      "\n",
      "Example RRF calculation:\n",
      "--------------------------------------------------------------------------------\n",
      "Query 1 ranks: [A(rank=1), B(rank=2), C(rank=3)]\n",
      "Query 2 ranks: [C(rank=1), A(rank=2), D(rank=3)]\n",
      "Query 3 ranks: [B(rank=1), C(rank=2), A(rank=3)]\n",
      "\n",
      "RRF scores (k=60):\n",
      "  Doc A: 1/61 + 1/62 + 1/63 = 0.0164 + 0.0161 + 0.0159 = 0.0484\n",
      "  Doc B: 1/62 + 1/61 = 0.0161 + 0.0164 = 0.0325\n",
      "  Doc C: 1/63 + 1/61 + 1/62 = 0.0159 + 0.0164 + 0.0161 = 0.0484\n",
      "  Doc D: 1/63 = 0.0159\n",
      "\n",
      "Final ranking: A=C (tie) > B > D\n"
     ]
    }
   ],
   "source": [
    "def reciprocal_rank_fusion(\n",
    "    results_list: List[List[Document]],\n",
    "    k: int = 60,\n",
    ") -> List[tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    Apply Reciprocal Rank Fusion to combine multiple ranked lists.\n",
    "    \n",
    "    Args:\n",
    "        results_list: List of ranked document lists (one per query)\n",
    "        k: RRF constant (default 60, as per literature)\n",
    "    \n",
    "    Returns:\n",
    "        List of (document, score) tuples, sorted by score descending\n",
    "    \"\"\"\n",
    "    # Track scores for each unique document\n",
    "    doc_scores = defaultdict(float)\n",
    "    doc_objects = {}  # Map content to document object\n",
    "    \n",
    "    # Process each query's results\n",
    "    for results in results_list:\n",
    "        for rank, doc in enumerate(results, start=1):\n",
    "            # Use page_content as unique identifier\n",
    "            doc_key = doc.page_content\n",
    "            \n",
    "            # Calculate RRF score contribution\n",
    "            score = 1.0 / (k + rank)\n",
    "            doc_scores[doc_key] += score\n",
    "            \n",
    "            # Store document object (in case of duplicates, keep first)\n",
    "            if doc_key not in doc_objects:\n",
    "                doc_objects[doc_key] = doc\n",
    "    \n",
    "    # Sort by score descending\n",
    "    ranked_docs = [\n",
    "        (doc_objects[doc_key], score)\n",
    "        for doc_key, score in sorted(\n",
    "            doc_scores.items(),\n",
    "            key=lambda x: x[1],\n",
    "            reverse=True,\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    return ranked_docs\n",
    "\n",
    "\n",
    "print(\"✓ Reciprocal Rank Fusion function defined\")\n",
    "\n",
    "# Test the function\n",
    "print(\"\\nExample RRF calculation:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Query 1 ranks: [A(rank=1), B(rank=2), C(rank=3)]\")\n",
    "print(\"Query 2 ranks: [C(rank=1), A(rank=2), D(rank=3)]\")\n",
    "print(\"Query 3 ranks: [B(rank=1), C(rank=2), A(rank=3)]\")\n",
    "print(\"\\nRRF scores (k=60):\")\n",
    "print(\"  Doc A: 1/61 + 1/62 + 1/63 = 0.0164 + 0.0161 + 0.0159 = 0.0484\")\n",
    "print(\"  Doc B: 1/62 + 1/61 = 0.0161 + 0.0164 = 0.0325\")\n",
    "print(\"  Doc C: 1/63 + 1/61 + 1/62 = 0.0159 + 0.0164 + 0.0161 = 0.0484\")\n",
    "print(\"  Doc D: 1/63 = 0.0159\")\n",
    "print(\"\\nFinal ranking: A=C (tie) > B > D\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Query Generation Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BUILDING QUERY GENERATION CHAIN\n",
      "================================================================================\n",
      "\n",
      "✓ Query generation chain created\n",
      "\n",
      "Test query: What is LCEL in LangChain?\n",
      "\n",
      "Generated alternatives:\n",
      "--------------------------------------------------------------------------------\n",
      "1. Can you explain what LCEL stands for in the context of LangChain?\n",
      "2. What does LCEL mean, and how is it used within LangChain? Additionally, what are its key features?\n",
      "3. In LangChain, what is the significance of LCEL, and how does it relate to other components of the framework?\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Building Query Generation Chain\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=DEFAULT_MODEL,\n",
    "    temperature=DEFAULT_TEMPERATURE,\n",
    ")\n",
    "\n",
    "# Create query generation chain\n",
    "query_gen_chain = FUSION_QUERY_GENERATION_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "print(\"✓ Query generation chain created\")\n",
    "\n",
    "# Test query generation\n",
    "test_query = \"What is LCEL in LangChain?\"\n",
    "print(f\"\\nTest query: {test_query}\")\n",
    "print(\"\\nGenerated alternatives:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "alternative_queries_text = query_gen_chain.invoke({\n",
    "    \"question\": test_query,\n",
    "    \"num_queries\": 3,\n",
    "})\n",
    "print(alternative_queries_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement Fusion RAG Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Fusion retriever function defined\n"
     ]
    }
   ],
   "source": [
    "def fusion_retriever(\n",
    "    query: str,\n",
    "    retriever,\n",
    "    llm,\n",
    "    num_queries: int = 4,\n",
    "    k_final: int = 6,\n",
    "    rrf_k: int = 60,\n",
    "    verbose: bool = False,\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Fusion RAG retriever with RRF.\n",
    "    \n",
    "    Args:\n",
    "        query: Original user query\n",
    "        retriever: LangChain retriever\n",
    "        llm: LLM for query generation\n",
    "        num_queries: Number of alternative queries to generate\n",
    "        k_final: Number of final documents to return\n",
    "        rrf_k: RRF constant\n",
    "        verbose: Print debug information\n",
    "    \n",
    "    Returns:\n",
    "        List of re-ranked documents\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n[Fusion Retriever] Original query: {query}\")\n",
    "    \n",
    "    # 1. Generate alternative queries\n",
    "    query_gen_chain = FUSION_QUERY_GENERATION_PROMPT | llm | StrOutputParser()\n",
    "    alternatives_text = query_gen_chain.invoke({\n",
    "        \"question\": query,\n",
    "        \"num_queries\": num_queries - 1,  # -1 because we include original\n",
    "    })\n",
    "    \n",
    "    # Parse alternative queries\n",
    "    alternative_queries = [\n",
    "        line.strip()\n",
    "        for line in alternatives_text.split(\"\\n\")\n",
    "        if line.strip() and any(char.isalpha() for char in line)\n",
    "    ]\n",
    "    \n",
    "    # Remove numbering if present (e.g., \"1. Query\" -> \"Query\")\n",
    "    alternative_queries = [\n",
    "        q.split(\".\", 1)[1].strip() if q[0].isdigit() and \".\" in q else q\n",
    "        for q in alternative_queries\n",
    "    ]\n",
    "    \n",
    "    # Include original query\n",
    "    all_queries = [query] + alternative_queries[:num_queries-1]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n[Fusion Retriever] Generated {len(all_queries)} queries:\")\n",
    "        for i, q in enumerate(all_queries, 1):\n",
    "            print(f\"  {i}. {q}\")\n",
    "    \n",
    "    # 2. Retrieve documents for each query\n",
    "    all_results = []\n",
    "    for q in all_queries:\n",
    "        results = retriever.invoke(q)\n",
    "        all_results.append(results)\n",
    "        if verbose:\n",
    "            print(f\"\\n[Fusion Retriever] Query '{q[:50]}...' retrieved {len(results)} docs\")\n",
    "    \n",
    "    # 3. Apply Reciprocal Rank Fusion\n",
    "    fused_results = reciprocal_rank_fusion(all_results, k=rrf_k)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n[Fusion Retriever] After RRF, top 3 scores:\")\n",
    "        for i, (doc, score) in enumerate(fused_results[:3], 1):\n",
    "            print(f\"  {i}. Score: {score:.4f} | Preview: {doc.page_content[:60]}...\")\n",
    "    \n",
    "    # 4. Return top-k documents\n",
    "    final_docs = [doc for doc, score in fused_results[:k_final]]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n[Fusion Retriever] Returning top {len(final_docs)} documents\")\n",
    "    \n",
    "    return final_docs\n",
    "\n",
    "\n",
    "print(\"✓ Fusion retriever function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build Fusion RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BUILDING FUSION RAG CHAIN\n",
      "================================================================================\n",
      "\n",
      "✓ Fusion RAG chain created\n"
     ]
    }
   ],
   "source": [
    "# Create a custom runnable for fusion retrieval\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "print_section_header(\"Building Fusion RAG Chain\")\n",
    "\n",
    "fusion_retriever_runnable = RunnableLambda(\n",
    "    lambda x: fusion_retriever(\n",
    "        query=x if isinstance(x, str) else str(x),\n",
    "        retriever=retriever,\n",
    "        llm=llm,\n",
    "        num_queries=4,\n",
    "        k_final=6,\n",
    "        verbose=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Build the RAG chain\n",
    "fusion_rag_chain = (\n",
    "    {\n",
    "        \"context\": fusion_retriever_runnable | format_docs,\n",
    "        \"input\": RunnablePassthrough(),\n",
    "        \"original_query\": RunnablePassthrough(),\n",
    "        \"num_queries\": lambda _: 4,\n",
    "        \"num_docs\": lambda _: 6,\n",
    "    }\n",
    "    | FUSION_RAG_ANSWER_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✓ Fusion RAG chain created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Fusion RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TESTING FUSION RAG\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Query 1: What is LCEL and how do I use it?\n",
      "================================================================================\n",
      "\n",
      "LCEL stands for LangChain's Enhanced Language model, which is a framework designed to facilitate the development of applications powered by large language models (LLMs). It provides a standardized interface for interacting with different model providers, making it easier to build and deploy applications without being locked into a specific provider.\n",
      "\n",
      "To use LCEL, you can follow these steps:\n",
      "\n",
      "1. **Installation**: First, you need to install LangChain using pip. You can do this by running the following command in your terminal:\n",
      "   ```\n",
      "   pip install -U langchain\n",
      "   ```\n",
      "   Make sure you have Python 3.10 or higher installed.\n",
      "\n",
      "2. **Building an Agent**: LangChain allows you to create agents with minimal code. You can build a simple agent in under 10 lines of code. This agent can handle various tasks, including answering questions, performing searches, and more.\n",
      "\n",
      "3. **Using LangGraph**: For more advanced needs, you can utilize LangGraph, which is the underlying framework for LangChain agents. It provides features like durable execution, human-in-the-loop support, and persistence.\n",
      "\n",
      "4. **Customization**: You can customize the behavior of your agents by adjusting parameters such as the level of reasoning the model should apply or by implementing prompt caching to reduce latency.\n",
      "\n",
      "5. **Local Models**: If data privacy is a concern or if you want to avoid cloud costs, you can run models locally on your hardware. LangChain supports various local integrations.\n",
      "\n",
      "6. **Building Applications**: You can create sophisticated applications, such as Q&A chatbots, that leverage Retrieval Augmented Generation (RAG) techniques to answer questions based on specific source information.\n",
      "\n",
      "By following these steps, you can effectively utilize LCEL to build powerful applications that harness the capabilities of large language models.\n",
      "\n",
      "⏱️  Time: 11.62s\n",
      "\n",
      "================================================================================\n",
      "Query 2: Explain different types of memory in conversational AI\n",
      "================================================================================\n",
      "\n",
      "In conversational AI, memory can be categorized into several types, each serving different purposes to enhance user interactions. Here are the main types:\n",
      "\n",
      "1. **Short-term Memory**: This type of memory is used to store information temporarily during a single conversation session. It allows the AI to remember context, user preferences, or specific details shared by the user while the conversation is ongoing. Once the session ends, this information is typically discarded.\n",
      "\n",
      "2. **Long-term Memory**: Long-term memory enables the AI to retain information across multiple interactions or sessions. This can include user preferences, past interactions, and other relevant data that can help personalize future conversations. Long-term memory is crucial for creating a more engaging and tailored user experience.\n",
      "\n",
      "3. **Working Memory**: This is a subset of short-term memory that focuses on actively processing and manipulating information during a conversation. It allows the AI to keep track of the dialogue flow, manage context, and respond appropriately based on the current state of the conversation.\n",
      "\n",
      "4. **Contextual Memory**: Contextual memory refers to the ability of the AI to remember the context of the conversation, including the topics discussed and the emotional tone. This helps the AI maintain coherence and relevance in its responses, making the interaction feel more natural.\n",
      "\n",
      "5. **Declarative Memory**: This type of memory involves storing factual information that the AI can retrieve when needed. For example, it can remember specific facts about a user or general knowledge that can be used to answer questions.\n",
      "\n",
      "6. **Procedural Memory**: Procedural memory is related to the AI's ability to perform tasks or follow procedures based on learned experiences. This can include remembering how to execute certain commands or workflows that the user may request.\n",
      "\n",
      "By effectively utilizing these different types of memory, conversational AI can provide a more seamless and personalized experience for users, adapting to their needs and preferences over time.\n",
      "\n",
      "⏱️  Time: 11.87s\n",
      "\n",
      "================================================================================\n",
      "Query 3: How do retrievers work in RAG applications?\n",
      "================================================================================\n",
      "\n",
      "In RAG (Retrieval-Augmented Generation) applications, retrievers play a crucial role in the process of obtaining relevant information to enhance the generation of responses. Here's how they work:\n",
      "\n",
      "1. **Indexing**: Initially, data from various sources is ingested and indexed. This indexing process creates a structured representation of the data, allowing for efficient retrieval later on.\n",
      "\n",
      "2. **Retrieval Process**: When a user submits a query, the retriever component is activated. It searches the indexed data to find relevant documents or information that match the user's input. This is done using various techniques, such as vector similarity search, which allows the system to find the most pertinent pieces of information based on the context of the query.\n",
      "\n",
      "3. **Generation**: After the relevant data is retrieved, it is passed along with the original user query to a language model (LLM). The model then generates a response by synthesizing the information from the retrieved documents and the user's question.\n",
      "\n",
      "4. **Two-Step RAG Chain**: In some implementations, a two-step RAG chain is used, which involves a single LLM call per query. This method is efficient for handling simple queries, as it minimizes the computational load while still providing relevant answers.\n",
      "\n",
      "Overall, retrievers in RAG applications serve to bridge the gap between raw data and meaningful responses by efficiently fetching relevant information that can be used to inform the generation process.\n",
      "\n",
      "⏱️  Time: 10.39s\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Testing Fusion RAG\")\n",
    "\n",
    "test_queries = [\n",
    "    \"What is LCEL and how do I use it?\",\n",
    "    \"Explain different types of memory in conversational AI\",\n",
    "    \"How do retrievers work in RAG applications?\",\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = fusion_rag_chain.invoke(query)\n",
    "    elapsed = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + response)\n",
    "    print(f\"\\n⏱️  Time: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Detailed Example: Inspect the Fusion Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "DETAILED FUSION PROCESS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Query: What are the main components of LangChain?\n",
      "\n",
      "================================================================================\n",
      "FUSION RETRIEVAL PROCESS:\n",
      "================================================================================\n",
      "\n",
      "[Fusion Retriever] Original query: What are the main components of LangChain?\n",
      "\n",
      "[Fusion Retriever] Generated 4 queries:\n",
      "  1. What are the main components of LangChain?\n",
      "  2. What are the key elements that make up LangChain?\n",
      "  3. Can you list the primary components of LangChain? Additionally, how do these components interact with each other?\n",
      "  4. What are the essential parts of the LangChain framework, and what roles do they play in its functionality?\n",
      "\n",
      "[Fusion Retriever] Query 'What are the main components of LangChain?...' retrieved 4 docs\n",
      "\n",
      "[Fusion Retriever] Query 'What are the key elements that make up LangChain?...' retrieved 4 docs\n",
      "\n",
      "[Fusion Retriever] Query 'Can you list the primary components of LangChain? ...' retrieved 4 docs\n",
      "\n",
      "[Fusion Retriever] Query 'What are the essential parts of the LangChain fram...' retrieved 4 docs\n",
      "\n",
      "[Fusion Retriever] After RRF, top 3 scores:\n",
      "  1. Score: 0.0650 | Preview: LangChain is the easiest way to start building agents and ap...\n",
      "  2. Score: 0.0645 | Preview: LangChain overview - Docs by LangChainSkip to main contentWe...\n",
      "  3. Score: 0.0640 | Preview: ​ Core benefits\n",
      "Standard model interfaceDifferent providers ...\n",
      "\n",
      "[Fusion Retriever] Returning top 4 documents\n",
      "\n",
      "================================================================================\n",
      "FINAL RETRIEVED DOCUMENTS:\n",
      "================================================================================\n",
      "\n",
      "Document 1:\n",
      "Source: https://python.langchain.com/docs/modules/data_connection/re\n",
      "Content: LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-bu...\n",
      "\n",
      "Document 2:\n",
      "Source: https://python.langchain.com/docs/modules/data_connection/re\n",
      "Content: LangChain overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch......\n",
      "\n",
      "Document 3:\n",
      "Source: https://python.langchain.com/docs/modules/data_connection/re\n",
      "Content: ​ Core benefits\n",
      "Standard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that...\n",
      "\n",
      "Document 4:\n",
      "Source: https://python.langchain.com/docs/use_cases/question_answeri\n",
      "Content: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Detailed Fusion Process Analysis\")\n",
    "\n",
    "query = \"What are the main components of LangChain?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Run with verbose mode\n",
    "print(\"=\" * 80)\n",
    "print(\"FUSION RETRIEVAL PROCESS:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fused_docs = fusion_retriever(\n",
    "    query=query,\n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    "    num_queries=4,\n",
    "    k_final=6,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL RETRIEVED DOCUMENTS:\")\n",
    "print(\"=\" * 80)\n",
    "for i, doc in enumerate(fused_docs, 1):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'unknown')[:60]}\")\n",
    "    print(f\"Content: {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparison: Standard vs Multi-Query vs Fusion RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARISON: STANDARD VS MULTI-QUERY VS FUSION RAG\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Query: How do I build chains in LangChain?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[1] STANDARD RAG\n",
      "--------------------------------------------------------------------------------\n",
      "The context provided does not contain specific information on how to build chains in LangChain. It mentions various components and features of LangChain, such as RAG agents and the integration with LangGraph, but does not detail the process for building chains. You may need to refer to the official LangChain documentation or tutorials for detailed instructions on building chains.\n",
      "\n",
      "⏱️  Time: 4.28s\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[2] MULTI-QUERY RAG (Simple Deduplication)\n",
      "--------------------------------------------------------------------------------\n",
      "The context provided does not contain specific information on how to build chains in LangChain. It mentions that LangChain allows for building agents and applications powered by LLMs and refers to various components and features, but it does not detail the steps or methods for constructing chains. \n",
      "\n",
      "For detailed instructions on building chains, you may need to refer to the official LangChain documentation or tutorials.\n",
      "\n",
      "⏱️  Time: 6.54s\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[3] FUSION RAG (Reciprocal Rank Fusion)\n",
      "--------------------------------------------------------------------------------\n",
      "To build chains in LangChain, you can follow these general steps:\n",
      "\n",
      "1. **Installation**: First, ensure you have LangChain installed. You can do this by running:\n",
      "   ```bash\n",
      "   pip install -U langchain\n",
      "   ```\n",
      "   Make sure you are using Python 3.10 or higher.\n",
      "\n",
      "2. **Create an Agent**: LangChain allows you to create agents with minimal code. You can build a simple agent in under 10 lines of code. The agent architecture is designed to be flexible, enabling you to customize it as needed.\n",
      "\n",
      "3. **Use LangGraph**: LangChain agents are built on top of LangGraph, which provides features like durable execution and human-in-the-loop support. You do not need to know the details of LangGraph for basic usage of LangChain agents.\n",
      "\n",
      "4. **Set Up Logging (Optional)**: If you want to debug and gain insights into your agent's behavior, you can use LangSmith for tracing. Set your environment variables to enable logging:\n",
      "   ```bash\n",
      "   export LANGSMITH_TRACING=\"true\"\n",
      "   export LANGSMITH_API_KEY=\"...\"\n",
      "   ```\n",
      "   Or set them in your Python code:\n",
      "   ```python\n",
      "   import getpass\n",
      "   import os\n",
      "\n",
      "   os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
      "   os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n",
      "   ```\n",
      "\n",
      "5. **Build Your Chain**: Depending on your application, you can create chains that involve multiple steps and LLM calls. You can refer to the LangChain documentation for specific examples and tutorials on building different types of chains, such as RAG (Retrieval-Augmented Generation) agents.\n",
      "\n",
      "6. **Explore Advanced Features**: As you become more familiar with LangChain, you can explore advanced features like context engineering, memory management, and multi-agent setups.\n",
      "\n",
      "For detailed examples and specific implementations, you can refer to the LangChain documentation and tutorials available on their website.\n",
      "\n",
      "⏱️  Time: 11.79s\n",
      "\n",
      "================================================================================\n",
      "PERFORMANCE SUMMARY:\n",
      "================================================================================\n",
      "Standard RAG:     4.28s (baseline)\n",
      "Multi-Query RAG:  6.54s (1.5x slower)\n",
      "Fusion RAG:       11.79s (2.8x slower)\n"
     ]
    }
   ],
   "source": [
    "# Build comparison chains\n",
    "from shared.prompts import RAG_PROMPT_TEMPLATE, MULTI_QUERY_PROMPT\n",
    "\n",
    "print_section_header(\"Comparison: Standard vs Multi-Query vs Fusion RAG\")\n",
    "\n",
    "# Standard RAG\n",
    "standard_chain = (\n",
    "    {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | RAG_PROMPT_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Multi-Query RAG (simple deduplication)\n",
    "def multi_query_retriever_simple(query: str) -> List[Document]:\n",
    "    \"\"\"Multi-Query RAG with simple deduplication.\"\"\"\n",
    "    # Generate alternatives\n",
    "    multi_query_chain = MULTI_QUERY_PROMPT | llm | StrOutputParser()\n",
    "    alternatives = multi_query_chain.invoke({\"question\": query})\n",
    "    alternative_queries = [\n",
    "        line.strip() for line in alternatives.split(\"\\n\") if line.strip()\n",
    "    ]\n",
    "    \n",
    "    # Retrieve for each query\n",
    "    all_docs = []\n",
    "    seen_content = set()\n",
    "    \n",
    "    for q in [query] + alternative_queries[:2]:\n",
    "        docs = retriever.invoke(q)\n",
    "        for doc in docs:\n",
    "            if doc.page_content not in seen_content:\n",
    "                all_docs.append(doc)\n",
    "                seen_content.add(doc.page_content)\n",
    "    \n",
    "    return all_docs[:6]\n",
    "\n",
    "multi_query_chain = (\n",
    "    {\n",
    "        \"context\": RunnableLambda(multi_query_retriever_simple) | format_docs,\n",
    "        \"input\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RAG_PROMPT_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test query\n",
    "test_query = \"How do I build chains in LangChain?\"\n",
    "\n",
    "print(f\"\\nQuery: {test_query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Standard RAG\n",
    "print(\"\\n[1] STANDARD RAG\")\n",
    "print(\"-\" * 80)\n",
    "start = time.time()\n",
    "response_standard = standard_chain.invoke(test_query)\n",
    "time_standard = time.time() - start\n",
    "print(response_standard)\n",
    "print(f\"\\n⏱️  Time: {time_standard:.2f}s\")\n",
    "\n",
    "# Multi-Query RAG\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n[2] MULTI-QUERY RAG (Simple Deduplication)\")\n",
    "print(\"-\" * 80)\n",
    "start = time.time()\n",
    "response_multi = multi_query_chain.invoke(test_query)\n",
    "time_multi = time.time() - start\n",
    "print(response_multi)\n",
    "print(f\"\\n⏱️  Time: {time_multi:.2f}s\")\n",
    "\n",
    "# Fusion RAG\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n[3] FUSION RAG (Reciprocal Rank Fusion)\")\n",
    "print(\"-\" * 80)\n",
    "start = time.time()\n",
    "response_fusion = fusion_rag_chain.invoke(test_query)\n",
    "time_fusion = time.time() - start\n",
    "print(response_fusion)\n",
    "print(f\"\\n⏱️  Time: {time_fusion:.2f}s\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERFORMANCE SUMMARY:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Standard RAG:     {time_standard:.2f}s (baseline)\")\n",
    "print(f\"Multi-Query RAG:  {time_multi:.2f}s ({time_multi/time_standard:.1f}x slower)\")\n",
    "print(f\"Fusion RAG:       {time_fusion:.2f}s ({time_fusion/time_standard:.1f}x slower)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. RRF Parameter Tuning\n",
    "\n",
    "Explore how different RRF parameters affect ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RRF PARAMETER TUNING\n",
      "================================================================================\n",
      "\n",
      "Query: What is LCEL?\n",
      "\n",
      "Testing different RRF k values:\n",
      "================================================================================\n",
      "\n",
      "k = 10:\n",
      "--------------------------------------------------------------------------------\n",
      "  1. ✅ Benefits⚠️ Drawbacks\n",
      "Search only when needed – The LLM can handle greetings, f...\n",
      "  2. ​ Core benefits\n",
      "Standard model interfaceDifferent providers have unique APIs for...\n",
      "  3. LangChain overview - Docs by LangChainSkip to main contentWe've raised a $125M S...\n",
      "\n",
      "k = 60:\n",
      "--------------------------------------------------------------------------------\n",
      "  1. ✅ Benefits⚠️ Drawbacks\n",
      "Search only when needed – The LLM can handle greetings, f...\n",
      "  2. ​ Core benefits\n",
      "Standard model interfaceDifferent providers have unique APIs for...\n",
      "  3. LangChain overview - Docs by LangChainSkip to main contentWe've raised a $125M S...\n",
      "\n",
      "k = 100:\n",
      "--------------------------------------------------------------------------------\n",
      "  1. ✅ Benefits⚠️ Drawbacks\n",
      "Search only when needed – The LLM can handle greetings, f...\n",
      "  2. ​ Core benefits\n",
      "Standard model interfaceDifferent providers have unique APIs for...\n",
      "  3. LangChain overview - Docs by LangChainSkip to main contentWe've raised a $125M S...\n",
      "\n",
      "================================================================================\n",
      "OBSERVATIONS:\n",
      "================================================================================\n",
      "• Lower k (e.g., 10): More weight on top-ranked documents\n",
      "• Higher k (e.g., 100): More uniform weighting across ranks\n",
      "• Standard k = 60: Good balance (recommended in literature)\n",
      "• Results often similar due to robust algorithm\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"RRF Parameter Tuning\")\n",
    "\n",
    "query = \"What is LCEL?\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Test different k values\n",
    "k_values = [10, 60, 100]\n",
    "\n",
    "print(\"Testing different RRF k values:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for k in k_values:\n",
    "    print(f\"\\nk = {k}:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    docs = fusion_retriever(\n",
    "        query=query,\n",
    "        retriever=retriever,\n",
    "        llm=llm,\n",
    "        num_queries=3,\n",
    "        k_final=3,\n",
    "        rrf_k=k,\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        print(f\"  {i}. {doc.page_content[:80]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"OBSERVATIONS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"• Lower k (e.g., 10): More weight on top-ranked documents\")\n",
    "print(\"• Higher k (e.g., 100): More uniform weighting across ranks\")\n",
    "print(\"• Standard k = 60: Good balance (recommended in literature)\")\n",
    "print(\"• Results often similar due to robust algorithm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PERFORMANCE METRICS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "COST BREAKDOWN:\n",
      "--------------------------------------------------------------------------------\n",
      "Query generation: 3 LLM calls\n",
      "Retrievals: 4 vector searches\n",
      "Documents retrieved (total): ~16\n",
      "Documents after RRF: 6 (deduplicated + reranked)\n",
      "Final generation: 1 LLM call\n",
      "\n",
      "Total LLM calls: 4\n",
      "Total vector searches: 4\n",
      "\n",
      "================================================================================\n",
      "COMPARISON WITH OTHER APPROACHES:\n",
      "================================================================================\n",
      "\n",
      "LLM Calls:\n",
      "  • Standard RAG: 1 (generation only)\n",
      "  • Multi-Query RAG: 4 (1 gen + 3 query gen)\n",
      "  • Fusion RAG: 4 (1 gen + 3 query gen)\n",
      "\n",
      "Vector Searches:\n",
      "  • Standard RAG: 1\n",
      "  • Multi-Query RAG: 3-4\n",
      "  • Fusion RAG: 4\n",
      "\n",
      "Ranking Quality:\n",
      "  • Standard RAG: ⭐⭐⭐ (baseline)\n",
      "  • Multi-Query RAG: ⭐⭐⭐⭐ (better coverage)\n",
      "  • Fusion RAG: ⭐⭐⭐⭐⭐ (best ranking)\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Performance Metrics\")\n",
    "\n",
    "num_queries_generated = 3  # Alternative queries (+ 1 original = 4 total)\n",
    "retrievals_per_query = 4\n",
    "total_retrievals = 4  # num_queries\n",
    "\n",
    "print(\"\\nCOST BREAKDOWN:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Query generation: {num_queries_generated} LLM calls\")\n",
    "print(f\"Retrievals: {total_retrievals} vector searches\")\n",
    "print(f\"Documents retrieved (total): ~{total_retrievals * retrievals_per_query}\")\n",
    "print(f\"Documents after RRF: 6 (deduplicated + reranked)\")\n",
    "print(f\"Final generation: 1 LLM call\")\n",
    "print(f\"\\nTotal LLM calls: {num_queries_generated + 1}\")\n",
    "print(f\"Total vector searches: {total_retrievals}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"COMPARISON WITH OTHER APPROACHES:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nLLM Calls:\")\n",
    "print(\"  • Standard RAG: 1 (generation only)\")\n",
    "print(\"  • Multi-Query RAG: 4 (1 gen + 3 query gen)\")\n",
    "print(\"  • Fusion RAG: 4 (1 gen + 3 query gen)\")\n",
    "print(\"\\nVector Searches:\")\n",
    "print(\"  • Standard RAG: 1\")\n",
    "print(\"  • Multi-Query RAG: 3-4\")\n",
    "print(\"  • Fusion RAG: 4\")\n",
    "print(\"\\nRanking Quality:\")\n",
    "print(\"  • Standard RAG: ⭐⭐⭐ (baseline)\")\n",
    "print(\"  • Multi-Query RAG: ⭐⭐⭐⭐ (better coverage)\")\n",
    "print(\"  • Fusion RAG: ⭐⭐⭐⭐⭐ (best ranking)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Takeaways\n",
    "\n",
    "### Summary\n",
    "\n",
    "**Fusion RAG** improves on Multi-Query RAG by using Reciprocal Rank Fusion:\n",
    "- Generates multiple query variations (like Multi-Query)\n",
    "- Applies sophisticated RRF algorithm for ranking\n",
    "- Preserves and combines ranking information\n",
    "- More robust than simple deduplication\n",
    "\n",
    "### RRF Algorithm Benefits\n",
    "\n",
    "✅ **Why RRF works:**\n",
    "- Favors documents appearing in multiple results\n",
    "- Balances frequency and rank position\n",
    "- No score normalization needed\n",
    "- Robust to outliers and variations\n",
    "- Proven effective in information retrieval\n",
    "\n",
    "### Cost-Benefit Analysis\n",
    "\n",
    "| Aspect | Impact | Notes |\n",
    "|--------|--------|-------|\n",
    "| **Query Latency** | ❌ 2-3x slower | Multiple retrievals |\n",
    "| **LLM Calls** | ❌ +3-4 calls | Query generation |\n",
    "| **Retrieval Quality** | ✅ Best | RRF ranking |\n",
    "| **Coverage** | ✅ Excellent | Multiple perspectives |\n",
    "| **Robustness** | ✅ High | Less sensitive to wording |\n",
    "| **Implementation** | ⚠️ Medium | More complex |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Query count**: 3-5 queries is optimal (more = diminishing returns)\n",
    "2. **RRF constant (k)**: 60 is standard, tune for your use case\n",
    "3. **Final k**: Return more docs than needed, let RRF decide quality\n",
    "4. **Caching**: Cache query generations for common queries\n",
    "5. **Monitoring**: Track which queries benefit most from fusion\n",
    "\n",
    "### When to Use\n",
    "\n",
    "Choose **Fusion RAG** when:\n",
    "- ✅ Query quality matters more than speed\n",
    "- ✅ Dealing with complex, multi-faceted questions\n",
    "- ✅ Need robust retrieval across query variations\n",
    "- ✅ Can afford extra latency and cost\n",
    "\n",
    "Choose **Multi-Query RAG** when:\n",
    "- ✅ Simpler implementation preferred\n",
    "- ✅ Deduplication is sufficient\n",
    "- ✅ Slightly lower costs desired\n",
    "\n",
    "Choose **Standard RAG** when:\n",
    "- ✅ Speed is critical\n",
    "- ✅ Simple queries\n",
    "- ✅ Tight budget constraints\n",
    "\n",
    "### Extensions\n",
    "\n",
    "- **Hybrid fusion**: Combine with keyword search\n",
    "- **Weighted queries**: Assign different weights to query types\n",
    "- **Query filtering**: Remove low-quality generated queries\n",
    "- **Adaptive fusion**: Use fusion only for complex queries\n",
    "\n",
    "---\n",
    "\n",
    "**Complexity Rating:** ⭐⭐⭐ (Medium - RRF algorithm adds sophistication)\n",
    "\n",
    "**Production Readiness:** ⭐⭐⭐⭐ (High - proven algorithm, manageable trade-offs)\n",
    "\n",
    "Continue to **14_sql_rag.ipynb** for Natural Language to SQL RAG!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
