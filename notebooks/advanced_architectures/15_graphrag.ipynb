{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. GraphRAG - Graph-based Knowledge Retrieval\n",
    "\n",
    "**Complexity:** ⭐⭐⭐⭐⭐\n",
    "\n",
    "## Overview\n",
    "\n",
    "**GraphRAG** (Microsoft Research) is a revolutionary approach that represents knowledge as a graph instead of vectors, enabling:\n",
    "- Multi-hop reasoning across relationships\n",
    "- Entity-centric retrieval\n",
    "- Community-based summarization\n",
    "- Relationship-aware context\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Traditional vector RAG struggles with:\n",
    "- **Multi-hop questions**: \"Who worked with someone who collaborated with Einstein?\"\n",
    "- **Relationship queries**: \"What's the connection between X and Y?\"\n",
    "- **Entity disambiguation**: Same name, different entities\n",
    "- **Structural knowledge**: Understanding how concepts relate\n",
    "\n",
    "### The Solution\n",
    "\n",
    "GraphRAG builds a **knowledge graph**:\n",
    "```\n",
    "[Entity] --relationship--> [Entity]\n",
    "```\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "LangChain --created_by--> Harrison Chase\n",
    "LangChain --includes--> LCEL\n",
    "LCEL --is_a--> Framework Component\n",
    "LCEL --enables--> Chain Building\n",
    "```\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "```\n",
    "Documents → Entity Extraction → Relationship Extraction\n",
    "    → Graph Construction → Community Detection\n",
    "    → Graph Indexing → Query Processing\n",
    "    → Graph Traversal → Answer Generation\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Entities**: People, organizations, concepts, technologies\n",
    "2. **Relationships**: How entities connect\n",
    "3. **Communities**: Clusters of related entities\n",
    "4. **Multi-hop**: Following paths through graph\n",
    "5. **Subgraph**: Relevant portion of full graph\n",
    "\n",
    "### Microsoft GraphRAG Features\n",
    "\n",
    "- **Global queries**: Use community summaries\n",
    "- **Local queries**: Traverse from specific entities\n",
    "- **Hierarchical communities**: Multi-level clustering\n",
    "- **Embeddings**: Vector representation of graph nodes\n",
    "\n",
    "### When to Use\n",
    "\n",
    "✅ **Good for:**\n",
    "- Complex knowledge domains\n",
    "- Multi-hop reasoning\n",
    "- Relationship-heavy content\n",
    "- Entity-centric queries\n",
    "- Knowledge exploration\n",
    "\n",
    "❌ **Not ideal for:**\n",
    "- Simple document retrieval\n",
    "- Real-time applications (slow indexing)\n",
    "- Unstructured narrative text\n",
    "- When relationships aren't explicit\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "**Pros:**\n",
    "- ✅ Best for multi-hop reasoning\n",
    "- ✅ Understands relationships\n",
    "- ✅ Handles complex queries\n",
    "- ✅ Explainable paths\n",
    "\n",
    "**Cons:**\n",
    "- ❌ Very slow indexing (entity + relationship extraction)\n",
    "- ❌ High LLM costs (many extraction calls)\n",
    "- ❌ Complex implementation\n",
    "- ❌ Graph storage overhead\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation\n",
    "\n",
    "We'll build a simplified GraphRAG system with all core components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Set\n",
    "from collections import defaultdict\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(str(Path(\"../..\").resolve()))\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from shared.config import (\n",
    "    verify_api_key,\n",
    "    LLM_MODEL,\n",
    "    LLM_TEMPERATURE,\n",
    "    EMBEDDINGS_MODEL,\n",
    "    VECTOR_STORE_PATH,\n",
    ")\n",
    "from shared.loaders import load_and_split\n",
    "from shared.prompts import (\n",
    "    ENTITY_EXTRACTION_PROMPT,\n",
    "    RELATIONSHIP_EXTRACTION_PROMPT,\n",
    "    ENTITY_DISAMBIGUATION_PROMPT,\n",
    "    GRAPH_SUMMARIZATION_PROMPT,\n",
    "    GRAPHRAG_ANSWER_PROMPT,\n",
    ")\n",
    "from shared.utils import (\n",
    "    print_section_header,\n",
    "    load_vector_store,\n",
    "    save_vector_store,\n",
    ")\n",
    "\n",
    "# Verify API key\n",
    "verify_api_key()\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "print(f\"✓ Using model: {LLM_MODEL}\")\n",
    "print(f\"✓ Using embeddings: {EMBEDDINGS_MODEL}\")\n",
    "print(f\"✓ NetworkX version: {nx.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Loading Documents\")\n",
    "\n",
    "# Load documents (we'll use a subset for demo)\n",
    "docs = load_and_split(\n",
    "    chunk_size=1500,  # Larger chunks for better entity extraction\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "# Use first 10 docs for demo (full corpus would take too long)\n",
    "docs_sample = docs[:10]\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(docs_sample)} documents for GraphRAG\")\n",
    "print(f\"✓ Average chunk size: {sum(len(d.page_content) for d in docs_sample) / len(docs_sample):.0f} chars\")\n",
    "\n",
    "print(\"\\nNote: Using subset for demo. Production would process full corpus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Entities\n",
    "\n",
    "First step: identify all important entities in documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Extracting Entities\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=LLM_MODEL,\n",
    "    temperature=0,  # Deterministic for extraction\n",
    ")\n",
    "\n",
    "# Entity extraction chain\n",
    "entity_chain = ENTITY_EXTRACTION_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "all_entities = []\n",
    "doc_entities_map = {}  # Map doc to its entities\n",
    "\n",
    "print(\"\\nExtracting entities from documents...\")\n",
    "print(\"(This may take a few minutes)\\n\")\n",
    "\n",
    "for i, doc in enumerate(docs_sample, 1):\n",
    "    print(f\"  Processing document {i}/{len(docs_sample)}...\")\n",
    "    \n",
    "    try:\n",
    "        # Extract entities\n",
    "        result = entity_chain.invoke({\"text\": doc.page_content})\n",
    "        \n",
    "        # Parse JSON response\n",
    "        # Remove markdown formatting if present\n",
    "        result_clean = result.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        entities = json.loads(result_clean)\n",
    "        \n",
    "        # Store entities\n",
    "        doc_entities_map[i] = entities\n",
    "        all_entities.extend(entities)\n",
    "        \n",
    "        print(f\"    → Found {len(entities)} entities\")\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"    → JSON parse error: {e}\")\n",
    "        doc_entities_map[i] = []\n",
    "    \n",
    "    # Rate limiting\n",
    "    if i < len(docs_sample):\n",
    "        time.sleep(0.5)\n",
    "\n",
    "print(f\"\\n✓ Extracted {len(all_entities)} total entities\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Example entities:\")\n",
    "print(\"=\" * 80)\n",
    "for entity in all_entities[:5]:\n",
    "    print(f\"\\n• {entity.get('name', 'N/A')}\")\n",
    "    print(f\"  Type: {entity.get('type', 'N/A')}\")\n",
    "    print(f\"  Description: {entity.get('description', 'N/A')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract Relationships\n",
    "\n",
    "Now find relationships between entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Extracting Relationships\")\n",
    "\n",
    "# Relationship extraction chain\n",
    "relationship_chain = RELATIONSHIP_EXTRACTION_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "all_relationships = []\n",
    "\n",
    "print(\"\\nExtracting relationships...\\n\")\n",
    "\n",
    "for i, doc in enumerate(docs_sample, 1):\n",
    "    entities = doc_entities_map.get(i, [])\n",
    "    \n",
    "    if not entities:\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Processing document {i}/{len(docs_sample)}...\")\n",
    "    \n",
    "    try:\n",
    "        # Format entities for prompt\n",
    "        entities_str = \"\\n\".join([\n",
    "            f\"- {e['name']} ({e['type']})\"\n",
    "            for e in entities\n",
    "        ])\n",
    "        \n",
    "        # Extract relationships\n",
    "        result = relationship_chain.invoke({\n",
    "            \"text\": doc.page_content,\n",
    "            \"entities\": entities_str,\n",
    "        })\n",
    "        \n",
    "        # Parse JSON\n",
    "        result_clean = result.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "        relationships = json.loads(result_clean)\n",
    "        \n",
    "        all_relationships.extend(relationships)\n",
    "        \n",
    "        print(f\"    → Found {len(relationships)} relationships\")\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"    → JSON parse error: {e}\")\n",
    "    \n",
    "    # Rate limiting\n",
    "    if i < len(docs_sample):\n",
    "        time.sleep(0.5)\n",
    "\n",
    "print(f\"\\n✓ Extracted {len(all_relationships)} relationships\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Example relationships:\")\n",
    "print(\"=\" * 80)\n",
    "for rel in all_relationships[:5]:\n",
    "    source = rel.get('source', 'N/A')\n",
    "    relation = rel.get('relation', 'N/A')\n",
    "    target = rel.get('target', 'N/A')\n",
    "    print(f\"\\n• {source} --[{relation}]--> {target}\")\n",
    "    print(f\"  {rel.get('description', 'N/A')[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build Knowledge Graph\n",
    "\n",
    "Construct graph from entities and relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Building Knowledge Graph\")\n",
    "\n",
    "# Create directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes (entities)\n",
    "print(\"\\nAdding entities as nodes...\")\n",
    "for entity in all_entities:\n",
    "    name = entity.get('name')\n",
    "    if name:\n",
    "        G.add_node(\n",
    "            name,\n",
    "            type=entity.get('type', 'UNKNOWN'),\n",
    "            description=entity.get('description', ''),\n",
    "        )\n",
    "\n",
    "print(f\"✓ Added {len(G.nodes())} nodes\")\n",
    "\n",
    "# Add edges (relationships)\n",
    "print(\"\\nAdding relationships as edges...\")\n",
    "edges_added = 0\n",
    "for rel in all_relationships:\n",
    "    source = rel.get('source')\n",
    "    target = rel.get('target')\n",
    "    relation = rel.get('relation')\n",
    "    \n",
    "    # Only add if both nodes exist\n",
    "    if source in G.nodes() and target in G.nodes():\n",
    "        G.add_edge(\n",
    "            source,\n",
    "            target,\n",
    "            relation=relation,\n",
    "            description=rel.get('description', ''),\n",
    "        )\n",
    "        edges_added += 1\n",
    "\n",
    "print(f\"✓ Added {edges_added} edges\")\n",
    "\n",
    "# Graph statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GRAPH STATISTICS:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Nodes (entities): {len(G.nodes())}\")\n",
    "print(f\"Edges (relationships): {len(G.edges())}\")\n",
    "print(f\"Density: {nx.density(G):.4f}\")\n",
    "print(f\"Is connected: {nx.is_weakly_connected(G)}\")\n",
    "\n",
    "if nx.is_weakly_connected(G):\n",
    "    print(f\"Average path length: {nx.average_shortest_path_length(G.to_undirected()):.2f}\")\n",
    "else:\n",
    "    # Count connected components\n",
    "    num_components = nx.number_weakly_connected_components(G)\n",
    "    print(f\"Connected components: {num_components}\")\n",
    "\n",
    "# Most connected nodes\n",
    "degree_dict = dict(G.degree())\n",
    "top_nodes = sorted(degree_dict.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "print(\"\\nMost connected entities:\")\n",
    "for node, degree in top_nodes:\n",
    "    print(f\"  • {node}: {degree} connections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Visualizing Knowledge Graph\")\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Use spring layout for better visualization\n",
    "pos = nx.spring_layout(G, k=0.5, iterations=50)\n",
    "\n",
    "# Draw nodes\n",
    "nx.draw_networkx_nodes(\n",
    "    G,\n",
    "    pos,\n",
    "    node_size=500,\n",
    "    node_color='lightblue',\n",
    "    alpha=0.7,\n",
    ")\n",
    "\n",
    "# Draw edges\n",
    "nx.draw_networkx_edges(\n",
    "    G,\n",
    "    pos,\n",
    "    edge_color='gray',\n",
    "    arrows=True,\n",
    "    arrowsize=10,\n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "# Draw labels (only for high-degree nodes to avoid clutter)\n",
    "high_degree_nodes = [n for n, d in dict(G.degree()).items() if d >= 2]\n",
    "labels = {n: n for n in high_degree_nodes}\n",
    "nx.draw_networkx_labels(\n",
    "    G,\n",
    "    pos,\n",
    "    labels,\n",
    "    font_size=8,\n",
    "    font_weight='bold',\n",
    ")\n",
    "\n",
    "plt.title(\"Knowledge Graph Visualization\", fontsize=16, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✓ Graph visualization complete\")\n",
    "print(\"\\nNote: Only high-degree nodes are labeled to reduce clutter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Community Detection\n",
    "\n",
    "Find clusters of related entities using the Louvain algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Community Detection\")\n",
    "\n",
    "# Convert to undirected for community detection\n",
    "G_undirected = G.to_undirected()\n",
    "\n",
    "# Louvain community detection\n",
    "try:\n",
    "    import community as community_louvain\n",
    "    \n",
    "    communities = community_louvain.best_partition(G_undirected)\n",
    "    \n",
    "    # Group nodes by community\n",
    "    community_groups = defaultdict(list)\n",
    "    for node, comm_id in communities.items():\n",
    "        community_groups[comm_id].append(node)\n",
    "    \n",
    "    print(f\"\\n✓ Found {len(community_groups)} communities\")\n",
    "    \n",
    "    # Show community sizes\n",
    "    print(\"\\nCommunity sizes:\")\n",
    "    for comm_id, members in sorted(community_groups.items(), key=lambda x: len(x[1]), reverse=True):\n",
    "        print(f\"  • Community {comm_id}: {len(members)} entities\")\n",
    "        print(f\"    Sample members: {', '.join(members[:5])}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"\\n⚠️  python-louvain not installed. Skipping community detection.\")\n",
    "    print(\"   Install with: pip install python-louvain\")\n",
    "    communities = {}\n",
    "    community_groups = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Graph Embeddings\n",
    "\n",
    "Embed nodes for semantic search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Creating Graph Embeddings\")\n",
    "\n",
    "# Create documents for each entity\n",
    "entity_docs = []\n",
    "\n",
    "for node in G.nodes():\n",
    "    node_data = G.nodes[node]\n",
    "    entity_type = node_data.get('type', 'UNKNOWN')\n",
    "    description = node_data.get('description', '')\n",
    "    \n",
    "    # Get connected entities\n",
    "    neighbors = list(G.neighbors(node))\n",
    "    neighbors_str = \", \".join(neighbors[:5]) if neighbors else \"none\"\n",
    "    \n",
    "    # Create document\n",
    "    content = f\"{node} ({entity_type}): {description}\\nConnected to: {neighbors_str}\"\n",
    "    \n",
    "    doc = Document(\n",
    "        page_content=content,\n",
    "        metadata={\n",
    "            \"entity_name\": node,\n",
    "            \"entity_type\": entity_type,\n",
    "            \"description\": description,\n",
    "            \"degree\": G.degree(node),\n",
    "        },\n",
    "    )\n",
    "    entity_docs.append(doc)\n",
    "\n",
    "print(f\"\\n✓ Created {len(entity_docs)} entity documents\")\n",
    "\n",
    "# Create vector store\n",
    "embeddings = OpenAIEmbeddings(model=EMBEDDINGS_MODEL)\n",
    "entity_store_path = VECTOR_STORE_PATH / \"graphrag_entities\"\n",
    "\n",
    "entity_vectorstore = load_vector_store(entity_store_path, embeddings)\n",
    "\n",
    "if entity_vectorstore is None:\n",
    "    print(\"\\nCreating entity vector store...\")\n",
    "    entity_vectorstore = FAISS.from_documents(entity_docs, embeddings)\n",
    "    save_vector_store(entity_vectorstore, entity_store_path)\n",
    "    print(\"✓ Entity vector store created\")\n",
    "else:\n",
    "    print(\"✓ Loaded existing entity vector store\")\n",
    "\n",
    "# Create retriever\n",
    "entity_retriever = entity_vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "print(\"✓ Entity retriever ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Implement Graph Traversal\n",
    "\n",
    "Navigate the graph to find relevant subgraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subgraph(\n",
    "    G: nx.DiGraph,\n",
    "    start_nodes: List[str],\n",
    "    max_hops: int = 2,\n",
    "    max_nodes: int = 20,\n",
    ") -> nx.DiGraph:\n",
    "    \"\"\"\n",
    "    Extract subgraph around starting nodes.\n",
    "    \n",
    "    Args:\n",
    "        G: Full knowledge graph\n",
    "        start_nodes: Starting entities\n",
    "        max_hops: Maximum distance to traverse\n",
    "        max_nodes: Maximum nodes in subgraph\n",
    "    \n",
    "    Returns:\n",
    "        Subgraph\n",
    "    \"\"\"\n",
    "    subgraph_nodes = set()\n",
    "    \n",
    "    # BFS from each start node\n",
    "    for start in start_nodes:\n",
    "        if start not in G:\n",
    "            continue\n",
    "        \n",
    "        # Get nodes within max_hops\n",
    "        for node in nx.single_source_shortest_path_length(\n",
    "            G.to_undirected(),\n",
    "            start,\n",
    "            cutoff=max_hops,\n",
    "        ).keys():\n",
    "            subgraph_nodes.add(node)\n",
    "            \n",
    "            if len(subgraph_nodes) >= max_nodes:\n",
    "                break\n",
    "        \n",
    "        if len(subgraph_nodes) >= max_nodes:\n",
    "            break\n",
    "    \n",
    "    # Create subgraph\n",
    "    return G.subgraph(subgraph_nodes).copy()\n",
    "\n",
    "\n",
    "def subgraph_to_text(subgraph: nx.DiGraph) -> str:\n",
    "    \"\"\"\n",
    "    Convert subgraph to text representation.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    \n",
    "    lines.append(f\"Entities ({len(subgraph.nodes())}):\")\n",
    "    for node in list(subgraph.nodes())[:15]:\n",
    "        node_data = subgraph.nodes[node]\n",
    "        entity_type = node_data.get('type', 'UNKNOWN')\n",
    "        description = node_data.get('description', '')[:100]\n",
    "        lines.append(f\"  • {node} ({entity_type}): {description}\")\n",
    "    \n",
    "    lines.append(f\"\\nRelationships ({len(subgraph.edges())}):\")\n",
    "    for source, target in list(subgraph.edges())[:15]:\n",
    "        edge_data = subgraph.edges[source, target]\n",
    "        relation = edge_data.get('relation', 'related_to')\n",
    "        lines.append(f\"  • {source} --[{relation}]--> {target}\")\n",
    "    \n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "print(\"✓ Graph traversal functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Build GraphRAG Query Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphrag_query(\n",
    "    question: str,\n",
    "    G: nx.DiGraph,\n",
    "    entity_retriever,\n",
    "    llm,\n",
    "    max_hops: int = 2,\n",
    "    verbose: bool = False,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    GraphRAG query pipeline.\n",
    "    \n",
    "    Args:\n",
    "        question: User question\n",
    "        G: Knowledge graph\n",
    "        entity_retriever: Entity retriever\n",
    "        llm: Language model\n",
    "        max_hops: Maximum graph traversal hops\n",
    "        verbose: Print debug info\n",
    "    \n",
    "    Returns:\n",
    "        Query results\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"\\n[GraphRAG] Question: {question}\")\n",
    "    \n",
    "    # 1. Retrieve relevant entities\n",
    "    relevant_docs = entity_retriever.invoke(question)\n",
    "    start_entities = [doc.metadata[\"entity_name\"] for doc in relevant_docs]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"[GraphRAG] Start entities: {', '.join(start_entities[:5])}\")\n",
    "    \n",
    "    # 2. Extract subgraph\n",
    "    subgraph = get_subgraph(G, start_entities, max_hops=max_hops)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"[GraphRAG] Subgraph: {len(subgraph.nodes())} nodes, {len(subgraph.edges())} edges\")\n",
    "    \n",
    "    # 3. Convert to text\n",
    "    context = subgraph_to_text(subgraph)\n",
    "    \n",
    "    # 4. Generate answer\n",
    "    answer_chain = GRAPHRAG_ANSWER_PROMPT | llm | StrOutputParser()\n",
    "    answer = answer_chain.invoke({\n",
    "        \"input\": question,\n",
    "        \"context\": context,\n",
    "        \"query_entities\": \", \".join(start_entities[:5]),\n",
    "        \"num_hops\": max_hops,\n",
    "        \"num_nodes\": len(subgraph.nodes()),\n",
    "    })\n",
    "    \n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"start_entities\": start_entities,\n",
    "        \"subgraph\": subgraph,\n",
    "        \"context\": context,\n",
    "        \"answer\": answer,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"✓ GraphRAG query pipeline defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Test GraphRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Testing GraphRAG\")\n",
    "\n",
    "test_questions = [\n",
    "    \"What is LangChain and what are its main components?\",\n",
    "    \"How do chains work in LangChain?\",\n",
    "    \"What role do retrievers play in RAG applications?\",\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Question {i}: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    result = graphrag_query(\n",
    "        question=question,\n",
    "        G=G,\n",
    "        entity_retriever=entity_retriever,\n",
    "        llm=llm,\n",
    "        max_hops=2,\n",
    "        verbose=True,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"ANSWER:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualize Query Subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Visualizing Query Subgraph\")\n",
    "\n",
    "# Get subgraph for a query\n",
    "query = \"What is LangChain?\"\n",
    "result = graphrag_query(\n",
    "    question=query,\n",
    "    G=G,\n",
    "    entity_retriever=entity_retriever,\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "subgraph = result[\"subgraph\"]\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 8))\n",
    "pos = nx.spring_layout(subgraph, k=1, iterations=50)\n",
    "\n",
    "# Draw\n",
    "nx.draw_networkx_nodes(subgraph, pos, node_size=700, node_color='lightgreen', alpha=0.8)\n",
    "nx.draw_networkx_edges(subgraph, pos, edge_color='gray', arrows=True, arrowsize=15, alpha=0.6)\n",
    "nx.draw_networkx_labels(subgraph, pos, font_size=9, font_weight='bold')\n",
    "\n",
    "plt.title(f\"Query Subgraph: '{query}'\", fontsize=14, fontweight='bold')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n✓ Subgraph for query: '{query}'\")\n",
    "print(f\"  • Nodes: {len(subgraph.nodes())}\")\n",
    "print(f\"  • Edges: {len(subgraph.edges())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Comparison: GraphRAG vs Vector RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"GraphRAG vs Vector RAG Comparison\")\n",
    "\n",
    "# Build simple vector RAG for comparison\n",
    "from shared.prompts import RAG_PROMPT_TEMPLATE\n",
    "from shared.utils import format_docs\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Load vector store\n",
    "vector_store_path = VECTOR_STORE_PATH / \"graphrag_comparison\"\n",
    "vectorstore = load_vector_store(vector_store_path, embeddings)\n",
    "\n",
    "if vectorstore is None:\n",
    "    vectorstore = FAISS.from_documents(docs_sample, embeddings)\n",
    "    save_vector_store(vectorstore, vector_store_path)\n",
    "\n",
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Build vector RAG chain\n",
    "vector_rag_chain = (\n",
    "    {\"context\": vector_retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | RAG_PROMPT_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Test query\n",
    "test_query = \"What are the relationships between LangChain components?\"\n",
    "\n",
    "print(f\"\\nQuery: {test_query}\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Vector RAG\n",
    "print(\"\\n[VECTOR RAG]\")\n",
    "print(\"-\" * 80)\n",
    "vector_answer = vector_rag_chain.invoke(test_query)\n",
    "print(vector_answer)\n",
    "\n",
    "# GraphRAG\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n[GRAPHRAG]\")\n",
    "print(\"-\" * 80)\n",
    "graph_result = graphrag_query(\n",
    "    question=test_query,\n",
    "    G=G,\n",
    "    entity_retriever=entity_retriever,\n",
    "    llm=llm,\n",
    "    verbose=False,\n",
    ")\n",
    "print(graph_result[\"answer\"])\n",
    "\n",
    "# Comparison\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Vector RAG:\")\n",
    "print(\"  • Finds semantically similar documents\")\n",
    "print(\"  • Fast retrieval\")\n",
    "print(\"  • May miss relationship context\")\n",
    "print(\"\\nGraphRAG:\")\n",
    "print(\"  • Understands entity relationships\")\n",
    "print(\"  • Multi-hop reasoning\")\n",
    "print(\"  • Explicit connection paths\")\n",
    "print(\"  • Better for 'how are X and Y related' queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Key Takeaways\n",
    "\n",
    "### Summary\n",
    "\n",
    "**GraphRAG** represents a paradigm shift from vectors to graphs:\n",
    "- Entities and relationships as first-class citizens\n",
    "- Multi-hop reasoning through graph traversal\n",
    "- Community-based summarization\n",
    "- Explainable reasoning paths\n",
    "\n",
    "### Pipeline Recap\n",
    "\n",
    "1. **Entity Extraction**: LLM identifies entities (⏱️ slow)\n",
    "2. **Relationship Extraction**: LLM finds connections (⏱️ slow)\n",
    "3. **Graph Construction**: Build NetworkX graph (✅ fast)\n",
    "4. **Community Detection**: Cluster related entities (✅ fast)\n",
    "5. **Graph Embeddings**: Index for search (✅ fast)\n",
    "6. **Query Processing**: Find relevant entities (✅ fast)\n",
    "7. **Graph Traversal**: Extract subgraph (✅ fast)\n",
    "8. **Answer Generation**: LLM synthesizes (⏱️ medium)\n",
    "\n",
    "### Cost Analysis\n",
    "\n",
    "| Phase | Cost | Notes |\n",
    "|-------|------|-------|\n",
    "| **Indexing** | ❌ Very High | 2N LLM calls (N docs) |\n",
    "| Entity extraction | High | 1 call per doc |\n",
    "| Relationship extraction | High | 1 call per doc |\n",
    "| Graph construction | Low | Local computation |\n",
    "| **Querying** | ✅ Low | 1-2 LLM calls |\n",
    "| Entity retrieval | Low | Vector search |\n",
    "| Graph traversal | Low | NetworkX operations |\n",
    "| Answer generation | Medium | 1 LLM call |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "**Entity Extraction:**\n",
    "- ✅ Use structured output (JSON)\n",
    "- ✅ Consistent entity types\n",
    "- ✅ Disambiguate similar entities\n",
    "- ✅ Merge duplicate entities\n",
    "\n",
    "**Relationship Extraction:**\n",
    "- ✅ Use semantic relation types\n",
    "- ✅ Validate both endpoints exist\n",
    "- ✅ Add confidence scores\n",
    "- ✅ Include descriptions\n",
    "\n",
    "**Graph Management:**\n",
    "- ✅ Limit subgraph size\n",
    "- ✅ Use efficient traversal algorithms\n",
    "- ✅ Cache common subgraphs\n",
    "- ✅ Persist graph to database (Neo4j, etc.)\n",
    "\n",
    "**Query Optimization:**\n",
    "- ✅ Start from high-degree nodes\n",
    "- ✅ Limit max hops (2-3)\n",
    "- ✅ Prune irrelevant branches\n",
    "- ✅ Use community summaries for global queries\n",
    "\n",
    "### GraphRAG vs Vector RAG\n",
    "\n",
    "| Aspect | Vector RAG | GraphRAG |\n",
    "|--------|------------|----------|\n",
    "| **Indexing Time** | ✅ Fast | ❌ Very Slow |\n",
    "| **Indexing Cost** | ✅ Low | ❌ Very High |\n",
    "| **Query Time** | ✅ Fast | ✅ Fast |\n",
    "| **Storage** | ✅ Efficient | ❌ Higher |\n",
    "| **Simple Queries** | ✅ Excellent | ⚠️ Overkill |\n",
    "| **Multi-hop** | ❌ Poor | ✅ Excellent |\n",
    "| **Relationships** | ❌ Implicit | ✅ Explicit |\n",
    "| **Explainability** | ⚠️ Medium | ✅ High |\n",
    "\n",
    "### When to Use GraphRAG\n",
    "\n",
    "Choose **GraphRAG** when:\n",
    "- ✅ Relationships are first-class citizens\n",
    "- ✅ Multi-hop reasoning is needed\n",
    "- ✅ Indexing cost is acceptable (one-time)\n",
    "- ✅ Explainability is important\n",
    "- ✅ Entity-centric queries\n",
    "- ✅ Complex knowledge domains\n",
    "\n",
    "Stick with **Vector RAG** when:\n",
    "- ✅ Simple document retrieval\n",
    "- ✅ Fast indexing required\n",
    "- ✅ Cost-sensitive\n",
    "- ✅ Relationships aren't critical\n",
    "- ✅ Semantic similarity is enough\n",
    "\n",
    "### Production Considerations\n",
    "\n",
    "**Scale:**\n",
    "- Use graph databases (Neo4j, Amazon Neptune)\n",
    "- Batch entity/relationship extraction\n",
    "- Implement incremental updates\n",
    "- Cache frequently accessed subgraphs\n",
    "\n",
    "**Quality:**\n",
    "- Manual entity review\n",
    "- Confidence scoring\n",
    "- Entity merging/deduplication\n",
    "- Relationship validation\n",
    "\n",
    "**Performance:**\n",
    "- Pre-compute common subgraphs\n",
    "- Index graph properties\n",
    "- Use graph algorithms (PageRank, etc.)\n",
    "- Optimize traversal paths\n",
    "\n",
    "### Extensions\n",
    "\n",
    "**Advanced Features:**\n",
    "- Temporal graphs (time-aware relationships)\n",
    "- Probabilistic edges (confidence scores)\n",
    "- Multi-modal graphs (text + images + code)\n",
    "- Dynamic graph updates\n",
    "- Graph neural networks (GNNs)\n",
    "\n",
    "**Microsoft GraphRAG Features:**\n",
    "- Global vs Local queries\n",
    "- Hierarchical communities\n",
    "- Community summaries\n",
    "- Map-Reduce over communities\n",
    "\n",
    "---\n",
    "\n",
    "**Complexity Rating:** ⭐⭐⭐⭐⭐ (Very High - most complex RAG architecture)\n",
    "\n",
    "**Production Readiness:** ⭐⭐ (Experimental - requires significant investment)\n",
    "\n",
    "**Best For:** Research, knowledge-intensive applications, when relationships matter more than documents\n",
    "\n",
    "Continue to **16_evaluation_ragas.ipynb** for comprehensive RAG evaluation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
