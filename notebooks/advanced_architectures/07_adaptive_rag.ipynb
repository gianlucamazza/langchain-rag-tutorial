{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - Adaptive RAG (Query Complexity Router)\n",
    "\n",
    "**Complexity:** ⭐⭐⭐⭐\n",
    "\n",
    "**Use Cases:** Mixed workloads, cost optimization, performance/quality balance\n",
    "\n",
    "**Key Feature:** Classifies query complexity and routes to optimal strategy.\n",
    "\n",
    "**Routing Logic:**\n",
    "```\n",
    "SIMPLE queries    → Fast similarity search\n",
    "MEDIUM queries    → MMR for diversity\n",
    "COMPLEX queries   → HyDe for better semantic matching\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SETUP: ADAPTIVE RAG\n",
      "================================================================================\n",
      "\n",
      "✓ Loaded vector store from /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/advanced_architectures/../../data/vector_stores/openai_embeddings\n",
      "✅ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from shared.config import OPENAI_VECTOR_STORE_PATH, DEFAULT_MODEL\n",
    "from shared.utils import load_vector_store, print_section_header, format_docs\n",
    "from shared.prompts import COMPLEXITY_CLASSIFIER_PROMPT, ADAPTIVE_RAG_PROMPT, HYDE_PROMPT\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "print_section_header(\"Setup: Adaptive RAG\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = load_vector_store(OPENAI_VECTOR_STORE_PATH, embeddings)\n",
    "llm = ChatOpenAI(model=DEFAULT_MODEL, temperature=0)\n",
    "\n",
    "# Create retrievers\n",
    "similarity_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "mmr_retriever = vectorstore.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 4, \"fetch_k\": 20, \"lambda_mult\": 0.5}\n",
    ")\n",
    "\n",
    "print(\"✅ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Complexity Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPLEXITY CLASSIFIER\n",
      "================================================================================\n",
      "\n",
      "SIMPLE   | What is FAISS?\n",
      "MEDIUM   | Compare OpenAI vs HuggingFace embeddings\n",
      "COMPLEX  | How to architect production RAG with privacy and cost constraints?\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Complexity Classifier\")\n",
    "\n",
    "complexity_classifier = COMPLEXITY_CLASSIFIER_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "# Test classifier\n",
    "test_queries = [\n",
    "    \"What is FAISS?\",  # SIMPLE\n",
    "    \"Compare OpenAI vs HuggingFace embeddings\",  # MEDIUM\n",
    "    \"How to architect production RAG with privacy and cost constraints?\"  # COMPLEX\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    complexity = complexity_classifier.invoke({\"query\": query}).strip()\n",
    "    print(f\"{complexity:8} | {query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adaptive Router"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ADAPTIVE ROUTER\n",
      "================================================================================\n",
      "\n",
      "✓ Adaptive router configured\n",
      "  - SIMPLE → Similarity (fast)\n",
      "  - MEDIUM → MMR (diverse)\n",
      "  - COMPLEX → HyDe (semantic)\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Adaptive Router\")\n",
    "\n",
    "# HyDe generator for complex queries\n",
    "hyde_generator = HYDE_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "def adaptive_route(query: str):\n",
    "    \"\"\"Route query to appropriate retrieval strategy.\"\"\"\n",
    "    complexity = complexity_classifier.invoke({\"query\": query}).strip()\n",
    "    \n",
    "    if \"SIMPLE\" in complexity:\n",
    "        docs = similarity_retriever.invoke(query)\n",
    "        strategy = \"SIMPLE-Similarity\"\n",
    "    elif \"MEDIUM\" in complexity:\n",
    "        docs = mmr_retriever.invoke(query)\n",
    "        strategy = \"MEDIUM-MMR\"\n",
    "    else:  # COMPLEX\n",
    "        hypo_doc = hyde_generator.invoke({\"question\": query})\n",
    "        docs = vectorstore.similarity_search(hypo_doc, k=4)\n",
    "        strategy = \"COMPLEX-HyDe\"\n",
    "    \n",
    "    return {\"context\": format_docs(docs), \"input\": query, \"strategy\": strategy}\n",
    "\n",
    "print(\"✓ Adaptive router configured\")\n",
    "print(\"  - SIMPLE → Similarity (fast)\")\n",
    "print(\"  - MEDIUM → MMR (diverse)\")\n",
    "print(\"  - COMPLEX → HyDe (semantic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adaptive RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ADAPTIVE RAG CHAIN\n",
      "================================================================================\n",
      "\n",
      "✓ Adaptive RAG chain created\n",
      "\n",
      "\n",
      "Query (SIMPLE): 'What is a retriever?'\n",
      "================================================================================\n",
      "A retriever is a component in a Retrieval Augmented Generation (RAG) system that is responsible for searching and retrieving relevant documents or information from a storage system based on a user's i...\n",
      "\n",
      "\n",
      "Query (MEDIUM): 'Compare similarity vs MMR'\n",
      "================================================================================\n",
      "Similarity and Maximum Marginal Relevance (MMR) are both techniques used in information retrieval and document summarization, but they serve different purposes and operate based on different principle...\n",
      "\n",
      "\n",
      "Query (COMPLEX): 'How to handle ambiguous queries in multi-domain systems?'\n",
      "================================================================================\n",
      "Handling ambiguous queries in multi-domain systems can be challenging, but there are several strategies that can be employed to improve clarity and accuracy in responses:\n",
      "\n",
      "1. **Clarification Questions...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Adaptive RAG Chain\")\n",
    "\n",
    "adaptive_chain = (\n",
    "    RunnableLambda(adaptive_route)\n",
    "    | ADAPTIVE_RAG_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✓ Adaptive RAG chain created\\n\")\n",
    "\n",
    "# Test with different complexity queries\n",
    "test_cases = [\n",
    "    (\"What is a retriever?\", \"SIMPLE\"),\n",
    "    (\"Compare similarity vs MMR\", \"MEDIUM\"),\n",
    "    (\"How to handle ambiguous queries in multi-domain systems?\", \"COMPLEX\")\n",
    "]\n",
    "\n",
    "for query, expected in test_cases:\n",
    "    print(f\"\\nQuery ({expected}): '{query}'\")\n",
    "    print(\"=\" * 80)\n",
    "    response = adaptive_chain.invoke(query)\n",
    "    # Show only first 200 chars\n",
    "    print(response[:200] + \"...\" if len(response) > 200 else response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "Query → Classify Complexity → Route to Strategy → Retrieve → LLM → Response\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "✅ Optimized cost (simple queries use fast path)  \n",
    "✅ Balanced quality/speed  \n",
    "✅ Adaptive to query difficulty  \n",
    "✅ Scalable for mixed workloads  \n",
    "\n",
    "**Limitations:**\n",
    "- Classification overhead\n",
    "- Requires tuning thresholds\n",
    "- More complex to debug\n",
    "\n",
    "**Production Tips:**\n",
    "- Cache classification results\n",
    "- Monitor routing distribution\n",
    "- A/B test routing logic\n",
    "- Add fallback strategy\n",
    "\n",
    "**Next:** [08_corrective_rag.ipynb](08_corrective_rag.ipynb) - CRAG with web search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
