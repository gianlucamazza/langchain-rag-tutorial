{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - HyDe (Hypothetical Document Embeddings)\n",
    "\n",
    "**Complexity:** ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "**Use Cases:** Ambiguous queries, domain jargon, queries with abbreviations\n",
    "\n",
    "**Key Feature:** Generates hypothetical \"perfect answer\" document, embeds it, uses for retrieval.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Query: \"How does MMR work?\"\n",
    "\n",
    "Hypothetical Doc:\n",
    "\"MMR (Maximal Marginal Relevance) balances relevance with diversity by\n",
    "iteratively selecting documents that are relevant to query AND dissimilar\n",
    "to already selected documents...\"\n",
    "\n",
    "‚Üí Embedding this detailed description finds better semantic matches\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SETUP: HYDE\n",
      "================================================================================\n",
      "\n",
      "‚úì Loaded vector store from /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/advanced_architectures/../../data/vector_stores/openai_embeddings\n",
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from shared.config import OPENAI_VECTOR_STORE_PATH, DEFAULT_MODEL\n",
    "from shared.utils import load_vector_store, print_section_header, format_docs\n",
    "from shared.prompts import HYDE_PROMPT, RAG_PROMPT_TEMPLATE\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "print_section_header(\"Setup: HyDe\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = load_vector_store(OPENAI_VECTOR_STORE_PATH, embeddings)\n",
    "llm = ChatOpenAI(model=DEFAULT_MODEL, temperature=0)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HyDe Document Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYDE GENERATOR\n",
      "================================================================================\n",
      "\n",
      "Query: 'What is semantic search?'\n",
      "\n",
      "Generated Hypothetical Document:\n",
      "================================================================================\n",
      "# Understanding Semantic Search\n",
      "\n",
      "## Introduction\n",
      "Semantic search is an advanced search technique that aims to improve search accuracy by understanding the intent and contextual meaning of search queries. Unlike traditional keyword-based search methods, which rely heavily on matching keywords in the query with those in the database, semantic search focuses on the relationships between words and the concepts they represent. This document provides a comprehensive overview of semantic search, its principles, technologies, applications, and benefits.\n",
      "\n",
      "## Key Principles of Semantic Search\n",
      "\n",
      "### 1. Contextual Understanding\n",
      "Semantic search systems analyze the context in which words are used. This involves understanding synonyms, antonyms, and the nuances of language. For example, the word \"bank\" can refer to a financial institution or the side of a river, and semantic search helps determine the correct meaning based on the surrounding words.\n",
      "\n",
      "### 2. Natural Language Processing (NLP)\n",
      "NLP is a critical component of semantic search. It enables machines to understand, interpret, and respond to human language in a valuable way. Techniques such as tokenization, part-of-speech tagging, and named entity recognition are employed to break down and analyze the structure of sentences.\n",
      "\n",
      "### 3. Knowledge Graphs\n",
      "Knowledge graphs are structured representations of information that illustrate the relationships between different entities. They play a vital role in semantic search by providing a framework for understanding how concepts are interconnected. For instance, a knowledge graph can link \"Barack Obama\" to \"President of the United States\" and \"Nobel Peace Prize,\" enhancing the search engine's ability to deliver relevant results.\n",
      "\n",
      "### 4. User Intent\n",
      "Understanding user intent is crucial for delivering relevant search results. Semantic search systems analyze the user's query to determine what they are truly looking for, whether it is information, a product, or a service. This involves considering factors such as search history, location, and the phrasing of the query.\n",
      "\n",
      "## Technologies Behind Semantic Search\n",
      "\n",
      "### 1. Machine Learning\n",
      "Machine learning algorithms are employed to improve the accuracy of semantic search. These algorithms learn from user interactions and feedback, allowing the system to refine its understanding of language and user intent over time.\n",
      "\n",
      "### 2. Ontologies\n",
      "Ontologies are formal representations of a set of concepts within a domain and the relationships between those concepts. They help semantic search engines understand the meaning of terms and their interrelations, facilitating more accurate search results.\n",
      "\n",
      "### 3. Semantic Indexing\n",
      "Semantic indexing involves creating an index that captures the meaning of words and phrases rather than just their literal definitions. This allows search engines to retrieve results based on the underlying concepts rather than exact keyword matches.\n",
      "\n",
      "## Applications of Semantic Search\n",
      "\n",
      "### 1. Search Engines\n",
      "Major search engines like Google and Bing utilize semantic search to enhance the user experience. Features such as \"People also ask\" and \"Related searches\" are examples of how these engines leverage semantic understanding to provide more relevant results.\n",
      "\n",
      "### 2. E-commerce\n",
      "In e-commerce, semantic search helps users find products more efficiently. By understanding the context of a user's search, e-commerce platforms can recommend products that align with the user's intent, even if the exact keywords are not used.\n",
      "\n",
      "### 3. Content Management Systems\n",
      "Semantic search can improve content discovery within large databases or content management systems. By understanding the relationships between different pieces of content, users can find relevant information more quickly.\n",
      "\n",
      "### 4. Virtual Assistants\n",
      "Virtual assistants like Siri, Alexa, and Google Assistant rely on semantic search to interpret user commands and provide accurate responses. This technology allows them to understand natural language queries and deliver relevant information or perform tasks.\n",
      "\n",
      "## Benefits of Semantic Search\n",
      "\n",
      "### 1. Improved Relevance\n",
      "Semantic search enhances the relevance of search results by focusing on user intent and context, leading to a better user experience.\n",
      "\n",
      "### 2. Enhanced User Engagement\n",
      "By providing more accurate and meaningful results, semantic search can increase user engagement and satisfaction, encouraging users to return to the platform.\n",
      "\n",
      "### 3. Reduced Search Time\n",
      "Users can find the information they need more quickly, as semantic search reduces the need for multiple queries and clarifications.\n",
      "\n",
      "### 4. Better Insights\n",
      "Organizations can gain valuable insights into user behavior and preferences through the analysis of semantic search data, allowing for more informed decision-making.\n",
      "\n",
      "## Conclusion\n",
      "Semantic search represents a significant advancement in the field of information retrieval. By focusing on understanding the meaning behind queries and the relationships between concepts, semantic search enhances the accuracy and relevance of search results. As technology continues to evolve, the capabilities of semantic search are expected to expand, further transforming how users interact with information online.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"HyDe Generator\")\n",
    "\n",
    "# Create HyDe document generator\n",
    "hyde_generator = HYDE_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "# Test\n",
    "query = \"What is semantic search?\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "hypo_doc = hyde_generator.invoke({\"question\": query})\n",
    "print(\"Generated Hypothetical Document:\")\n",
    "print(\"=\" * 80)\n",
    "print(hypo_doc)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HyDe Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYDE VS STANDARD RETRIEVAL\n",
      "================================================================================\n",
      "\n",
      "[STANDARD RETRIEVAL]\n",
      "\n",
      "Retrieved Documents\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are appl...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are appl...\n",
      "\n",
      "... and 1 more documents\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[HYDE RETRIEVAL]\n",
      "\n",
      "Generated doc preview: # Improving Retrieval Quality: A Comprehensive Guide\n",
      "\n",
      "## Introduction\n",
      "Retrieval quality refers to the effectiveness and accuracy with which information is retrieved from a database, search engine, or ...\n",
      "\n",
      "\n",
      "Retrieved Documents\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are appl...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are appl...\n",
      "\n",
      "... and 1 more documents\n",
      "\n",
      "üí° HyDe often finds more semantically relevant documents\n"
     ]
    }
   ],
   "source": [
    "from shared.utils import print_results\n",
    "\n",
    "print_section_header(\"HyDe vs Standard Retrieval\")\n",
    "\n",
    "query = \"How to improve retrieval quality?\"\n",
    "\n",
    "# Standard retrieval\n",
    "print(\"[STANDARD RETRIEVAL]\")\n",
    "standard_docs = vectorstore.similarity_search(query, k=3)\n",
    "print_results(standard_docs, max_docs=2, preview_length=120)\n",
    "\n",
    "# HyDe retrieval\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n[HYDE RETRIEVAL]\")\n",
    "hypo_doc = hyde_generator.invoke({\"question\": query})\n",
    "print(f\"\\nGenerated doc preview: {hypo_doc[:200]}...\\n\")\n",
    "hyde_docs = vectorstore.similarity_search(hypo_doc, k=3)\n",
    "print_results(hyde_docs, max_docs=2, preview_length=120)\n",
    "\n",
    "print(\"\\nüí° HyDe often finds more semantically relevant documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HyDe RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYDE RAG CHAIN\n",
      "================================================================================\n",
      "\n",
      "‚úì HyDe RAG chain created\n",
      "\n",
      "Query: 'Best practices for chunk sizing?'\n",
      "\n",
      "================================================================================\n",
      "The context provided does not contain specific information about best practices for chunk sizing. It only mentions that \"Text splitters break large Documents into smaller chunks\" and that \"large chunks are harder to search over and won‚Äôt fit in a model‚Äôs finite context window.\" \n",
      "\n",
      "For best practices on chunk sizing, you may want to consider factors such as the model's context window size, the nature of the data, and the intended use of the chunks, but this information is not detailed in the context provided.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"HyDe RAG Chain\")\n",
    "\n",
    "def hyde_retrieve(query: str):\n",
    "    hypo_doc = hyde_generator.invoke({\"question\": query})\n",
    "    docs = vectorstore.similarity_search(hypo_doc, k=4)\n",
    "    return docs\n",
    "\n",
    "hyde_retriever = RunnableLambda(hyde_retrieve)\n",
    "\n",
    "hyde_chain = (\n",
    "    {\"context\": hyde_retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | RAG_PROMPT_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úì HyDe RAG chain created\")\n",
    "\n",
    "# Test\n",
    "query = \"Best practices for chunk sizing?\"\n",
    "print(f\"\\nQuery: '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "response = hyde_chain.invoke(query)\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "Query ‚Üí Generate Hypo Doc ‚Üí Embed ‚Üí Retrieve ‚Üí LLM ‚Üí Response\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "‚úÖ Better for ambiguous queries  \n",
    "‚úÖ Handles jargon and abbreviations  \n",
    "‚úÖ Improves semantic matching  \n",
    "‚úÖ Works with specialized domains  \n",
    "\n",
    "**Limitations:**\n",
    "- Extra LLM call (cost + latency)\n",
    "- May hallucinate in hypo doc\n",
    "- Not always better than standard\n",
    "\n",
    "**When to Use:**\n",
    "- Vague or ambiguous queries\n",
    "- Technical jargon\n",
    "- Queries with abbreviations\n",
    "\n",
    "**Next:** [07_adaptive_rag.ipynb](07_adaptive_rag.ipynb) - Intelligent query routing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
