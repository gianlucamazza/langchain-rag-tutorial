{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - HyDe (Hypothetical Document Embeddings)\n",
    "\n",
    "**Complexity:** ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "**Use Cases:** Ambiguous queries, domain jargon, queries with abbreviations\n",
    "\n",
    "**Key Feature:** Generates hypothetical \"perfect answer\" document, embeds it, uses for retrieval.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Query: \"How does MMR work?\"\n",
    "\n",
    "Hypothetical Doc:\n",
    "\"MMR (Maximal Marginal Relevance) balances relevance with diversity by\n",
    "iteratively selecting documents that are relevant to query AND dissimilar\n",
    "to already selected documents...\"\n",
    "\n",
    "‚Üí Embedding this detailed description finds better semantic matches\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from shared.config import OPENAI_VECTOR_STORE_PATH, DEFAULT_MODEL\n",
    "from shared.utils import load_vector_store, print_section_header, format_docs\n",
    "from shared.prompts import HYDE_PROMPT, RAG_PROMPT_TEMPLATE\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "print_section_header(\"Setup: HyDe\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = load_vector_store(OPENAI_VECTOR_STORE_PATH, embeddings)\n",
    "llm = ChatOpenAI(model=DEFAULT_MODEL, temperature=0)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HyDe Document Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"HyDe Generator\")\n",
    "\n",
    "# Create HyDe document generator\n",
    "hyde_generator = HYDE_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "# Test\n",
    "query = \"What is semantic search?\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "hypo_doc = hyde_generator.invoke({\"question\": query})\n",
    "print(\"Generated Hypothetical Document:\")\n",
    "print(\"=\" * 80)\n",
    "print(hypo_doc)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HyDe Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.utils import print_results\n",
    "\n",
    "print_section_header(\"HyDe vs Standard Retrieval\")\n",
    "\n",
    "query = \"How to improve retrieval quality?\"\n",
    "\n",
    "# Standard retrieval\n",
    "print(\"[STANDARD RETRIEVAL]\")\n",
    "standard_docs = vectorstore.similarity_search(query, k=3)\n",
    "print_results(standard_docs, max_docs=2, preview_length=120)\n",
    "\n",
    "# HyDe retrieval\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n[HYDE RETRIEVAL]\")\n",
    "hypo_doc = hyde_generator.invoke({\"question\": query})\n",
    "print(f\"\\nGenerated doc preview: {hypo_doc[:200]}...\\n\")\n",
    "hyde_docs = vectorstore.similarity_search(hypo_doc, k=3)\n",
    "print_results(hyde_docs, max_docs=2, preview_length=120)\n",
    "\n",
    "print(\"\\nüí° HyDe often finds more semantically relevant documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HyDe RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"HyDe RAG Chain\")\n",
    "\n",
    "def hyde_retrieve(query: str):\n",
    "    hypo_doc = hyde_generator.invoke({\"question\": query})\n",
    "    docs = vectorstore.similarity_search(hypo_doc, k=4)\n",
    "    return docs\n",
    "\n",
    "hyde_retriever = RunnableLambda(hyde_retrieve)\n",
    "\n",
    "hyde_chain = (\n",
    "    {\"context\": hyde_retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | RAG_PROMPT_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úì HyDe RAG chain created\")\n",
    "\n",
    "# Test\n",
    "query = \"Best practices for chunk sizing?\"\n",
    "print(f\"\\nQuery: '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "response = hyde_chain.invoke(query)\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "Query ‚Üí Generate Hypo Doc ‚Üí Embed ‚Üí Retrieve ‚Üí LLM ‚Üí Response\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "‚úÖ Better for ambiguous queries  \n",
    "‚úÖ Handles jargon and abbreviations  \n",
    "‚úÖ Improves semantic matching  \n",
    "‚úÖ Works with specialized domains  \n",
    "\n",
    "**Limitations:**\n",
    "- Extra LLM call (cost + latency)\n",
    "- May hallucinate in hypo doc\n",
    "- Not always better than standard\n",
    "\n",
    "**When to Use:**\n",
    "- Vague or ambiguous queries\n",
    "- Technical jargon\n",
    "- Queries with abbreviations\n",
    "\n",
    "**Next:** [07_adaptive_rag.ipynb](07_adaptive_rag.ipynb) - Intelligent query routing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
