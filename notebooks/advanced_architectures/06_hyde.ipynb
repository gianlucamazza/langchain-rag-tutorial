{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - HyDe (Hypothetical Document Embeddings)\n",
    "\n",
    "**Complexity:** ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "**Use Cases:** Ambiguous queries, domain jargon, queries with abbreviations\n",
    "\n",
    "**Key Feature:** Generates hypothetical \"perfect answer\" document, embeds it, uses for retrieval.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Query: \"How does MMR work?\"\n",
    "\n",
    "Hypothetical Doc:\n",
    "\"MMR (Maximal Marginal Relevance) balances relevance with diversity by\n",
    "iteratively selecting documents that are relevant to query AND dissimilar\n",
    "to already selected documents...\"\n",
    "\n",
    "‚Üí Embedding this detailed description finds better semantic matches\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gianlucamazza/Workspace/notebooks/llm_rag/venv/lib/python3.14/site-packages/langchain_core/_api/deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SETUP: HYDE\n",
      "================================================================================\n",
      "\n",
      "‚úì Loaded vector store from /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/advanced_architectures/../../data/vector_stores/openai_embeddings\n",
      "‚úÖ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from shared.config import OPENAI_VECTOR_STORE_PATH, DEFAULT_MODEL\n",
    "from shared.utils import load_vector_store, print_section_header, format_docs\n",
    "from shared.prompts import HYDE_PROMPT, RAG_PROMPT_TEMPLATE\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "print_section_header(\"Setup: HyDe\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = load_vector_store(OPENAI_VECTOR_STORE_PATH, embeddings)\n",
    "llm = ChatOpenAI(model=DEFAULT_MODEL, temperature=0)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. HyDe Document Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYDE GENERATOR\n",
      "================================================================================\n",
      "\n",
      "Query: 'What is semantic search?'\n",
      "\n",
      "Generated Hypothetical Document:\n",
      "================================================================================\n",
      "# Understanding Semantic Search\n",
      "\n",
      "## Introduction\n",
      "Semantic search is an advanced search technique that aims to improve search accuracy by understanding the intent and contextual meaning of search queries. Unlike traditional keyword-based search methods, which rely heavily on matching keywords in the query with those in the database, semantic search focuses on the relationships between words and the concepts they represent. This document provides a comprehensive overview of semantic search, its principles, technologies, applications, and benefits.\n",
      "\n",
      "## Key Principles of Semantic Search\n",
      "\n",
      "### 1. Contextual Understanding\n",
      "Semantic search systems analyze the context in which words are used. This involves understanding synonyms, antonyms, and the nuances of language. For example, the word \"bank\" can refer to a financial institution or the side of a river, and semantic search helps determine the correct meaning based on the surrounding words.\n",
      "\n",
      "### 2. Intent Recognition\n",
      "Semantic search aims to discern the user's intent behind a query. This involves analyzing the query to understand what the user is truly looking for, whether it is information, a product, or a service. For instance, a search for \"best restaurants\" may indicate a desire for recommendations rather than just a list of restaurants.\n",
      "\n",
      "### 3. Knowledge Graphs\n",
      "Many semantic search engines utilize knowledge graphs, which are structured representations of knowledge that connect entities and their relationships. Knowledge graphs help search engines provide more relevant results by understanding how different concepts relate to one another.\n",
      "\n",
      "### 4. Natural Language Processing (NLP)\n",
      "NLP is a critical component of semantic search. It enables machines to understand, interpret, and respond to human language in a meaningful way. Techniques such as tokenization, part-of-speech tagging, and named entity recognition are employed to analyze and process queries.\n",
      "\n",
      "### 5. Machine Learning\n",
      "Machine learning algorithms are used to improve the accuracy of semantic search over time. By analyzing user interactions and feedback, these algorithms can learn to refine search results and better understand user preferences.\n",
      "\n",
      "## Technologies Behind Semantic Search\n",
      "\n",
      "### 1. Ontologies\n",
      "Ontologies are formal representations of a set of concepts within a domain and the relationships between those concepts. They provide a framework for semantic search engines to understand the structure of knowledge in a specific area.\n",
      "\n",
      "### 2. Semantic Markup\n",
      "Semantic markup involves using specific tags in web content (such as Schema.org) to provide additional context to search engines. This helps search engines better understand the content and improve the relevance of search results.\n",
      "\n",
      "### 3. Latent Semantic Analysis (LSA)\n",
      "LSA is a technique used to identify patterns in the relationships between terms and concepts in large datasets. It helps in understanding the underlying meaning of words and phrases, enhancing the search engine's ability to retrieve relevant information.\n",
      "\n",
      "### 4. Vector Space Models\n",
      "These models represent words and phrases as vectors in a multi-dimensional space, allowing for the measurement of similarity between different terms based on their context and usage.\n",
      "\n",
      "## Applications of Semantic Search\n",
      "\n",
      "### 1. Search Engines\n",
      "Major search engines like Google and Bing have integrated semantic search capabilities to enhance user experience. They provide more relevant search results by understanding user intent and context.\n",
      "\n",
      "### 2. E-commerce\n",
      "E-commerce platforms utilize semantic search to improve product discovery. By understanding user queries, these platforms can recommend products that align with user preferences and intent.\n",
      "\n",
      "### 3. Content Management Systems\n",
      "Semantic search is used in content management systems to help users find relevant documents and information quickly. By understanding the context of queries, these systems can deliver more accurate results.\n",
      "\n",
      "### 4. Virtual Assistants\n",
      "Virtual assistants like Siri, Alexa, and Google Assistant leverage semantic search to interpret user commands and provide relevant responses, enhancing user interaction and satisfaction.\n",
      "\n",
      "## Benefits of Semantic Search\n",
      "\n",
      "### 1. Improved Relevance\n",
      "Semantic search provides more accurate and relevant search results by understanding the context and intent behind queries, leading to a better user experience.\n",
      "\n",
      "### 2. Enhanced User Engagement\n",
      "By delivering more relevant content, semantic search can increase user engagement and satisfaction, encouraging users to spend more time on a platform.\n",
      "\n",
      "### 3. Better Content Discovery\n",
      "Semantic search helps users discover content that they may not have found through traditional keyword searches, broadening their access to information.\n",
      "\n",
      "### 4. Increased Conversion Rates\n",
      "In e-commerce, semantic search can lead to higher conversion rates by presenting users with products that closely match their needs and preferences.\n",
      "\n",
      "## Conclusion\n",
      "Semantic search represents a significant advancement in the field of information retrieval. By focusing on understanding the meaning and context of queries, it enhances the search experience for users across various domains. As technology continues to evolve, the capabilities of semantic search are expected to expand, further improving the way we access and interact with information.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"HyDe Generator\")\n",
    "\n",
    "# Create HyDe document generator\n",
    "hyde_generator = HYDE_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "# Test\n",
    "query = \"What is semantic search?\"\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "\n",
    "hypo_doc = hyde_generator.invoke({\"question\": query})\n",
    "print(\"Generated Hypothetical Document:\")\n",
    "print(\"=\" * 80)\n",
    "print(hypo_doc)\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. HyDe Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYDE VS STANDARD RETRIEVAL\n",
      "================================================================================\n",
      "\n",
      "[STANDARD RETRIEVAL]\n",
      "\n",
      "Retrieved Documents\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are appl...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are appl...\n",
      "\n",
      "... and 1 more documents\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[HYDE RETRIEVAL]\n",
      "\n",
      "Generated doc preview: # Improving Retrieval Quality: A Comprehensive Guide\n",
      "\n",
      "## Introduction\n",
      "Retrieval quality refers to the effectiveness and accuracy with which information is retrieved from a database, search engine, or ...\n",
      "\n",
      "\n",
      "Retrieved Documents\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are appl...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are appl...\n",
      "\n",
      "... and 1 more documents\n",
      "\n",
      "üí° HyDe often finds more semantically relevant documents\n"
     ]
    }
   ],
   "source": [
    "from shared.utils import print_results\n",
    "\n",
    "print_section_header(\"HyDe vs Standard Retrieval\")\n",
    "\n",
    "query = \"How to improve retrieval quality?\"\n",
    "\n",
    "# Standard retrieval\n",
    "print(\"[STANDARD RETRIEVAL]\")\n",
    "standard_docs = vectorstore.similarity_search(query, k=3)\n",
    "print_results(standard_docs, max_docs=2, preview_length=120)\n",
    "\n",
    "# HyDe retrieval\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n[HYDE RETRIEVAL]\")\n",
    "hypo_doc = hyde_generator.invoke({\"question\": query})\n",
    "print(f\"\\nGenerated doc preview: {hypo_doc[:200]}...\\n\")\n",
    "hyde_docs = vectorstore.similarity_search(hypo_doc, k=3)\n",
    "print_results(hyde_docs, max_docs=2, preview_length=120)\n",
    "\n",
    "print(\"\\nüí° HyDe often finds more semantically relevant documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. HyDe RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "HYDE RAG CHAIN\n",
      "================================================================================\n",
      "\n",
      "‚úì HyDe RAG chain created\n",
      "\n",
      "Query: 'Best practices for chunk sizing?'\n",
      "\n",
      "================================================================================\n",
      "The context provided does not contain specific information about best practices for chunk sizing. It only mentions that \"Text splitters break large Documents into smaller chunks\" and that \"large chunks are harder to search over and won‚Äôt fit in a model‚Äôs finite context window.\" \n",
      "\n",
      "For best practices, you may want to consider factors such as the model's context window size, the nature of the data, and the intended use of the chunks, but this information is not detailed in the context provided.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"HyDe RAG Chain\")\n",
    "\n",
    "def hyde_retrieve(query: str):\n",
    "    hypo_doc = hyde_generator.invoke({\"question\": query})\n",
    "    docs = vectorstore.similarity_search(hypo_doc, k=4)\n",
    "    return docs\n",
    "\n",
    "hyde_retriever = RunnableLambda(hyde_retrieve)\n",
    "\n",
    "hyde_chain = (\n",
    "    {\"context\": hyde_retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | RAG_PROMPT_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úì HyDe RAG chain created\")\n",
    "\n",
    "# Test\n",
    "query = \"Best practices for chunk sizing?\"\n",
    "print(f\"\\nQuery: '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "response = hyde_chain.invoke(query)\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "Query ‚Üí Generate Hypo Doc ‚Üí Embed ‚Üí Retrieve ‚Üí LLM ‚Üí Response\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "‚úÖ Better for ambiguous queries  \n",
    "‚úÖ Handles jargon and abbreviations  \n",
    "‚úÖ Improves semantic matching  \n",
    "‚úÖ Works with specialized domains  \n",
    "\n",
    "**Limitations:**\n",
    "- Extra LLM call (cost + latency)\n",
    "- May hallucinate in hypo doc\n",
    "- Not always better than standard\n",
    "\n",
    "**When to Use:**\n",
    "- Vague or ambiguous queries\n",
    "- Technical jargon\n",
    "- Queries with abbreviations\n",
    "\n",
    "**Next:** [07_adaptive_rag.ipynb](07_adaptive_rag.ipynb) - Intelligent query routing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
