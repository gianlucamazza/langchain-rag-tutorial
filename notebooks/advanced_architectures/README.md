# Advanced RAG Architectures

This directory contains implementations of **12 advanced RAG architectures**, each optimized for different use cases and requirements, plus a comprehensive evaluation framework.

## Prerequisites

Before exploring advanced architectures, complete the fundamentals:

1. ‚úì `fundamentals/01_setup_and_basics.ipynb`
2. ‚úì `fundamentals/02_embeddings_comparison.ipynb`
3. ‚úì `fundamentals/03_simple_rag.ipynb`

These provide the baseline components (vector stores, embeddings, retrievers) used by all advanced architectures.

---

## Architecture Overview

| Notebook | Architecture | Complexity | Use Case | Key Feature |
|----------|--------------|------------|----------|-------------|
| **04** | RAG with Memory | ‚≠ê‚≠ê | Conversational AI, Support Bots | Maintains chat history for follow-up questions |
| **05** | Branched RAG | ‚≠ê‚≠ê‚≠ê | Multi-domain search, Analysis | Parallel sub-query generation |
| **06** | HyDe | ‚≠ê‚≠ê‚≠ê | Ambiguous queries, Specialized domains | Hypothetical document generation |
| **07** | Adaptive RAG | ‚≠ê‚≠ê‚≠ê‚≠ê | Mixed workloads, Search tools | Query complexity routing |
| **08** | Corrective RAG (CRAG) | ‚≠ê‚≠ê‚≠ê‚≠ê | High-stakes domains (legal, medical) | Relevance grading + web fallback |
| **09** | Self-RAG | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Exploratory research, Dynamic Q&A | Self-critique and refinement |
| **10** | Agentic RAG | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Multi-step reasoning, BI dashboards | Autonomous agents with tools |
| **11** | Comparison | - | Benchmarking | Side-by-side performance analysis |
| **12** ‚ú® | Contextual RAG | ‚≠ê‚≠ê‚≠ê | Technical docs, Code documentation | Context-augmented chunking (Anthropic) |
| **13** ‚ú® | Fusion RAG | ‚≠ê‚≠ê‚≠ê | Research, Best ranking quality | Reciprocal Rank Fusion algorithm |
| **14** ‚ú® | SQL RAG | ‚≠ê‚≠ê‚≠ê‚≠ê | Analytics, BI, Structured data | Natural Language to SQL with safety |
| **15** ‚ú® | GraphRAG | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Knowledge graphs, Relationships | Entity extraction + multi-hop reasoning |
| **16** ‚ú® | RAGAS Evaluation | - | Quality assessment | Comprehensive RAG metrics framework |

---

## Detailed Descriptions

### 04_rag_with_memory.ipynb

#### RAG with Conversational Memory

Extends Simple RAG with conversation history to handle follow-up questions and anaphoric references.

**When to Use:**

- Chatbots and conversational interfaces
- Customer support systems
- Interactive Q&A sessions

**Key Components:**

- `ConversationBufferMemory` or `ConversationBufferWindowMemory`
- `RunnableWithMessageHistory` for LCEL integration
- Modified prompts with `MessagesPlaceholder`

**Example Query Flow:**

```text
User: "What is RAG?"
Bot: "RAG is Retrieval-Augmented Generation..."
User: "What are its main components?" ‚Üê References "RAG" from context
Bot: "The main components of RAG are..." ‚Üê Understands reference
```

Duration: ~10 minutes

---

### 05_branched_rag.ipynb

#### Multi-Query Parallel Retrieval

Generates multiple sub-queries from a single user question and retrieves documents in parallel for better coverage.

**When to Use:**

- Multi-intent queries
- Cross-domain research
- Comprehensive topic exploration

**Key Components:**

- `MultiQueryRetriever` (LangChain built-in)
- Query generation prompts
- Document deduplication

**Example:**

```text
Query: "Compare OpenAI and HuggingFace embeddings for cost and performance"

Generated sub-queries:
1. "OpenAI embeddings pricing and cost"
2. "HuggingFace embeddings performance benchmarks"
3. "Comparison of embedding providers"

‚Üí Retrieves diverse documents covering all aspects
```

Duration: ~8 minutes

---

### 06_hyde.ipynb

#### Hypothetical Document Embeddings

Generates a hypothetical "perfect answer" document, embeds it, and uses it for retrieval instead of the raw query.

**When to Use:**

- Ambiguous or vague queries
- Domain-specific jargon
- Queries with abbreviations or shorthand

**Key Components:**

- HyDe prompt for document generation
- Two-step process: generate ‚Üí embed ‚Üí search
- Semantic similarity improvement

**Example:**

```text
Query: "How does MMR work?"

Hypothetical Doc (generated):
"MMR (Maximal Marginal Relevance) is a retrieval strategy that balances
relevance with diversity. It works by first fetching a larger set of
candidate documents, then iteratively selecting documents that are both
relevant to the query and dissimilar to already selected documents..."

‚Üí Embedding this detailed description finds better matches
```

Duration: ~10 minutes

---

### 07_adaptive_rag.ipynb

#### Query Complexity-Based Routing

Analyzes query complexity and routes to the optimal retrieval strategy (simple, MMR, or HyDe).

**When to Use:**

- Mixed workload systems
- Cost optimization (use simple retrieval when possible)
- Performance/quality balance

**Key Components:**

- LLM-based complexity classifier
- Router logic (SIMPLE ‚Üí similarity, MEDIUM ‚Üí MMR, COMPLEX ‚Üí HyDe)
- Performance monitoring

**Example:**

```text
"What is FAISS?" ‚Üí SIMPLE ‚Üí Fast similarity search
"Compare vector databases" ‚Üí MEDIUM ‚Üí MMR for diversity
"How to architect production RAG with privacy constraints?" ‚Üí COMPLEX ‚Üí HyDe
```

Duration: ~12 minutes

---

### 08_corrective_rag.ipynb

#### CRAG - Relevance Grading with Web Fallback

Grades retrieved documents for relevance and triggers web search if quality is low.

**When to Use:**

- High-accuracy requirements (legal, medical)
- Out-of-domain queries
- Fact-checking applications

**Key Components:**

- Relevance grader (LLM-based)
- DuckDuckGo web search tool
- Quality threshold logic

**Example:**

```text
Query: "What is the latest LangChain version released in 2025?"

Vector DB retrieval ‚Üí Low relevance (outdated docs)
‚Üí Trigger web search ‚Üí Find current information
‚Üí Combine sources ‚Üí High-quality answer
```

Duration: ~15 minutes

---

### 09_self_rag.ipynb

#### Self-Reflective RAG with Auto-Critique

LLM decides autonomously when to retrieve, evaluates its own responses, and retries if quality is low.

**When to Use:**

- Exploratory research
- High-quality requirements
- Systems requiring self-correction

**Key Components:**

- Retrieval need classifier
- Response self-critique
- Iterative refinement loop
- Citation validation

**Example:**

```text
Query: "What is 5 + 7?"

Retrieval need: NO (general knowledge)
‚Üí Direct answer: "12"
‚Üí Self-critique: SCORE 5 ‚Üí Approved

Query: "What are MMR parameters in LangChain?"

Retrieval need: YES (specific info needed)
‚Üí Retrieve docs ‚Üí Generate answer
‚Üí Self-critique: SCORE 3 ‚Üí Retry with more context
‚Üí Improved answer ‚Üí SCORE 5 ‚Üí Approved
```

Duration: ~20 minutes

---

### 10_agentic_rag.ipynb

#### Autonomous Agent with Tools

Combines RAG with ReAct agents that can reason, plan, and use multiple tools (retriever, calculator, web search).

**When to Use:**

- Multi-step reasoning tasks
- BI dashboards and analytics
- Complex decision-making workflows

**Key Components:**

- ReAct agent (Reasoning + Acting)
- Tool suite (retriever, calculator, web search)
- Agent memory for conversation
- LangGraph orchestration

**Example:**

```text
Query: "If I have 10,000 documents and process 1M tokens/day,
        should I use OpenAI or HuggingFace embeddings?"

Agent reasoning:
1. Thought: Need to calculate embedding costs
   Action: Calculator ‚Üí Cost estimation
2. Thought: Need embedding comparison info
   Action: Knowledge Base ‚Üí Retrieve comparison
3. Thought: Analyze privacy/cost trade-offs
   Final Answer: "HuggingFace is better for your use case because..."
```

Duration: ~25 minutes

---

### 11_comparison.ipynb

#### Comprehensive Benchmark

Side-by-side comparison of all 12 architectures across various query types and metrics.

#### Metrics Evaluated

- Response time (latency)
- Token usage (cost)
- Success rate per query type
- Qualitative response quality

#### Query Types Tested

- Simple factual
- Follow-up questions
- Multi-concept queries
- Ambiguous queries
- Out-of-domain queries
- Complex reasoning

Duration: ~30 minutes (runs all architectures)

---

### 12_contextual_rag.ipynb ‚ú®

#### Context-Augmented Chunking (Anthropic Technique)

Enhances document chunks by prepending them with document-level context, improving retrieval precision with minimal query overhead.

**When to Use:**

- Technical documentation
- Code documentation
- Legal/policy documents
- Any domain where chunks need broader context

**Key Components:**

- Document summarization with LLM
- Chunk-specific contextualization
- Context-augmented embeddings
- ~15-30% better retrieval quality

**Example:**

```text
Original chunk: "The function returns a list of tokens."

Contextualized chunk:
"Document: LangChain Text Splitting API
Section: RecursiveCharacterTextSplitter methods
The function returns a list of tokens."

‚Üí Embedding this contextualized version improves semantic matching
```

Duration: ~12 minutes

---

### 13_fusion_rag.ipynb ‚ú®

#### RAG-Fusion with Reciprocal Rank Fusion

Generates multiple query perspectives and combines results using the RRF algorithm for superior ranking quality.

**When to Use:**

- Research and literature review
- Complex multi-aspect queries
- When ranking quality is critical
- Exploratory information gathering

**Key Components:**

- Multi-query generation (3-5 perspectives)
- Parallel retrieval for each query
- Reciprocal Rank Fusion (RRF) algorithm
- De-duplication with score aggregation

**Example:**

```text
Query: "How do I optimize RAG performance?"

Generated queries:
1. "RAG performance optimization techniques"
2. "Reduce latency in retrieval augmented generation"
3. "Improve RAG accuracy and speed"
4. "RAG caching and indexing strategies"

RRF Score Calculation:
For each document: score = sum(1 / (k + rank_i)) across all queries
‚Üí Documents appearing in multiple result sets get higher scores
```

Duration: ~15 minutes

---

### 14_sql_rag.ipynb ‚ú®

#### Natural Language to SQL

Converts natural language questions into SQL queries, executes them safely, and interprets results.

**When to Use:**

- Business intelligence and analytics
- Data exploration tools
- Reporting dashboards
- Any structured database queries

**Key Components:**

- Schema retrieval with semantic search
- Text-to-SQL generation with validation
- Safe SQL execution (read-only, SELECT only)
- SQL error recovery
- Result interpretation with LLM
- Chinook sample database (music store)

**Example:**

Query: "Show me the top 5 customers by total purchase amount"

Pipeline:

1. Retrieve relevant schema (Customer, Invoice, InvoiceLine tables)
2. Generate SQL:

   ```sql
   SELECT c.FirstName, c.LastName, SUM(i.Total) as TotalSpent
   FROM Customer c JOIN Invoice i ON c.CustomerId = i.CustomerId
   GROUP BY c.CustomerId ORDER BY TotalSpent DESC LIMIT 5
   ```

3. Execute safely (read-only connection)
4. Interpret results: "The top customer is Frank Harris who spent $144..."

Duration: ~18 minutes

---

### 15_graphrag.ipynb ‚ú®

#### Graph-Based Knowledge Retrieval (Microsoft Research)

Extracts entities and relationships from documents, constructs a knowledge graph, and performs multi-hop reasoning.

**When to Use:**

- Knowledge graphs and ontologies
- Relationship-centric queries
- Multi-hop reasoning ("friend of a friend")
- Network analysis and community detection
- Exploratory knowledge discovery

**Key Components:**

- Entity extraction with LLM
- Relationship extraction and typing
- NetworkX graph construction
- Graph traversal algorithms
- Community detection (Louvain algorithm)
- Graph visualization

**Example:**

```text
Documents: "Alice works at OpenAI. Bob works at Anthropic. Alice and Bob are friends."

Graph Construction:
Nodes: [Alice, Bob, OpenAI, Anthropic]
Edges: [Alice --WORKS_AT--> OpenAI,
        Bob --WORKS_AT--> Anthropic,
        Alice --FRIEND--> Bob]

Query: "Who are Alice's colleagues' friends?"
Multi-hop: Alice ‚Üí OpenAI ‚Üí [employees] ‚Üí [their friends]
```

Duration: ~25 minutes

---

### 16_evaluation_ragas.ipynb ‚ú®

#### RAGAS Evaluation Framework

Comprehensive quality assessment for RAG systems using 6 evaluation metrics.

**When to Use:**

- Benchmarking multiple architectures
- Quality assurance before production
- A/B testing RAG improvements
- Cost-quality trade-off analysis

#### Key Metrics

1. **Faithfulness**: Answer grounded in context?
2. **Answer Relevancy**: Response addresses the question?
3. **Context Precision**: Relevant chunks ranked high?
4. **Context Recall**: All needed info retrieved?
5. **Answer Similarity**: Semantic match with ground truth?
6. **Answer Correctness**: Factual accuracy score

#### Example Evaluation

```text
Test Dataset:
- Question: "What is RAG?"
- Ground Truth: "RAG is Retrieval-Augmented Generation..."
- Context: [retrieved chunks]
- Answer: [generated response]

Scores:
- Faithfulness: 0.95 (well-grounded)
- Relevancy: 0.92 (on-topic)
- Precision: 0.88 (good retrieval)
- Recall: 0.85 (mostly complete)
- Similarity: 0.90 (semantically close)
- Correctness: 0.93 (factually accurate)
```

Duration: ~20 minutes

---

## Comparison Matrix

| Architecture | Latency | Cost | Accuracy | Complexity | Best For |
|--------------|---------|------|----------|------------|----------|
| Simple RAG | Fast (2s) | Low | Good | ‚≠ê | General purpose |
| Memory RAG | Fast (2-3s) | Low-Med | Good | ‚≠ê‚≠ê | Conversations |
| Branched RAG | Medium (5-8s) | Medium | Very Good | ‚≠ê‚≠ê‚≠ê | Multi-intent |
| HyDe | Medium (4-6s) | Medium | Very Good | ‚≠ê‚≠ê‚≠ê | Ambiguous queries |
| Contextual RAG ‚ú® | Fast (2-3s) | Low | Very Good | ‚≠ê‚≠ê‚≠ê | Technical docs |
| Fusion RAG ‚ú® | Medium (5-8s) | Medium | Excellent | ‚≠ê‚≠ê‚≠ê | Research |
| Adaptive RAG | Variable | Optimized | Very Good | ‚≠ê‚≠ê‚≠ê‚≠ê | Mixed workloads |
| SQL RAG ‚ú® | Fast (2-5s) | Low-Med | Perfect* | ‚≠ê‚≠ê‚≠ê‚≠ê | Analytics |
| CRAG | Slow (10-15s) | High | Excellent | ‚≠ê‚≠ê‚≠ê‚≠ê | High-accuracy |
| Self-RAG | Slow (10-20s) | High | Excellent | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Quality-critical |
| GraphRAG ‚ú® | Medium (3-8s) | High | Excellent** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Knowledge graphs |
| Agentic RAG | Very Slow (20-40s) | Very High | Excellent | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Complex reasoning |

*Perfect for structured data queries | **Excellent for relationship queries

---

## Shared Dependencies

All notebooks reuse components from `fundamentals`:

```python
# Shared utilities
from shared import (
    load_vector_store,          # Load pre-built vector stores
    RAG_PROMPT_TEMPLATE,        # Base prompts
    MEMORY_RAG_PROMPT,          # Memory-specific prompts
    HYDE_PROMPT,                # HyDe prompts
    # ... etc
)

# Shared artifacts
vectorstore_openai = load_vector_store("data/vector_stores/openai_embeddings", embeddings)
```

This avoids redundant embedding computation and ensures consistent baselines.

---

## Installation Notes

Some architectures require additional dependencies:

```bash
# For CRAG (web search)
pip install duckduckgo-search>=4.0.0

# For Agentic RAG (agent orchestration)
pip install langgraph>=0.0.20

# For GraphRAG (graph algorithms) ‚ú®
pip install networkx>=3.2 matplotlib>=3.8.0

# For SQL RAG (database operations) ‚ú®
pip install sqlalchemy>=2.0.25 pandas>=2.2.0

# For RAGAS Evaluation ‚ú®
pip install ragas>=0.1.7 datasets>=2.16.0

# For advanced NLP (entity extraction) ‚ú®
pip install spacy>=3.7.0
# Download spaCy model:
python -m spacy download en_core_web_sm

# Optional: Premium web search for CRAG
pip install tavily-python>=0.3.0
```

These are already included in `requirements.txt`.

---

## Progression Recommendations

**Beginner Path** (Start here):

1. 04_rag_with_memory.ipynb ‚Üê Easiest extension
2. 05_branched_rag.ipynb
3. 06_hyde.ipynb

**Intermediate Path**:

1. 12_contextual_rag.ipynb ‚ú® ‚Üê Context-augmented chunks
2. 13_fusion_rag.ipynb ‚ú® ‚Üê Best ranking quality
3. 07_adaptive_rag.ipynb
4. 08_corrective_rag.ipynb

**Advanced Path**:

1. 14_sql_rag.ipynb ‚ú® ‚Üê Natural language to SQL
2. 09_self_rag.ipynb
3. 10_agentic_rag.ipynb

**Expert Path** ‚ú®:

1. 15_graphrag.ipynb ‚ú® ‚Üê Graph-based reasoning

**Analysis & Evaluation**:

1. 11_comparison.ipynb ‚Üê Benchmark all 12 architectures
2. 16_evaluation_ragas.ipynb ‚ú® ‚Üê Comprehensive quality metrics

---

## Production Considerations

Before deploying any advanced architecture:

1. **Cost Analysis**: Track token usage with `tiktoken`
2. **Latency Monitoring**: Profile each component
3. **Error Handling**: Implement robust fallbacks
4. **Caching**: Cache embeddings and frequent queries
5. **Rate Limiting**: Prevent API overuse
6. **Logging**: Use LangSmith for tracing

See each notebook's "Production Optimizations" section for specific guidance.

---

## üìñ Documentation

For comprehensive guides, see:

- üöÄ **[Getting Started](../../docs/GETTING_STARTED.md)** - Quick start (5 min)
- üèóÔ∏è **[Architecture](../../docs/ARCHITECTURE.md)** - Design decisions
- ‚ö° **[Performance](../../docs/PERFORMANCE.md)** - Benchmarks & optimization
- üöÄ **[Deployment](../../docs/DEPLOYMENT.md)** - Production setup
- üìù **[Examples](../../docs/EXAMPLES.md)** - Usage patterns
- üêõ **[Troubleshooting](../../docs/TROUBLESHOOTING.md)** - Detailed troubleshooting
- ‚ùì **[FAQ](../../docs/FAQ.md)** - Common questions

---

## Resources

### Core RAG

- [LangChain Documentation](https://python.langchain.com/)
- [RAG Paper (Lewis et al.)](https://arxiv.org/abs/2005.11401)

### Advanced Architectures

- [Self-RAG Paper](https://arxiv.org/abs/2310.11511)
- [CRAG Paper](https://arxiv.org/abs/2401.15884)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)

### New Architectures ‚ú®

- [Contextual Retrieval (Anthropic)](https://www.anthropic.com/news/contextual-retrieval) - Context-augmented chunking
- [RAG-Fusion Paper](https://arxiv.org/abs/2402.03367) - Reciprocal Rank Fusion
- [GraphRAG (Microsoft Research)](https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/) - Graph-based RAG
- [RAGAS Framework](https://docs.ragas.io/) - RAG evaluation metrics
- [Text-to-SQL Survey](https://arxiv.org/abs/2208.13629) - Natural language to SQL

---

## Troubleshooting

**Issue**: "Vector store not found"

- Solution: Run `fundamentals/02_embeddings_comparison.ipynb` first

**Issue**: "Module 'shared' not found"

- Solution: Ensure you're in the project root or adjust `sys.path`

**Issue**: "Rate limit exceeded"

- Solution: Add delays between API calls or use batch processing

**Issue**: "DuckDuckGo search fails"

- Solution: Check internet connection or use Tavily as alternative

See main `README.md` for full troubleshooting guide.
