# Advanced RAG Architectures

This directory contains implementations of 8 advanced RAG architectures, each optimized for different use cases and requirements.

## Prerequisites

Before exploring advanced architectures, complete the fundamentals:

1. ‚úì `fundamentals/01_setup_and_basics.ipynb`
2. ‚úì `fundamentals/02_embeddings_comparison.ipynb`
3. ‚úì `fundamentals/03_simple_rag.ipynb`

These provide the baseline components (vector stores, embeddings, retrievers) used by all advanced architectures.

---

## Architecture Overview

| Notebook | Architecture | Complexity | Use Case | Key Feature |
|----------|--------------|------------|----------|-------------|
| **04** | RAG with Memory | ‚≠ê‚≠ê | Conversational AI, Support Bots | Maintains chat history for follow-up questions |
| **05** | Branched RAG | ‚≠ê‚≠ê‚≠ê | Multi-domain search, Analysis | Parallel sub-query generation |
| **06** | HyDe | ‚≠ê‚≠ê‚≠ê | Ambiguous queries, Specialized domains | Hypothetical document generation |
| **07** | Adaptive RAG | ‚≠ê‚≠ê‚≠ê‚≠ê | Mixed workloads, Search tools | Query complexity routing |
| **08** | Corrective RAG (CRAG) | ‚≠ê‚≠ê‚≠ê‚≠ê | High-stakes domains (legal, medical) | Relevance grading + web fallback |
| **09** | Self-RAG | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Exploratory research, Dynamic Q&A | Self-critique and refinement |
| **10** | Agentic RAG | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Multi-step reasoning, BI dashboards | Autonomous agents with tools |
| **11** | Comparison | - | Benchmarking | Side-by-side performance analysis |

---

## Detailed Descriptions

### 04_rag_with_memory.ipynb

**RAG with Conversational Memory**

Extends Simple RAG with conversation history to handle follow-up questions and anaphoric references.

**When to Use:**

- Chatbots and conversational interfaces
- Customer support systems
- Interactive Q&A sessions

**Key Components:**

- `ConversationBufferMemory` or `ConversationBufferWindowMemory`
- `RunnableWithMessageHistory` for LCEL integration
- Modified prompts with `MessagesPlaceholder`

**Example Query Flow:**

```
User: "What is RAG?"
Bot: "RAG is Retrieval-Augmented Generation..."
User: "What are its main components?" ‚Üê References "RAG" from context
Bot: "The main components of RAG are..." ‚Üê Understands reference
```

**Duration:** ~10 minutes

---

### 05_branched_rag.ipynb

**Multi-Query Parallel Retrieval**

Generates multiple sub-queries from a single user question and retrieves documents in parallel for better coverage.

**When to Use:**

- Multi-intent queries
- Cross-domain research
- Comprehensive topic exploration

**Key Components:**

- `MultiQueryRetriever` (LangChain built-in)
- Query generation prompts
- Document deduplication

**Example:**

```
Query: "Compare OpenAI and HuggingFace embeddings for cost and performance"

Generated sub-queries:
1. "OpenAI embeddings pricing and cost"
2. "HuggingFace embeddings performance benchmarks"
3. "Comparison of embedding providers"

‚Üí Retrieves diverse documents covering all aspects
```

**Duration:** ~8 minutes

---

### 06_hyde.ipynb

**Hypothetical Document Embeddings**

Generates a hypothetical "perfect answer" document, embeds it, and uses it for retrieval instead of the raw query.

**When to Use:**

- Ambiguous or vague queries
- Domain-specific jargon
- Queries with abbreviations or shorthand

**Key Components:**

- HyDe prompt for document generation
- Two-step process: generate ‚Üí embed ‚Üí search
- Semantic similarity improvement

**Example:**

```
Query: "How does MMR work?"

Hypothetical Doc (generated):
"MMR (Maximal Marginal Relevance) is a retrieval strategy that balances
relevance with diversity. It works by first fetching a larger set of
candidate documents, then iteratively selecting documents that are both
relevant to the query and dissimilar to already selected documents..."

‚Üí Embedding this detailed description finds better matches
```

**Duration:** ~10 minutes

---

### 07_adaptive_rag.ipynb

**Query Complexity-Based Routing**

Analyzes query complexity and routes to the optimal retrieval strategy (simple, MMR, or HyDe).

**When to Use:**

- Mixed workload systems
- Cost optimization (use simple retrieval when possible)
- Performance/quality balance

**Key Components:**

- LLM-based complexity classifier
- Router logic (SIMPLE ‚Üí similarity, MEDIUM ‚Üí MMR, COMPLEX ‚Üí HyDe)
- Performance monitoring

**Example:**

```
"What is FAISS?" ‚Üí SIMPLE ‚Üí Fast similarity search
"Compare vector databases" ‚Üí MEDIUM ‚Üí MMR for diversity
"How to architect production RAG with privacy constraints?" ‚Üí COMPLEX ‚Üí HyDe
```

**Duration:** ~12 minutes

---

### 08_corrective_rag.ipynb

**CRAG - Relevance Grading with Web Fallback**

Grades retrieved documents for relevance and triggers web search if quality is low.

**When to Use:**

- High-accuracy requirements (legal, medical)
- Out-of-domain queries
- Fact-checking applications

**Key Components:**

- Relevance grader (LLM-based)
- DuckDuckGo web search tool
- Quality threshold logic

**Example:**

```
Query: "What is the latest LangChain version released in 2025?"

Vector DB retrieval ‚Üí Low relevance (outdated docs)
‚Üí Trigger web search ‚Üí Find current information
‚Üí Combine sources ‚Üí High-quality answer
```

**Duration:** ~15 minutes

---

### 09_self_rag.ipynb

**Self-Reflective RAG with Auto-Critique**

LLM decides autonomously when to retrieve, evaluates its own responses, and retries if quality is low.

**When to Use:**

- Exploratory research
- High-quality requirements
- Systems requiring self-correction

**Key Components:**

- Retrieval need classifier
- Response self-critique
- Iterative refinement loop
- Citation validation

**Example:**

```
Query: "What is 5 + 7?"

Retrieval need: NO (general knowledge)
‚Üí Direct answer: "12"
‚Üí Self-critique: SCORE 5 ‚Üí Approved

Query: "What are MMR parameters in LangChain?"

Retrieval need: YES (specific info needed)
‚Üí Retrieve docs ‚Üí Generate answer
‚Üí Self-critique: SCORE 3 ‚Üí Retry with more context
‚Üí Improved answer ‚Üí SCORE 5 ‚Üí Approved
```

**Duration:** ~20 minutes

---

### 10_agentic_rag.ipynb

**Autonomous Agent with Tools**

Combines RAG with ReAct agents that can reason, plan, and use multiple tools (retriever, calculator, web search).

**When to Use:**

- Multi-step reasoning tasks
- BI dashboards and analytics
- Complex decision-making workflows

**Key Components:**

- ReAct agent (Reasoning + Acting)
- Tool suite (retriever, calculator, web search)
- Agent memory for conversation
- LangGraph orchestration

**Example:**

```
Query: "If I have 10,000 documents and process 1M tokens/day,
        should I use OpenAI or HuggingFace embeddings?"

Agent reasoning:
1. Thought: Need to calculate embedding costs
   Action: Calculator ‚Üí Cost estimation
2. Thought: Need embedding comparison info
   Action: Knowledge Base ‚Üí Retrieve comparison
3. Thought: Analyze privacy/cost trade-offs
   Final Answer: "HuggingFace is better for your use case because..."
```

**Duration:** ~25 minutes

---

### 11_comparison.ipynb

**Comprehensive Benchmark**

Side-by-side comparison of all 8 architectures across various query types and metrics.

**Metrics Evaluated:**

- Response time (latency)
- Token usage (cost)
- Success rate per query type
- Qualitative response quality

**Query Types Tested:**

- Simple factual
- Follow-up questions
- Multi-concept queries
- Ambiguous queries
- Out-of-domain queries
- Complex reasoning

**Duration:** ~30 minutes (runs all architectures)

---

## Comparison Matrix

| Architecture | Latency | Cost | Accuracy | Complexity | Best For |
|--------------|---------|------|----------|------------|----------|
| Simple RAG | Fast (2s) | Low | Good | ‚≠ê | General purpose |
| Memory RAG | Fast (2-3s) | Low-Med | Good | ‚≠ê‚≠ê | Conversations |
| Branched RAG | Medium (5-8s) | Medium | Very Good | ‚≠ê‚≠ê‚≠ê | Multi-intent |
| HyDe | Medium (4-6s) | Medium | Very Good | ‚≠ê‚≠ê‚≠ê | Ambiguous queries |
| Adaptive RAG | Variable | Optimized | Very Good | ‚≠ê‚≠ê‚≠ê‚≠ê | Mixed workloads |
| CRAG | Slow (10-15s) | High | Excellent | ‚≠ê‚≠ê‚≠ê‚≠ê | High-accuracy |
| Self-RAG | Slow (10-20s) | High | Excellent | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Quality-critical |
| Agentic RAG | Very Slow (20-40s) | Very High | Excellent | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Complex reasoning |

---

## Shared Dependencies

All notebooks reuse components from `fundamentals`:

```python
# Shared utilities
from shared import (
    load_vector_store,          # Load pre-built vector stores
    RAG_PROMPT_TEMPLATE,        # Base prompts
    MEMORY_RAG_PROMPT,          # Memory-specific prompts
    HYDE_PROMPT,                # HyDe prompts
    # ... etc
)

# Shared artifacts
vectorstore_openai = load_vector_store("data/vector_stores/openai_embeddings", embeddings)
```

This avoids redundant embedding computation and ensures consistent baselines.

---

## Installation Notes

Some architectures require additional dependencies:

```bash
# For CRAG (web search)
pip install duckduckgo-search>=4.0.0

# For Agentic RAG (agent orchestration)
pip install langgraph>=0.0.20

# Optional: Premium web search for CRAG
pip install tavily-python>=0.3.0
```

These are already included in `requirements.txt`.

---

## Progression Recommendations

**Beginner Path** (Start here):

1. 04_rag_with_memory.ipynb ‚Üê Easiest extension
2. 05_branched_rag.ipynb
3. 06_hyde.ipynb

**Intermediate Path**:
4. 07_adaptive_rag.ipynb
5. 08_corrective_rag.ipynb

**Advanced Path**:
6. 09_self_rag.ipynb
7. 10_agentic_rag.ipynb

**Final Step**:
8. 11_comparison.ipynb ‚Üê Benchmark everything

---

## Production Considerations

Before deploying any advanced architecture:

1. **Cost Analysis**: Track token usage with `tiktoken`
2. **Latency Monitoring**: Profile each component
3. **Error Handling**: Implement robust fallbacks
4. **Caching**: Cache embeddings and frequent queries
5. **Rate Limiting**: Prevent API overuse
6. **Logging**: Use LangSmith for tracing

See each notebook's "Production Optimizations" section for specific guidance.

---

## üìñ Documentation

For comprehensive guides, see:

- üöÄ **[Getting Started](../../docs/GETTING_STARTED.md)** - Quick start (5 min)
- üèóÔ∏è **[Architecture](../../docs/ARCHITECTURE.md)** - Design decisions
- ‚ö° **[Performance](../../docs/PERFORMANCE.md)** - Benchmarks & optimization
- üöÄ **[Deployment](../../docs/DEPLOYMENT.md)** - Production setup
- üìù **[Examples](../../docs/EXAMPLES.md)** - Usage patterns
- üêõ **[Troubleshooting](../../docs/TROUBLESHOOTING.md)** - Detailed troubleshooting
- ‚ùì **[FAQ](../../docs/FAQ.md)** - Common questions

---

## Resources

- [LangChain Documentation](https://python.langchain.com/)
- [RAG Paper (Lewis et al.)](https://arxiv.org/abs/2005.11401)
- [Self-RAG Paper](https://arxiv.org/abs/2310.11511)
- [CRAG Paper](https://arxiv.org/abs/2401.15884)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)

---

## Troubleshooting

**Issue**: "Vector store not found"

- **Solution**: Run `fundamentals/02_embeddings_comparison.ipynb` first

**Issue**: "Module 'shared' not found"

- **Solution**: Ensure you're in the project root or adjust `sys.path`

**Issue**: "Rate limit exceeded"

- **Solution**: Add delays between API calls or use batch processing

**Issue**: "DuckDuckGo search fails"

- **Solution**: Check internet connection or use Tavily as alternative

See main `README.md` for full troubleshooting guide.
