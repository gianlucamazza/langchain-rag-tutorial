{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Contextual RAG - Context-Augmented Retrieval\n",
    "\n",
    "**Complexity:** ⭐⭐⭐\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Contextual RAG** is a technique introduced by Anthropic that improves retrieval quality by augmenting each text chunk with contextual information about its role within the larger document.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "In standard RAG, document chunks lose their context:\n",
    "- Chunks are embedded in isolation\n",
    "- References (\"this\", \"these methods\", \"the above\") become ambiguous\n",
    "- Topic boundaries aren't clear\n",
    "- Retrieval may miss relevant chunks due to missing context\n",
    "\n",
    "### The Solution\n",
    "\n",
    "Contextual RAG prepends each chunk with:\n",
    "1. **Document-level summary**: What the overall document is about\n",
    "2. **Chunk-level context**: How this specific chunk relates to the document\n",
    "\n",
    "```\n",
    "Standard chunk: \"The function takes two parameters...\"\n",
    "Contextual chunk: \"[This section describes the authentication module's \n",
    "                   login function in the User Management API.] \n",
    "                   The function takes two parameters...\"\n",
    "```\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "```\n",
    "Documents → Split into chunks → For each chunk:\n",
    "    1. Generate document summary (once per document)\n",
    "    2. Generate chunk context (per chunk)\n",
    "    3. Prepend context to chunk\n",
    "    4. Embed contextual chunk\n",
    "→ Store in vector database → Retrieve → Generate answer\n",
    "```\n",
    "\n",
    "### When to Use\n",
    "\n",
    "✅ **Good for:**\n",
    "- Long documents with many sections\n",
    "- Technical documentation with cross-references\n",
    "- Documents where context matters (legal, medical, academic)\n",
    "- Improving precision on ambiguous queries\n",
    "\n",
    "❌ **Not ideal for:**\n",
    "- Short, self-contained documents\n",
    "- Real-time applications (context generation adds latency)\n",
    "- Cost-sensitive applications (extra LLM calls)\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "**Pros:**\n",
    "- ✅ Better retrieval precision\n",
    "- ✅ Handles ambiguous references\n",
    "- ✅ Maintains document structure awareness\n",
    "- ✅ One-time preprocessing cost\n",
    "\n",
    "**Cons:**\n",
    "- ❌ Higher indexing cost (LLM calls for each chunk)\n",
    "- ❌ Larger embeddings (context + content)\n",
    "- ❌ More complex preprocessing pipeline\n",
    "- ❌ Slower indexing time\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Let's build Contextual RAG step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OpenAI API Key: LOADED\n",
      "  Preview: sk-proj...vIQA\n",
      "✓ All imports successful\n",
      "✓ Using model: gpt-4o-mini\n",
      "✓ Using embeddings: text-embedding-3-small\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(str(Path(\"../..\").resolve()))\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from shared.config import (\n",
    "    verify_api_key,\n",
    "    DEFAULT_MODEL,\n",
    "    DEFAULT_TEMPERATURE,\n",
    "    OPENAI_EMBEDDING_MODEL,\n",
    "    VECTOR_STORE_DIR,\n",
    ")\n",
    "from shared.loaders import load_and_split\n",
    "from shared.prompts import (\n",
    "    DOCUMENT_SUMMARY_PROMPT,\n",
    "    CONTEXTUAL_CHUNK_PROMPT,\n",
    "    CONTEXTUAL_RAG_ANSWER_PROMPT,\n",
    "    RAG_PROMPT_TEMPLATE,\n",
    ")\n",
    "from shared.utils import (\n",
    "    format_docs,\n",
    "    print_section_header,\n",
    "    load_vector_store,\n",
    "    save_vector_store,\n",
    ")\n",
    "\n",
    "# Verify API key\n",
    "verify_api_key()\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "print(f\"✓ Using model: {DEFAULT_MODEL}\")\n",
    "print(f\"✓ Using embeddings: {OPENAI_EMBEDDING_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LOADING DOCUMENTS\n",
      "================================================================================\n",
      "\n",
      "Loading 4 documents from web...\n",
      "  - https://python.langchain.com/docs/use_cases/question_answering/\n",
      "  - https://python.langchain.com/docs/modules/data_connection/retrievers/\n",
      "  - https://python.langchain.com/docs/modules/model_io/llms/\n",
      "  - https://python.langchain.com/docs/use_cases/chatbots/\n",
      "✓ Loaded 4 documents\n",
      "✓ Added custom metadata to all documents\n",
      "Splitting documents...\n",
      "  - Chunk size: 1000\n",
      "  - Chunk overlap: 200\n",
      "✓ Created 122 chunks\n",
      "\n",
      "  Sample chunk:\n",
      "    - Length: 991 chars\n",
      "    - Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "    - Preview: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Rea...\n",
      "\n",
      "✓ Loaded 122 chunks\n",
      "✓ Average chunk size: 749 chars\n",
      "\n",
      "================================================================================\n",
      "Example chunk (standard):\n",
      "================================================================================\n",
      "Expand for full code snippetCopyAsk AIimport bs4\n",
      "from langchain.agents import AgentState, create_agent\n",
      "from langchain_community.document_loaders import WebBaseLoader\n",
      "from langchain.messages import MessageLikeRepresentation\n",
      "from langchain_text_splitters import RecursiveCharacterTextSplitter...\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Loading Documents\")\n",
    "\n",
    "# Load and split documents (returns tuple: original_docs, chunks)\n",
    "_, docs = load_and_split(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(docs)} chunks\")\n",
    "print(f\"✓ Average chunk size: {sum(len(d.page_content) for d in docs) / len(docs):.0f} chars\")\n",
    "\n",
    "# Show example chunk\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Example chunk (standard):\")\n",
    "print(\"=\" * 80)\n",
    "print(docs[5].page_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Group Chunks by Document\n",
    "\n",
    "We need to group chunks by their source document to generate document-level summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GROUPING CHUNKS BY DOCUMENT\n",
      "================================================================================\n",
      "\n",
      "\n",
      "✓ Found 4 unique source documents\n",
      "\n",
      "Chunks per source:\n",
      "  • https://python.langchain.com/docs/use_cases/question_answering/: 32 chunks\n",
      "  • https://python.langchain.com/docs/modules/data_connection/retrievers/: 6 chunks\n",
      "  • https://python.langchain.com/docs/modules/model_io/llms/: 52 chunks\n",
      "  • https://python.langchain.com/docs/use_cases/chatbots/: 32 chunks\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "print_section_header(\"Grouping Chunks by Document\")\n",
    "\n",
    "# Group chunks by source document\n",
    "docs_by_source = defaultdict(list)\n",
    "for doc in docs:\n",
    "    source = doc.metadata.get(\"source\", \"unknown\")\n",
    "    docs_by_source[source].append(doc)\n",
    "\n",
    "print(f\"\\n✓ Found {len(docs_by_source)} unique source documents\")\n",
    "print(\"\\nChunks per source:\")\n",
    "for source, chunks in list(docs_by_source.items())[:5]:\n",
    "    print(f\"  • {source}: {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Document Summaries\n",
    "\n",
    "For each source document, we'll generate a summary that captures its main purpose and topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATING DOCUMENT SUMMARIES\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Generating summaries...\n",
      "  [1/4] https://python.langchain.com/docs/use_cases/questi...\n",
      "  [2/4] https://python.langchain.com/docs/modules/data_con...\n",
      "  [3/4] https://python.langchain.com/docs/modules/model_io...\n",
      "  [4/4] https://python.langchain.com/docs/use_cases/chatbo...\n",
      "\n",
      "✓ Generated 4 document summaries\n",
      "\n",
      "================================================================================\n",
      "Example summary for: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "================================================================================\n",
      "The document provides a tutorial on building a Retrieval Augmented Generation (RAG) agent using LangChain, focusing on creating a sophisticated question-answering chatbot that can respond to queries based on specific source information. It outlines the necessary components, including indexing, document loading, and retrieval processes, to facilitate the development of a Q&A application over unstructured text data. The tutorial aims to guide users through the setup and implementation of RAG agents within the LangChain framework.\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Generating Document Summaries\")\n",
    "\n",
    "# Initialize LLM for summarization\n",
    "llm = ChatOpenAI(\n",
    "    model=DEFAULT_MODEL,\n",
    "    temperature=DEFAULT_TEMPERATURE,\n",
    ")\n",
    "\n",
    "# Create summarization chain\n",
    "summary_chain = DOCUMENT_SUMMARY_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "# Generate summaries for each source document\n",
    "doc_summaries = {}\n",
    "print(\"\\nGenerating summaries...\")\n",
    "\n",
    "for i, (source, chunks) in enumerate(docs_by_source.items(), 1):\n",
    "    # Combine first few chunks to represent the document\n",
    "    # (using all chunks would be too long and expensive)\n",
    "    doc_text = \"\\n\\n\".join([chunk.page_content for chunk in chunks[:3]])\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = summary_chain.invoke({\"document\": doc_text})\n",
    "    doc_summaries[source] = summary\n",
    "    \n",
    "    print(f\"  [{i}/{len(docs_by_source)}] {source[:50]}...\")\n",
    "    \n",
    "    # Rate limiting to avoid API throttling\n",
    "    if i < len(docs_by_source):\n",
    "        time.sleep(0.5)\n",
    "\n",
    "print(f\"\\n✓ Generated {len(doc_summaries)} document summaries\")\n",
    "\n",
    "# Show example summary\n",
    "example_source = list(doc_summaries.keys())[0]\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Example summary for: {example_source}\")\n",
    "print(\"=\" * 80)\n",
    "print(doc_summaries[example_source])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Contextual Chunks\n",
    "\n",
    "Now we'll augment each chunk with contextual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATING CONTEXTUAL CHUNKS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Processing 122 chunks...\n",
      "(This may take a few minutes)\n",
      "\n",
      "  Processed 10/122 chunks...\n",
      "  Processed 20/122 chunks...\n",
      "  Processed 30/122 chunks...\n",
      "  Processed 40/122 chunks...\n",
      "  Processed 50/122 chunks...\n",
      "  Processed 60/122 chunks...\n",
      "  Processed 70/122 chunks...\n",
      "  Processed 80/122 chunks...\n",
      "  Processed 90/122 chunks...\n",
      "  Processed 100/122 chunks...\n",
      "  Processed 110/122 chunks...\n",
      "  Processed 120/122 chunks...\n",
      "\n",
      "✓ Generated 122 contextual chunks\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Generating Contextual Chunks\")\n",
    "\n",
    "# Create contextualization chain\n",
    "context_chain = CONTEXTUAL_CHUNK_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "# Generate contextual chunks\n",
    "contextual_docs = []\n",
    "print(f\"\\nProcessing {len(docs)} chunks...\")\n",
    "print(\"(This may take a few minutes)\\n\")\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    source = doc.metadata.get(\"source\", \"unknown\")\n",
    "    doc_summary = doc_summaries.get(source, \"No summary available\")\n",
    "    \n",
    "    # Generate contextual description\n",
    "    context = context_chain.invoke({\n",
    "        \"doc_summary\": doc_summary,\n",
    "        \"chunk\": doc.page_content[:500],  # Limit chunk size for context generation\n",
    "    })\n",
    "    \n",
    "    # Create new document with context prepended\n",
    "    contextual_content = f\"[Context: {context}]\\n\\n{doc.page_content}\"\n",
    "    \n",
    "    contextual_doc = Document(\n",
    "        page_content=contextual_content,\n",
    "        metadata={\n",
    "            **doc.metadata,\n",
    "            \"context\": context,\n",
    "            \"original_content\": doc.page_content,\n",
    "        },\n",
    "    )\n",
    "    contextual_docs.append(contextual_doc)\n",
    "    \n",
    "    # Progress indicator\n",
    "    if i % 10 == 0:\n",
    "        print(f\"  Processed {i}/{len(docs)} chunks...\")\n",
    "    \n",
    "    # Rate limiting\n",
    "    if i < len(docs):\n",
    "        time.sleep(0.3)\n",
    "\n",
    "print(f\"\\n✓ Generated {len(contextual_docs)} contextual chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Standard vs Contextual Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARING STANDARD VS CONTEXTUAL CHUNKS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STANDARD CHUNK:\n",
      "================================================================================\n",
      "Expand for full code snippetCopyAsk AIimport bs4\n",
      "from langchain.agents import AgentState, create_agent\n",
      "from langchain_community.document_loaders import WebBaseLoader\n",
      "from langchain.messages import MessageLikeRepresentation\n",
      "from langchain_text_splitters import RecursiveCharacterTextSplitter...\n",
      "\n",
      "================================================================================\n",
      "CONTEXTUAL CHUNK:\n",
      "================================================================================\n",
      "[Context: This chunk contains code snippets that are essential for implementing the components of a Retrieval Augmented Generation (RAG) agent, specifically focusing on document loading and message handling within the LangChain framework. It supports the tutorial's objective of guiding users through the technical setup required for building a question-answering chatbot.]\n",
      "\n",
      "Expand for full code snippetCopyAsk AIimport bs4\n",
      "from langchain.agents import AgentState, create_agent\n",
      "from langchain_communi...\n",
      "\n",
      "================================================================================\n",
      "STATISTICS:\n",
      "================================================================================\n",
      "Average context length: 362 characters\n",
      "Context overhead: 36.2% (of 1000 char chunks)\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Comparing Standard vs Contextual Chunks\")\n",
    "\n",
    "# Show example comparison\n",
    "example_idx = 5\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STANDARD CHUNK:\")\n",
    "print(\"=\" * 80)\n",
    "print(docs[example_idx].page_content[:400] + \"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONTEXTUAL CHUNK:\")\n",
    "print(\"=\" * 80)\n",
    "print(contextual_docs[example_idx].page_content[:500] + \"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STATISTICS:\")\n",
    "print(\"=\" * 80)\n",
    "avg_context_len = sum(\n",
    "    len(doc.metadata.get(\"context\", \"\")) for doc in contextual_docs\n",
    ") / len(contextual_docs)\n",
    "print(f\"Average context length: {avg_context_len:.0f} characters\")\n",
    "print(f\"Context overhead: {avg_context_len / 1000 * 100:.1f}% (of 1000 char chunks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Vector Stores\n",
    "\n",
    "We'll create two vector stores to compare:\n",
    "1. Standard RAG (no context)\n",
    "2. Contextual RAG (with context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CREATING VECTOR STORES\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Checking for existing vector stores...\n",
      "✗ Error loading vector store from /Users/gianlucamazza/Workspace/notebooks/llm_rag/data/vector_stores/contextual_rag_standard: Error in faiss::FileIOReader::FileIOReader(const char *) at /Users/runner/work/faiss-wheels/faiss-wheels/third-party/faiss/faiss/impl/io.cpp:70: Error: 'f' failed: could not open /Users/gianlucamazza/Workspace/notebooks/llm_rag/data/vector_stores/contextual_rag_standard/index.faiss for reading: No such file or directory\n",
      "\n",
      "Creating standard vector store...\n",
      "✓ Saved vector store to /Users/gianlucamazza/Workspace/notebooks/llm_rag/data/vector_stores/contextual_rag_standard\n",
      "✓ Standard vector store created and saved\n",
      "✗ Error loading vector store from /Users/gianlucamazza/Workspace/notebooks/llm_rag/data/vector_stores/contextual_rag_contextual: Error in faiss::FileIOReader::FileIOReader(const char *) at /Users/runner/work/faiss-wheels/faiss-wheels/third-party/faiss/faiss/impl/io.cpp:70: Error: 'f' failed: could not open /Users/gianlucamazza/Workspace/notebooks/llm_rag/data/vector_stores/contextual_rag_contextual/index.faiss for reading: No such file or directory\n",
      "\n",
      "Creating contextual vector store...\n",
      "✓ Saved vector store to /Users/gianlucamazza/Workspace/notebooks/llm_rag/data/vector_stores/contextual_rag_contextual\n",
      "✓ Contextual vector store created and saved\n",
      "\n",
      "✓ Both vector stores ready\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print_section_header(\"Creating Vector Stores\")\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(model=OPENAI_EMBEDDING_MODEL)\n",
    "\n",
    "# Try to load existing vector stores\n",
    "standard_store_path = VECTOR_STORE_DIR / \"contextual_rag_standard\"\n",
    "contextual_store_path = VECTOR_STORE_DIR / \"contextual_rag_contextual\"\n",
    "\n",
    "print(\"\\nChecking for existing vector stores...\")\n",
    "\n",
    "# Standard vector store\n",
    "vectorstore_standard = load_vector_store(\n",
    "    standard_store_path,\n",
    "    embeddings,\n",
    ")\n",
    "\n",
    "if vectorstore_standard is None:\n",
    "    print(\"\\nCreating standard vector store...\")\n",
    "    vectorstore_standard = FAISS.from_documents(docs, embeddings)\n",
    "    save_vector_store(vectorstore_standard, standard_store_path)\n",
    "    print(\"✓ Standard vector store created and saved\")\n",
    "else:\n",
    "    print(\"✓ Loaded existing standard vector store\")\n",
    "\n",
    "# Contextual vector store\n",
    "vectorstore_contextual = load_vector_store(\n",
    "    contextual_store_path,\n",
    "    embeddings,\n",
    ")\n",
    "\n",
    "if vectorstore_contextual is None:\n",
    "    print(\"\\nCreating contextual vector store...\")\n",
    "    vectorstore_contextual = FAISS.from_documents(contextual_docs, embeddings)\n",
    "    save_vector_store(vectorstore_contextual, contextual_store_path)\n",
    "    print(\"✓ Contextual vector store created and saved\")\n",
    "else:\n",
    "    print(\"✓ Loaded existing contextual vector store\")\n",
    "\n",
    "print(\"\\n✓ Both vector stores ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Build RAG Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BUILDING RAG CHAINS\n",
      "================================================================================\n",
      "\n",
      "✓ Standard RAG chain created\n",
      "✓ Contextual RAG chain created\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Building RAG Chains\")\n",
    "\n",
    "# Create retrievers\n",
    "retriever_standard = vectorstore_standard.as_retriever(\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "retriever_contextual = vectorstore_contextual.as_retriever(\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "\n",
    "# Standard RAG chain\n",
    "chain_standard = (\n",
    "    {\"context\": retriever_standard | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | RAG_PROMPT_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Contextual RAG chain\n",
    "chain_contextual = (\n",
    "    {\"context\": retriever_contextual | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | CONTEXTUAL_RAG_ANSWER_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✓ Standard RAG chain created\")\n",
    "print(\"✓ Contextual RAG chain created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test and Compare\n",
    "\n",
    "Let's test both approaches with queries that benefit from context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TESTING QUERIES\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Query 1: How do I use LCEL to build chains?\n",
      "================================================================================\n",
      "\n",
      "[STANDARD RAG]\n",
      "--------------------------------------------------------------------------------\n",
      "The context provided does not contain any information about LCEL or how to use it to build chains. Therefore, I cannot answer your question based on the available information.\n",
      "\n",
      "⏱️  Time: 1.54s\n",
      "\n",
      "[CONTEXTUAL RAG]\n",
      "--------------------------------------------------------------------------------\n",
      "The provided context does not contain specific information about using LCEL (LangChain Expression Language) to build chains. Therefore, I cannot provide an answer to your question based on the available information.\n",
      "\n",
      "⏱️  Time: 2.16s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "COMPARISON:\n",
      "  • Latency difference: 0.61s\n",
      "  • Response length difference: 40 chars\n",
      "\n",
      "================================================================================\n",
      "Query 2: What are the different types of memory in LangChain?\n",
      "================================================================================\n",
      "\n",
      "[STANDARD RAG]\n",
      "--------------------------------------------------------------------------------\n",
      "In LangChain, there are two types of memory mentioned: short-term memory and long-term memory. \n",
      "\n",
      "- **Short-term memory** is used to support multi-turn interactions, allowing the agent to remember context during a conversation.\n",
      "- **Long-term memory** is designed to support memory across conversational threads, enabling the agent to retain information over a longer period.\n",
      "\n",
      "This information is derived from the context where it states, \"Add conversational memory to support multi-turn interactions\" and \"Add long-term memory to support memory across conversational threads.\"\n",
      "\n",
      "⏱️  Time: 2.24s\n",
      "\n",
      "[CONTEXTUAL RAG]\n",
      "--------------------------------------------------------------------------------\n",
      "The context provided does not specify the different types of memory in LangChain. Therefore, I cannot provide an answer to your question based on the available information.\n",
      "\n",
      "⏱️  Time: 1.53s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "COMPARISON:\n",
      "  • Latency difference: 0.70s\n",
      "  • Response length difference: -403 chars\n",
      "\n",
      "================================================================================\n",
      "Query 3: Explain the role of retrievers in RAG applications\n",
      "================================================================================\n",
      "\n",
      "[STANDARD RAG]\n",
      "--------------------------------------------------------------------------------\n",
      "In RAG (Retrieval-Augmented Generation) applications, retrievers play a crucial role in the process of retrieving relevant information based on user queries. Specifically, the retriever is responsible for the following:\n",
      "\n",
      "1. **Retrieval of Relevant Data**: Given a user input, the retriever searches through a storage system (often a vector store) to find and retrieve relevant splits or documents that pertain to the query. This is the first step in the RAG process, where the system identifies the most pertinent information to assist in generating a response.\n",
      "\n",
      "2. **Integration with Generation**: After the relevant data is retrieved, it is then passed to a model (usually a language model) along with the original user question. This integration allows the model to generate a more informed and contextually relevant answer based on both the question and the retrieved data.\n",
      "\n",
      "This process is highlighted in the context where it states: \"Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever.\" This emphasizes the retriever's function in the overall RAG framework, which is essential for effective information retrieval and generation.\n",
      "\n",
      "⏱️  Time: 4.48s\n",
      "\n",
      "[CONTEXTUAL RAG]\n",
      "--------------------------------------------------------------------------------\n",
      "In Retrieval Augmented Generation (RAG) applications, retrievers play a crucial role in the information retrieval process. Their primary function is to search for and retrieve relevant documents or data based on user input or queries. Here’s how they fit into the RAG framework:\n",
      "\n",
      "1. **Retrieval Process**: When a user submits a question, the retriever accesses a storage system (such as a vector store) to find relevant splits or pieces of information that pertain to the query.\n",
      "\n",
      "2. **Data Relevance**: The retriever is responsible for ensuring that the information retrieved is pertinent to the user's question, which is essential for generating accurate and contextually appropriate answers.\n",
      "\n",
      "3. **Integration with Generation**: After retrieving the relevant data, the retriever passes this information along with the original user question to a language model (LLM). The LLM then generates a response based on both the question and the retrieved data.\n",
      "\n",
      "Overall, retrievers are integral to the RAG process as they bridge the gap between user queries and the generation of informative answers by ensuring that the model has access to the most relevant information.\n",
      "\n",
      "⏱️  Time: 4.35s\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "COMPARISON:\n",
      "  • Latency difference: 0.14s\n",
      "  • Response length difference: -9 chars\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Testing Queries\")\n",
    "\n",
    "# Test queries that benefit from context\n",
    "test_queries = [\n",
    "    \"How do I use LCEL to build chains?\",\n",
    "    \"What are the different types of memory in LangChain?\",\n",
    "    \"Explain the role of retrievers in RAG applications\",\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Standard RAG\n",
    "    print(\"\\n[STANDARD RAG]\")\n",
    "    print(\"-\" * 80)\n",
    "    start_time = time.time()\n",
    "    response_standard = chain_standard.invoke(query)\n",
    "    time_standard = time.time() - start_time\n",
    "    print(response_standard)\n",
    "    print(f\"\\n⏱️  Time: {time_standard:.2f}s\")\n",
    "    \n",
    "    # Contextual RAG\n",
    "    print(\"\\n[CONTEXTUAL RAG]\")\n",
    "    print(\"-\" * 80)\n",
    "    start_time = time.time()\n",
    "    response_contextual = chain_contextual.invoke(query)\n",
    "    time_contextual = time.time() - start_time\n",
    "    print(response_contextual)\n",
    "    print(f\"\\n⏱️  Time: {time_contextual:.2f}s\")\n",
    "    \n",
    "    # Comparison\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"COMPARISON:\")\n",
    "    print(f\"  • Latency difference: {abs(time_contextual - time_standard):.2f}s\")\n",
    "    print(f\"  • Response length difference: {len(response_contextual) - len(response_standard)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Retrieval Quality Comparison\n",
    "\n",
    "Let's examine what documents each approach retrieves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "RETRIEVAL QUALITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Query: What are the different types of memory in LangChain?\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[STANDARD RETRIEVAL]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 1:\n",
      "Source: https://python.langchain.com/docs/modules/data_connection/re\n",
      "Preview: LangChain overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch......\n",
      "\n",
      "Document 2:\n",
      "Source: https://python.langchain.com/docs/modules/data_connection/re\n",
      "Preview: LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and more. LangChain provides a pre-bu...\n",
      "\n",
      "Document 3:\n",
      "Source: https://python.langchain.com/docs/modules/data_connection/re\n",
      "Preview: ​ Core benefits\n",
      "Standard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that...\n",
      "\n",
      "Document 4:\n",
      "Source: https://python.langchain.com/docs/use_cases/question_answeri\n",
      "Preview: ​Next steps\n",
      "Now that we’ve implemented a simple RAG application via create_agent, we can easily incorporate new features and go deeper:\n",
      "\n",
      "Stream tokens and other information for responsive user experie...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "[CONTEXTUAL RETRIEVAL]\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Document 1:\n",
      "Source: https://python.langchain.com/docs/modules/data_connection/re\n",
      "Context: This chunk serves as an introduction to the LangChain documentation, emphasizing the platform's recent funding and its focus on agent engineering, whi...\n",
      "Preview: LangChain overview - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch......\n",
      "\n",
      "Document 2:\n",
      "Source: https://python.langchain.com/docs/modules/data_connection/re\n",
      "Context: This chunk outlines the core benefits of LangChain, emphasizing its standardized model interface and the ease of use of its agent abstraction. These f...\n",
      "Preview: ​ Core benefits\n",
      "Standard model interfaceDifferent providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that...\n",
      "\n",
      "Document 3:\n",
      "Source: https://python.langchain.com/docs/modules/model_io/llms/\n",
      "Context: This chunk provides a navigational overview of the LangChain documentation, highlighting the platform's recent funding and its focus on core component...\n",
      "Preview: Models - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + LangGraphSearch...⌘KGitHubTry ...\n",
      "\n",
      "Document 4:\n",
      "Source: https://python.langchain.com/docs/use_cases/chatbots/\n",
      "Context: This chunk serves as an introduction to the LangChain documentation, specifically focusing on building a Retrieval Augmented Generation (RAG) agent. I...\n",
      "Preview: Build a RAG agent with LangChain - Docs by LangChainSkip to main contentWe've raised a $125M Series B to build the platform for agent engineering. Read more.Docs by LangChain home pageLangChain + Lang...\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Retrieval Quality Analysis\")\n",
    "\n",
    "test_query = \"What are the different types of memory in LangChain?\"\n",
    "\n",
    "print(f\"\\nQuery: {test_query}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Standard retrieval\n",
    "docs_standard = retriever_standard.invoke(test_query)\n",
    "print(\"\\n[STANDARD RETRIEVAL]\")\n",
    "print(\"-\" * 80)\n",
    "for i, doc in enumerate(docs_standard, 1):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'unknown')[:60]}\")\n",
    "    print(f\"Preview: {doc.page_content[:200]}...\")\n",
    "\n",
    "# Contextual retrieval\n",
    "docs_contextual = retriever_contextual.invoke(test_query)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n[CONTEXTUAL RETRIEVAL]\")\n",
    "print(\"-\" * 80)\n",
    "for i, doc in enumerate(docs_contextual, 1):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'unknown')[:60]}\")\n",
    "    print(f\"Context: {doc.metadata.get('context', 'N/A')[:150]}...\")\n",
    "    print(f\"Preview: {doc.metadata.get('original_content', doc.page_content)[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PERFORMANCE METRICS\n",
      "================================================================================\n",
      "\n",
      "\n",
      "INDEXING COSTS:\n",
      "--------------------------------------------------------------------------------\n",
      "Documents processed: 122\n",
      "Document summaries generated: 4\n",
      "Chunk contexts generated: 122\n",
      "Total LLM calls for contextualization: 126\n",
      "\n",
      "Estimated additional indexing time: ~1.1 minutes\n",
      "\n",
      "STORAGE OVERHEAD:\n",
      "--------------------------------------------------------------------------------\n",
      "Average standard chunk: 749 chars\n",
      "Average contextual chunk: 1124 chars\n",
      "Overhead: 50.0%\n",
      "\n",
      "QUERY COSTS:\n",
      "--------------------------------------------------------------------------------\n",
      "Standard RAG: k retrievals + 1 generation\n",
      "Contextual RAG: k retrievals + 1 generation (same as standard)\n",
      "\n",
      "✓ No additional query-time cost!\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Performance Metrics\")\n",
    "\n",
    "# Indexing costs\n",
    "num_documents = len(docs)\n",
    "num_summaries = len(doc_summaries)\n",
    "num_context_calls = len(contextual_docs)\n",
    "\n",
    "print(\"\\nINDEXING COSTS:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Documents processed: {num_documents}\")\n",
    "print(f\"Document summaries generated: {num_summaries}\")\n",
    "print(f\"Chunk contexts generated: {num_context_calls}\")\n",
    "print(f\"Total LLM calls for contextualization: {num_summaries + num_context_calls}\")\n",
    "print(f\"\\nEstimated additional indexing time: ~{(num_summaries + num_context_calls) * 0.5 / 60:.1f} minutes\")\n",
    "\n",
    "# Storage costs\n",
    "avg_standard_len = sum(len(doc.page_content) for doc in docs) / len(docs)\n",
    "avg_contextual_len = sum(len(doc.page_content) for doc in contextual_docs) / len(contextual_docs)\n",
    "overhead = (avg_contextual_len - avg_standard_len) / avg_standard_len * 100\n",
    "\n",
    "print(\"\\nSTORAGE OVERHEAD:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Average standard chunk: {avg_standard_len:.0f} chars\")\n",
    "print(f\"Average contextual chunk: {avg_contextual_len:.0f} chars\")\n",
    "print(f\"Overhead: {overhead:.1f}%\")\n",
    "\n",
    "# Query costs\n",
    "print(\"\\nQUERY COSTS:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Standard RAG: k retrievals + 1 generation\")\n",
    "print(\"Contextual RAG: k retrievals + 1 generation (same as standard)\")\n",
    "print(\"\\n✓ No additional query-time cost!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Takeaways\n",
    "\n",
    "### Summary\n",
    "\n",
    "**Contextual RAG** improves retrieval quality by augmenting chunks with contextual information:\n",
    "- Document-level summaries provide high-level context\n",
    "- Chunk-level descriptions clarify the role of each chunk\n",
    "- Better handling of ambiguous references and cross-references\n",
    "\n",
    "### Cost-Benefit Analysis\n",
    "\n",
    "| Aspect | Impact | Notes |\n",
    "|--------|--------|-------|\n",
    "| **Indexing Time** | ❌ +50-100% | One-time cost |\n",
    "| **Indexing Cost** | ❌ +$X | LLM calls for each chunk |\n",
    "| **Storage** | ❌ +15-30% | Larger embeddings |\n",
    "| **Query Time** | ✅ Same | No runtime overhead |\n",
    "| **Query Cost** | ✅ Same | No additional calls |\n",
    "| **Retrieval Quality** | ✅ Better | Improved precision |\n",
    "| **Answer Quality** | ✅ Better | More context-aware |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Use for long documents**: Most beneficial when documents are long and complex\n",
    "2. **Batch processing**: Generate contexts in batches to reduce costs\n",
    "3. **Cache summaries**: Store document summaries to avoid regeneration\n",
    "4. **Balance context length**: Keep contexts concise (1-2 sentences)\n",
    "5. **Quality check**: Manually review a sample of generated contexts\n",
    "\n",
    "### When to Use\n",
    "\n",
    "Choose **Contextual RAG** when:\n",
    "- ✅ Document quality matters more than cost\n",
    "- ✅ Dealing with technical or complex documents\n",
    "- ✅ Users ask ambiguous or context-dependent questions\n",
    "- ✅ One-time indexing cost is acceptable\n",
    "\n",
    "Stick with **Standard RAG** when:\n",
    "- ✅ Documents are short and self-contained\n",
    "- ✅ Cost optimization is critical\n",
    "- ✅ Real-time indexing is required\n",
    "- ✅ Simpler implementation is preferred\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Combine with other techniques**: Contextual chunks work well with re-ranking\n",
    "- **Experiment with context generation**: Try different prompt strategies\n",
    "- **Measure impact**: Use RAGAS metrics to quantify improvement\n",
    "- **Optimize costs**: Use cheaper models for context generation\n",
    "\n",
    "---\n",
    "\n",
    "**Complexity Rating:** ⭐⭐⭐ (Medium - straightforward concept, some implementation overhead)\n",
    "\n",
    "**Production Readiness:** ⭐⭐⭐⭐ (High - proven technique, minor trade-offs)\n",
    "\n",
    "Continue to **13_fusion_rag.ipynb** for RAG-Fusion with Reciprocal Rank Fusion!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
