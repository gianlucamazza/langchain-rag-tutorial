{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Contextual RAG - Context-Augmented Retrieval\n",
    "\n",
    "**Complexity:** ⭐⭐⭐\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Contextual RAG** is a technique introduced by Anthropic that improves retrieval quality by augmenting each text chunk with contextual information about its role within the larger document.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "In standard RAG, document chunks lose their context:\n",
    "- Chunks are embedded in isolation\n",
    "- References (\"this\", \"these methods\", \"the above\") become ambiguous\n",
    "- Topic boundaries aren't clear\n",
    "- Retrieval may miss relevant chunks due to missing context\n",
    "\n",
    "### The Solution\n",
    "\n",
    "Contextual RAG prepends each chunk with:\n",
    "1. **Document-level summary**: What the overall document is about\n",
    "2. **Chunk-level context**: How this specific chunk relates to the document\n",
    "\n",
    "```\n",
    "Standard chunk: \"The function takes two parameters...\"\n",
    "Contextual chunk: \"[This section describes the authentication module's \n",
    "                   login function in the User Management API.] \n",
    "                   The function takes two parameters...\"\n",
    "```\n",
    "\n",
    "### Pipeline\n",
    "\n",
    "```\n",
    "Documents → Split into chunks → For each chunk:\n",
    "    1. Generate document summary (once per document)\n",
    "    2. Generate chunk context (per chunk)\n",
    "    3. Prepend context to chunk\n",
    "    4. Embed contextual chunk\n",
    "→ Store in vector database → Retrieve → Generate answer\n",
    "```\n",
    "\n",
    "### When to Use\n",
    "\n",
    "✅ **Good for:**\n",
    "- Long documents with many sections\n",
    "- Technical documentation with cross-references\n",
    "- Documents where context matters (legal, medical, academic)\n",
    "- Improving precision on ambiguous queries\n",
    "\n",
    "❌ **Not ideal for:**\n",
    "- Short, self-contained documents\n",
    "- Real-time applications (context generation adds latency)\n",
    "- Cost-sensitive applications (extra LLM calls)\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "**Pros:**\n",
    "- ✅ Better retrieval precision\n",
    "- ✅ Handles ambiguous references\n",
    "- ✅ Maintains document structure awareness\n",
    "- ✅ One-time preprocessing cost\n",
    "\n",
    "**Cons:**\n",
    "- ❌ Higher indexing cost (LLM calls for each chunk)\n",
    "- ❌ Larger embeddings (context + content)\n",
    "- ❌ More complex preprocessing pipeline\n",
    "- ❌ Slower indexing time\n",
    "\n",
    "---\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Let's build Contextual RAG step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OpenAI API Key: LOADED\n",
      "  Preview: sk-proj...vIQA\n",
      "✓ All imports successful\n",
      "✓ Using model: gpt-4o-mini\n",
      "✓ Using embeddings: text-embedding-3-small\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Add parent directory to path for imports\n",
    "sys.path.append(str(Path(\"../..\").resolve()))\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from shared.config import (\n",
    "    verify_api_key,\n",
    "    DEFAULT_MODEL,\n",
    "    DEFAULT_TEMPERATURE,\n",
    "    OPENAI_EMBEDDING_MODEL,\n",
    "    VECTOR_STORE_DIR,\n",
    ")\n",
    "from shared.loaders import load_and_split\n",
    "from shared.prompts import (\n",
    "    DOCUMENT_SUMMARY_PROMPT,\n",
    "    CONTEXTUAL_CHUNK_PROMPT,\n",
    "    CONTEXTUAL_RAG_ANSWER_PROMPT,\n",
    "    RAG_PROMPT_TEMPLATE,\n",
    ")\n",
    "from shared.utils import (\n",
    "    format_docs,\n",
    "    print_section_header,\n",
    "    load_vector_store,\n",
    "    save_vector_store,\n",
    ")\n",
    "\n",
    "# Verify API key\n",
    "verify_api_key()\n",
    "\n",
    "print(\"✓ All imports successful\")\n",
    "print(f\"✓ Using model: {DEFAULT_MODEL}\")\n",
    "print(f\"✓ Using embeddings: {OPENAI_EMBEDDING_MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print_section_header(\"Loading Documents\")\n\n# Load and split documents (returns tuple: original_docs, chunks)\n_, docs = load_and_split(\n    chunk_size=1000,\n    chunk_overlap=200,\n)\n\nprint(f\"\\n✓ Loaded {len(docs)} chunks\")\nprint(f\"✓ Average chunk size: {sum(len(d.page_content) for d in docs) / len(docs):.0f} chars\")\n\n# Show example chunk\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Example chunk (standard):\")\nprint(\"=\" * 80)\nprint(docs[5].page_content[:300] + \"...\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Group Chunks by Document\n",
    "\n",
    "We need to group chunks by their source document to generate document-level summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "print_section_header(\"Grouping Chunks by Document\")\n",
    "\n",
    "# Group chunks by source document\n",
    "docs_by_source = defaultdict(list)\n",
    "for doc in docs:\n",
    "    source = doc.metadata.get(\"source\", \"unknown\")\n",
    "    docs_by_source[source].append(doc)\n",
    "\n",
    "print(f\"\\n✓ Found {len(docs_by_source)} unique source documents\")\n",
    "print(\"\\nChunks per source:\")\n",
    "for source, chunks in list(docs_by_source.items())[:5]:\n",
    "    print(f\"  • {source}: {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Document Summaries\n",
    "\n",
    "For each source document, we'll generate a summary that captures its main purpose and topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Generating Document Summaries\")\n",
    "\n",
    "# Initialize LLM for summarization\n",
    "llm = ChatOpenAI(\n",
    "    model=DEFAULT_MODEL,\n",
    "    temperature=DEFAULT_TEMPERATURE,\n",
    ")\n",
    "\n",
    "# Create summarization chain\n",
    "summary_chain = DOCUMENT_SUMMARY_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "# Generate summaries for each source document\n",
    "doc_summaries = {}\n",
    "print(\"\\nGenerating summaries...\")\n",
    "\n",
    "for i, (source, chunks) in enumerate(docs_by_source.items(), 1):\n",
    "    # Combine first few chunks to represent the document\n",
    "    # (using all chunks would be too long and expensive)\n",
    "    doc_text = \"\\n\\n\".join([chunk.page_content for chunk in chunks[:3]])\n",
    "    \n",
    "    # Generate summary\n",
    "    summary = summary_chain.invoke({\"document\": doc_text})\n",
    "    doc_summaries[source] = summary\n",
    "    \n",
    "    print(f\"  [{i}/{len(docs_by_source)}] {source[:50]}...\")\n",
    "    \n",
    "    # Rate limiting to avoid API throttling\n",
    "    if i < len(docs_by_source):\n",
    "        time.sleep(0.5)\n",
    "\n",
    "print(f\"\\n✓ Generated {len(doc_summaries)} document summaries\")\n",
    "\n",
    "# Show example summary\n",
    "example_source = list(doc_summaries.keys())[0]\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Example summary for: {example_source}\")\n",
    "print(\"=\" * 80)\n",
    "print(doc_summaries[example_source])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Contextual Chunks\n",
    "\n",
    "Now we'll augment each chunk with contextual information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Generating Contextual Chunks\")\n",
    "\n",
    "# Create contextualization chain\n",
    "context_chain = CONTEXTUAL_CHUNK_PROMPT | llm | StrOutputParser()\n",
    "\n",
    "# Generate contextual chunks\n",
    "contextual_docs = []\n",
    "print(f\"\\nProcessing {len(docs)} chunks...\")\n",
    "print(\"(This may take a few minutes)\\n\")\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    source = doc.metadata.get(\"source\", \"unknown\")\n",
    "    doc_summary = doc_summaries.get(source, \"No summary available\")\n",
    "    \n",
    "    # Generate contextual description\n",
    "    context = context_chain.invoke({\n",
    "        \"doc_summary\": doc_summary,\n",
    "        \"chunk\": doc.page_content[:500],  # Limit chunk size for context generation\n",
    "    })\n",
    "    \n",
    "    # Create new document with context prepended\n",
    "    contextual_content = f\"[Context: {context}]\\n\\n{doc.page_content}\"\n",
    "    \n",
    "    contextual_doc = Document(\n",
    "        page_content=contextual_content,\n",
    "        metadata={\n",
    "            **doc.metadata,\n",
    "            \"context\": context,\n",
    "            \"original_content\": doc.page_content,\n",
    "        },\n",
    "    )\n",
    "    contextual_docs.append(contextual_doc)\n",
    "    \n",
    "    # Progress indicator\n",
    "    if i % 10 == 0:\n",
    "        print(f\"  Processed {i}/{len(docs)} chunks...\")\n",
    "    \n",
    "    # Rate limiting\n",
    "    if i < len(docs):\n",
    "        time.sleep(0.3)\n",
    "\n",
    "print(f\"\\n✓ Generated {len(contextual_docs)} contextual chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Standard vs Contextual Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Comparing Standard vs Contextual Chunks\")\n",
    "\n",
    "# Show example comparison\n",
    "example_idx = 5\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STANDARD CHUNK:\")\n",
    "print(\"=\" * 80)\n",
    "print(docs[example_idx].page_content[:400] + \"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONTEXTUAL CHUNK:\")\n",
    "print(\"=\" * 80)\n",
    "print(contextual_docs[example_idx].page_content[:500] + \"...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STATISTICS:\")\n",
    "print(\"=\" * 80)\n",
    "avg_context_len = sum(\n",
    "    len(doc.metadata.get(\"context\", \"\")) for doc in contextual_docs\n",
    ") / len(contextual_docs)\n",
    "print(f\"Average context length: {avg_context_len:.0f} characters\")\n",
    "print(f\"Context overhead: {avg_context_len / 1000 * 100:.1f}% (of 1000 char chunks)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Vector Stores\n",
    "\n",
    "We'll create two vector stores to compare:\n",
    "1. Standard RAG (no context)\n",
    "2. Contextual RAG (with context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "print_section_header(\"Creating Vector Stores\")\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(model=OPENAI_EMBEDDING_MODEL)\n",
    "\n",
    "# Try to load existing vector stores\n",
    "standard_store_path = VECTOR_STORE_DIR / \"contextual_rag_standard\"\n",
    "contextual_store_path = VECTOR_STORE_DIR / \"contextual_rag_contextual\"\n",
    "\n",
    "print(\"\\nChecking for existing vector stores...\")\n",
    "\n",
    "# Standard vector store\n",
    "vectorstore_standard = load_vector_store(\n",
    "    standard_store_path,\n",
    "    embeddings,\n",
    ")\n",
    "\n",
    "if vectorstore_standard is None:\n",
    "    print(\"\\nCreating standard vector store...\")\n",
    "    vectorstore_standard = FAISS.from_documents(docs, embeddings)\n",
    "    save_vector_store(vectorstore_standard, standard_store_path)\n",
    "    print(\"✓ Standard vector store created and saved\")\n",
    "else:\n",
    "    print(\"✓ Loaded existing standard vector store\")\n",
    "\n",
    "# Contextual vector store\n",
    "vectorstore_contextual = load_vector_store(\n",
    "    contextual_store_path,\n",
    "    embeddings,\n",
    ")\n",
    "\n",
    "if vectorstore_contextual is None:\n",
    "    print(\"\\nCreating contextual vector store...\")\n",
    "    vectorstore_contextual = FAISS.from_documents(contextual_docs, embeddings)\n",
    "    save_vector_store(vectorstore_contextual, contextual_store_path)\n",
    "    print(\"✓ Contextual vector store created and saved\")\n",
    "else:\n",
    "    print(\"✓ Loaded existing contextual vector store\")\n",
    "\n",
    "print(f\"\\n✓ Both vector stores ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Build RAG Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Building RAG Chains\")\n",
    "\n",
    "# Create retrievers\n",
    "retriever_standard = vectorstore_standard.as_retriever(\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "retriever_contextual = vectorstore_contextual.as_retriever(\n",
    "    search_kwargs={\"k\": 4}\n",
    ")\n",
    "\n",
    "# Standard RAG chain\n",
    "chain_standard = (\n",
    "    {\"context\": retriever_standard | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | RAG_PROMPT_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Contextual RAG chain\n",
    "chain_contextual = (\n",
    "    {\"context\": retriever_contextual | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | CONTEXTUAL_RAG_ANSWER_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✓ Standard RAG chain created\")\n",
    "print(\"✓ Contextual RAG chain created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test and Compare\n",
    "\n",
    "Let's test both approaches with queries that benefit from context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Testing Queries\")\n",
    "\n",
    "# Test queries that benefit from context\n",
    "test_queries = [\n",
    "    \"How do I use LCEL to build chains?\",\n",
    "    \"What are the different types of memory in LangChain?\",\n",
    "    \"Explain the role of retrievers in RAG applications\",\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries, 1):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"Query {i}: {query}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Standard RAG\n",
    "    print(\"\\n[STANDARD RAG]\")\n",
    "    print(\"-\" * 80)\n",
    "    start_time = time.time()\n",
    "    response_standard = chain_standard.invoke(query)\n",
    "    time_standard = time.time() - start_time\n",
    "    print(response_standard)\n",
    "    print(f\"\\n⏱️  Time: {time_standard:.2f}s\")\n",
    "    \n",
    "    # Contextual RAG\n",
    "    print(\"\\n[CONTEXTUAL RAG]\")\n",
    "    print(\"-\" * 80)\n",
    "    start_time = time.time()\n",
    "    response_contextual = chain_contextual.invoke(query)\n",
    "    time_contextual = time.time() - start_time\n",
    "    print(response_contextual)\n",
    "    print(f\"\\n⏱️  Time: {time_contextual:.2f}s\")\n",
    "    \n",
    "    # Comparison\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"COMPARISON:\")\n",
    "    print(f\"  • Latency difference: {abs(time_contextual - time_standard):.2f}s\")\n",
    "    print(f\"  • Response length difference: {len(response_contextual) - len(response_standard)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Retrieval Quality Comparison\n",
    "\n",
    "Let's examine what documents each approach retrieves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Retrieval Quality Analysis\")\n",
    "\n",
    "test_query = \"What are the different types of memory in LangChain?\"\n",
    "\n",
    "print(f\"\\nQuery: {test_query}\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "\n",
    "# Standard retrieval\n",
    "docs_standard = retriever_standard.invoke(test_query)\n",
    "print(\"\\n[STANDARD RETRIEVAL]\")\n",
    "print(\"-\" * 80)\n",
    "for i, doc in enumerate(docs_standard, 1):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'unknown')[:60]}\")\n",
    "    print(f\"Preview: {doc.page_content[:200]}...\")\n",
    "\n",
    "# Contextual retrieval\n",
    "docs_contextual = retriever_contextual.invoke(test_query)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n[CONTEXTUAL RETRIEVAL]\")\n",
    "print(\"-\" * 80)\n",
    "for i, doc in enumerate(docs_contextual, 1):\n",
    "    print(f\"\\nDocument {i}:\")\n",
    "    print(f\"Source: {doc.metadata.get('source', 'unknown')[:60]}\")\n",
    "    print(f\"Context: {doc.metadata.get('context', 'N/A')[:150]}...\")\n",
    "    print(f\"Preview: {doc.metadata.get('original_content', doc.page_content)[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Performance Metrics\")\n",
    "\n",
    "# Indexing costs\n",
    "num_documents = len(docs)\n",
    "num_summaries = len(doc_summaries)\n",
    "num_context_calls = len(contextual_docs)\n",
    "\n",
    "print(\"\\nINDEXING COSTS:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Documents processed: {num_documents}\")\n",
    "print(f\"Document summaries generated: {num_summaries}\")\n",
    "print(f\"Chunk contexts generated: {num_context_calls}\")\n",
    "print(f\"Total LLM calls for contextualization: {num_summaries + num_context_calls}\")\n",
    "print(f\"\\nEstimated additional indexing time: ~{(num_summaries + num_context_calls) * 0.5 / 60:.1f} minutes\")\n",
    "\n",
    "# Storage costs\n",
    "avg_standard_len = sum(len(doc.page_content) for doc in docs) / len(docs)\n",
    "avg_contextual_len = sum(len(doc.page_content) for doc in contextual_docs) / len(contextual_docs)\n",
    "overhead = (avg_contextual_len - avg_standard_len) / avg_standard_len * 100\n",
    "\n",
    "print(\"\\nSTORAGE OVERHEAD:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Average standard chunk: {avg_standard_len:.0f} chars\")\n",
    "print(f\"Average contextual chunk: {avg_contextual_len:.0f} chars\")\n",
    "print(f\"Overhead: {overhead:.1f}%\")\n",
    "\n",
    "# Query costs\n",
    "print(\"\\nQUERY COSTS:\")\n",
    "print(\"-\" * 80)\n",
    "print(\"Standard RAG: k retrievals + 1 generation\")\n",
    "print(\"Contextual RAG: k retrievals + 1 generation (same as standard)\")\n",
    "print(\"\\n✓ No additional query-time cost!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Takeaways\n",
    "\n",
    "### Summary\n",
    "\n",
    "**Contextual RAG** improves retrieval quality by augmenting chunks with contextual information:\n",
    "- Document-level summaries provide high-level context\n",
    "- Chunk-level descriptions clarify the role of each chunk\n",
    "- Better handling of ambiguous references and cross-references\n",
    "\n",
    "### Cost-Benefit Analysis\n",
    "\n",
    "| Aspect | Impact | Notes |\n",
    "|--------|--------|-------|\n",
    "| **Indexing Time** | ❌ +50-100% | One-time cost |\n",
    "| **Indexing Cost** | ❌ +$X | LLM calls for each chunk |\n",
    "| **Storage** | ❌ +15-30% | Larger embeddings |\n",
    "| **Query Time** | ✅ Same | No runtime overhead |\n",
    "| **Query Cost** | ✅ Same | No additional calls |\n",
    "| **Retrieval Quality** | ✅ Better | Improved precision |\n",
    "| **Answer Quality** | ✅ Better | More context-aware |\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Use for long documents**: Most beneficial when documents are long and complex\n",
    "2. **Batch processing**: Generate contexts in batches to reduce costs\n",
    "3. **Cache summaries**: Store document summaries to avoid regeneration\n",
    "4. **Balance context length**: Keep contexts concise (1-2 sentences)\n",
    "5. **Quality check**: Manually review a sample of generated contexts\n",
    "\n",
    "### When to Use\n",
    "\n",
    "Choose **Contextual RAG** when:\n",
    "- ✅ Document quality matters more than cost\n",
    "- ✅ Dealing with technical or complex documents\n",
    "- ✅ Users ask ambiguous or context-dependent questions\n",
    "- ✅ One-time indexing cost is acceptable\n",
    "\n",
    "Stick with **Standard RAG** when:\n",
    "- ✅ Documents are short and self-contained\n",
    "- ✅ Cost optimization is critical\n",
    "- ✅ Real-time indexing is required\n",
    "- ✅ Simpler implementation is preferred\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Combine with other techniques**: Contextual chunks work well with re-ranking\n",
    "- **Experiment with context generation**: Try different prompt strategies\n",
    "- **Measure impact**: Use RAGAS metrics to quantify improvement\n",
    "- **Optimize costs**: Use cheaper models for context generation\n",
    "\n",
    "---\n",
    "\n",
    "**Complexity Rating:** ⭐⭐⭐ (Medium - straightforward concept, some implementation overhead)\n",
    "\n",
    "**Production Readiness:** ⭐⭐⭐⭐ (High - proven technique, minor trade-offs)\n",
    "\n",
    "Continue to **13_fusion_rag.ipynb** for RAG-Fusion with Reciprocal Rank Fusion!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}