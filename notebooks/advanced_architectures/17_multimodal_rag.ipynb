{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17. Multimodal RAG - Images + Text üñºÔ∏è\n",
    "\n",
    "**Complexity:** ‚≠ê‚≠ê‚≠ê‚≠ê | **Duration:** ~25-30 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Multimodal RAG** extends traditional text-based RAG to handle **images, diagrams, charts, and visual content** alongside text. This is essential for:\n",
    "\n",
    "- üìä **Technical documentation** with diagrams\n",
    "- üìÑ **PDF reports** with charts and tables\n",
    "- üèóÔ∏è **Architectural drawings** and blueprints\n",
    "- üì∏ **Visual Q&A** systems\n",
    "- üé® **Design documents** with mockups\n",
    "\n",
    "### Key Technologies\n",
    "\n",
    "1. **GPT-4 Vision (GPT-4V)**: Multimodal LLM that understands images\n",
    "2. **OCR (Tesseract)**: Extract text from images\n",
    "3. **PDF Processing**: Extract images from PDFs\n",
    "4. **Image Embeddings**: Vector representations of images\n",
    "\n",
    "### Architecture Pattern\n",
    "\n",
    "```\n",
    "Document (PDF/HTML) \n",
    "  ‚îú‚îÄ‚Üí Extract Text ‚Üí Embed ‚Üí Vector Store\n",
    "  ‚îî‚îÄ‚Üí Extract Images ‚Üí OCR/Vision ‚Üí Embed ‚Üí Vector Store\n",
    "                              ‚Üì\n",
    "Query ‚Üí Retrieve (Text + Images) ‚Üí GPT-4V ‚Üí Answer\n",
    "```\n",
    "\n",
    "### When to Use\n",
    "\n",
    "- ‚úÖ Documents contain critical visual information\n",
    "- ‚úÖ Charts/graphs convey key data\n",
    "- ‚úÖ Diagrams explain complex processes\n",
    "- ‚úÖ Tables contain structured information\n",
    "- ‚ùå Purely textual documents (use standard RAG)\n",
    "- ‚ùå Real-time video analysis (different architecture)\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "```bash\n",
    "pip install pillow pytesseract pdf2image\n",
    "```\n",
    "\n",
    "**System Requirements:**\n",
    "- Tesseract OCR installed: `brew install tesseract` (macOS) or `apt-get install tesseract-ocr` (Linux)\n",
    "- Poppler installed (for PDF processing): `brew install poppler` (macOS)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import dependencies and configure environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport base64\nfrom io import BytesIO\nfrom pathlib import Path\nfrom typing import List, Dict, Any\n\n# Add project root to path\nsys.path.append('../..')\n\n# Core dependencies\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.messages import HumanMessage\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\n# Image processing\nfrom PIL import Image\nimport pytesseract\n\n# Shared utilities\nfrom shared import (\n    load_vector_store,\n    save_vector_store,\n    format_docs,\n    print_section_header,\n    print_results,\n    VECTOR_STORE_DIR,\n    SECTION_WIDTH\n)\n\nprint(\"=\" * SECTION_WIDTH)\nprint(\"MULTIMODAL RAG SETUP\")\nprint(\"=\" * SECTION_WIDTH)\nprint(\"\\n‚úÖ Imports successful\")\nprint(f\"‚úÖ Vector store directory: {VECTOR_STORE_DIR}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Image Processing Utilities\n",
    "\n",
    "Functions to handle image extraction, OCR, and encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_to_base64(image: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Encode PIL Image to base64 string for API transmission.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image object\n",
    "        \n",
    "    Returns:\n",
    "        Base64 encoded string\n",
    "    \"\"\"\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=\"PNG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "\n",
    "def extract_text_from_image(image: Image.Image) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from image using Tesseract OCR.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image object\n",
    "        \n",
    "    Returns:\n",
    "        Extracted text string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        text = pytesseract.image_to_string(image)\n",
    "        return text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  OCR failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def describe_image_with_vision(image: Image.Image, prompt: str = \"Describe this image in detail\") -> str:\n",
    "    \"\"\"\n",
    "    Use GPT-4 Vision to describe image content.\n",
    "    \n",
    "    Args:\n",
    "        image: PIL Image object\n",
    "        prompt: Description prompt\n",
    "        \n",
    "    Returns:\n",
    "        Image description from GPT-4V\n",
    "    \"\"\"\n",
    "    # Encode image\n",
    "    base64_image = encode_image_to_base64(image)\n",
    "    \n",
    "    # Initialize GPT-4 Vision\n",
    "    vision_model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "    \n",
    "    # Create message with image\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\"type\": \"text\", \"text\": prompt},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"}\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Get description\n",
    "    response = vision_model.invoke([message])\n",
    "    return response.content\n",
    "\n",
    "\n",
    "print(\"‚úÖ Image processing utilities defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Sample Documents with Images\n",
    "\n",
    "For demonstration, we'll create synthetic documents that combine text and image descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents with embedded image descriptions\n",
    "multimodal_docs = [\n",
    "    {\n",
    "        \"text\": \"\"\"LangChain Expression Language (LCEL) Architecture\n",
    "        \n",
    "        LCEL is a declarative way to compose chains. The architecture follows a pipe operator pattern\n",
    "        where components are connected using the | operator. This enables:\n",
    "        - Composability: Chain components together\n",
    "        - Streaming: Stream tokens as they're generated\n",
    "        - Async: Run chains concurrently\n",
    "        - Fallbacks: Add error handling easily\n",
    "        \n",
    "        The typical LCEL chain looks like: prompt | model | output_parser\n",
    "        \"\"\",\n",
    "        \"image_description\": \"\"\"[DIAGRAM] The diagram shows three boxes connected by pipe operators:\n",
    "        Box 1: 'ChatPromptTemplate' with input variables (context, input)\n",
    "        Box 2: 'ChatOpenAI' (gpt-4o-mini) with temperature=0\n",
    "        Box 3: 'StrOutputParser' outputting final string\n",
    "        Arrows flow left to right showing data transformation at each stage.\"\"\",\n",
    "        \"metadata\": {\"source\": \"langchain_docs\", \"type\": \"architecture_diagram\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"\"\"RAG Performance Benchmarks\n",
    "        \n",
    "        Performance comparison across different RAG architectures shows significant trade-offs\n",
    "        between speed and quality. Simple RAG offers the fastest response times at ~2 seconds,\n",
    "        while Agentic RAG provides the highest quality at the cost of 20-40 second latency.\n",
    "        \n",
    "        Cost per query ranges from $0.00036 for Simple RAG to $0.00360 for Agentic RAG,\n",
    "        representing a 10x difference.\n",
    "        \"\"\",\n",
    "        \"image_description\": \"\"\"[CHART] Bar chart showing:\n",
    "        X-axis: Architecture names (Simple RAG, Memory RAG, Branched RAG, HyDe, Adaptive RAG, CRAG, Self-RAG, Agentic RAG)\n",
    "        Y-axis Left: Latency in seconds (bars in blue)\n",
    "        Y-axis Right: Cost per query in dollars (line in red)\n",
    "        The chart clearly shows latency increasing from 2s to 40s, and cost increasing from $0.00036 to $0.00360.\n",
    "        Agentic RAG has the highest bars and highest cost point.\"\"\",\n",
    "        \"metadata\": {\"source\": \"performance_docs\", \"type\": \"performance_chart\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"\"\"Vector Store Architecture\n",
    "        \n",
    "        FAISS (Facebook AI Similarity Search) is used for efficient vector similarity search.\n",
    "        The architecture consists of:\n",
    "        1. Document Embedding: Convert text to 1536-dimensional vectors (OpenAI)\n",
    "        2. Index Building: Create FAISS index for fast nearest neighbor search\n",
    "        3. Persistence: Save index to disk for reuse\n",
    "        4. Retrieval: Query the index with embedded questions\n",
    "        \n",
    "        FAISS supports billions of vectors with millisecond query times.\n",
    "        \"\"\",\n",
    "        \"image_description\": \"\"\"[FLOWCHART] Process flow diagram:\n",
    "        Step 1: 'Documents' (blue box) ‚Üí 'Text Splitter' (green box) produces 'Chunks'\n",
    "        Step 2: 'Chunks' ‚Üí 'Embeddings' (orange box) produces 'Vectors (1536d)'\n",
    "        Step 3: 'Vectors' ‚Üí 'FAISS Index' (purple box) with note 'Save to disk'\n",
    "        Step 4: 'Query' ‚Üí 'Embed Query' ‚Üí 'Search Index' ‚Üí 'Top-k Documents'\n",
    "        Dashed line shows persistence path from FAISS Index to disk storage.\"\"\",\n",
    "        \"metadata\": {\"source\": \"architecture_docs\", \"type\": \"flowchart\"}\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"\"\"Contextual RAG Results\n",
    "        \n",
    "        Anthropic's Contextual RAG technique shows impressive improvements in retrieval quality.\n",
    "        By prepending document-level context to each chunk before embedding, we see:\n",
    "        - 15-30% improvement in retrieval precision\n",
    "        - Minimal query-time overhead (context added during indexing)\n",
    "        - Better semantic matching for technical documents\n",
    "        \n",
    "        The technique is especially effective for code documentation and API references.\n",
    "        \"\"\",\n",
    "        \"image_description\": \"\"\"[TABLE] Comparison table with 3 columns:\n",
    "        Column 1: Metric | Column 2: Simple RAG | Column 3: Contextual RAG\n",
    "        Row 1: Precision@5 | 68% | 87% (+19%)\n",
    "        Row 2: Recall@10 | 72% | 89% (+17%)\n",
    "        Row 3: Query Time | 1.8s | 2.1s (+0.3s)\n",
    "        Row 4: Index Size | 12MB | 15MB (+25%)\n",
    "        Green highlighting on Contextual RAG improvements.\"\"\",\n",
    "        \"metadata\": {\"source\": \"evaluation_docs\", \"type\": \"comparison_table\"}\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(multimodal_docs)} multimodal documents\")\n",
    "print(f\"   - {sum(1 for d in multimodal_docs if 'DIAGRAM' in d['image_description'])} diagrams\")\n",
    "print(f\"   - {sum(1 for d in multimodal_docs if 'CHART' in d['image_description'])} charts\")\n",
    "print(f\"   - {sum(1 for d in multimodal_docs if 'FLOWCHART' in d['image_description'])} flowcharts\")\n",
    "print(f\"   - {sum(1 for d in multimodal_docs if 'TABLE' in d['image_description'])} tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Multimodal Vector Store\n",
    "\n",
    "Combine text and image descriptions into a unified retrieval system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LangChain Document objects\n",
    "langchain_docs = []\n",
    "\n",
    "for doc in multimodal_docs:\n",
    "    # Combine text and image description for embedding\n",
    "    combined_content = f\"{doc['text']}\\n\\n[VISUAL CONTENT]: {doc['image_description']}\"\n",
    "    \n",
    "    langchain_docs.append(\n",
    "        Document(\n",
    "            page_content=combined_content,\n",
    "            metadata=doc['metadata']\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Create embeddings and vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(langchain_docs, embeddings)\n",
    "\n",
    "# Save for reuse\n",
    "save_vector_store(vectorstore, VECTOR_STORE_DIR / \"multimodal\")\n",
    "\n",
    "print(\"\\n‚úÖ Multimodal vector store created\")\n",
    "print(f\"   Total documents: {len(langchain_docs)}\")\n",
    "print(f\"   Saved to: {VECTOR_STORE_DIR / 'multimodal'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Standard Multimodal RAG\n",
    "\n",
    "Query the multimodal vector store and generate answers using GPT-4o (which has vision capabilities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.prompts import RAG_PROMPT_TEMPLATE\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Build multimodal RAG chain with GPT-4o (supports vision context)\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "multimodal_chain = (\n",
    "    {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | RAG_PROMPT_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Multimodal RAG chain created with GPT-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Query 1: LCEL Architecture\n",
    "\n",
    "Ask about LCEL chain composition (references diagram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"How does LCEL chain composition work? What are the main components?\"\n",
    "\n",
    "print_section_header(\"Query 1: LCEL Architecture\")\n",
    "response1 = multimodal_chain.invoke(query1)\n",
    "print_results(query1, response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Query 2: Performance Comparison\n",
    "\n",
    "Ask about performance trade-offs (references chart)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"What are the performance trade-offs between Simple RAG and Agentic RAG?\"\n",
    "\n",
    "print_section_header(\"Query 2: Performance Trade-offs\")\n",
    "response2 = multimodal_chain.invoke(query2)\n",
    "print_results(query2, response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Query 3: Technical Details\n",
    "\n",
    "Ask about Contextual RAG improvements (references table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query3 = \"What improvement metrics does Contextual RAG achieve compared to Simple RAG?\"\n",
    "\n",
    "print_section_header(\"Query 3: Contextual RAG Metrics\")\n",
    "response3 = multimodal_chain.invoke(query3)\n",
    "print_results(query3, response3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced: Direct Image Analysis\n",
    "\n",
    "For scenarios where you have actual image files, use GPT-4 Vision directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_with_rag(image_path: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyze an image using GPT-4 Vision, then use RAG for additional context.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image file\n",
    "        question: Question about the image\n",
    "        \n",
    "    Returns:\n",
    "        Combined answer from vision + RAG\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Get vision description\n",
    "    vision_prompt = f\"\"\"Analyze this image and answer the question: {question}\n",
    "    \n",
    "    Provide a detailed technical description of what you see, focusing on elements\n",
    "    relevant to the question.\"\"\"\n",
    "    \n",
    "    vision_description = describe_image_with_vision(image, vision_prompt)\n",
    "    \n",
    "    # Use vision description to retrieve relevant context\n",
    "    relevant_docs = retriever.invoke(vision_description)\n",
    "    context = format_docs(relevant_docs)\n",
    "    \n",
    "    # Generate final answer combining vision + RAG\n",
    "    final_prompt = f\"\"\"Based on the image analysis and retrieved context, answer the question.\n",
    "    \n",
    "    Image Analysis:\n",
    "    {vision_description}\n",
    "    \n",
    "    Retrieved Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Provide a comprehensive answer that combines insights from both the image and the context.\"\"\"\n",
    "    \n",
    "    response = llm.invoke(final_prompt)\n",
    "    return response.content\n",
    "\n",
    "print(\"‚úÖ Advanced image analysis function defined\")\n",
    "print(\"   Use: analyze_image_with_rag('path/to/image.png', 'your question')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Production Optimizations\n",
    "\n",
    "Best practices for multimodal RAG in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Image Preprocessing Pipeline\n",
    "\n",
    "```python\n",
    "def preprocess_image(image: Image.Image) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Optimize image for vision model:\n",
    "    - Resize to max 2048x2048 (GPT-4V limit)\n",
    "    - Convert to RGB if needed\n",
    "    - Compress if file size > 20MB\n",
    "    \"\"\"\n",
    "    max_size = 2048\n",
    "    \n",
    "    # Resize if too large\n",
    "    if max(image.size) > max_size:\n",
    "        image.thumbnail((max_size, max_size), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Convert to RGB\n",
    "    if image.mode != 'RGB':\n",
    "        image = image.convert('RGB')\n",
    "    \n",
    "    return image\n",
    "```\n",
    "\n",
    "### 6.2 Caching Strategy\n",
    "\n",
    "```python\n",
    "# Cache vision descriptions to avoid redundant API calls\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "vision_cache = {}\n",
    "\n",
    "def cached_describe_image(image: Image.Image, prompt: str) -> str:\n",
    "    # Generate cache key from image + prompt\n",
    "    image_bytes = BytesIO()\n",
    "    image.save(image_bytes, format='PNG')\n",
    "    image_hash = hashlib.md5(image_bytes.getvalue()).hexdigest()\n",
    "    cache_key = f\"{image_hash}_{prompt}\"\n",
    "    \n",
    "    if cache_key in vision_cache:\n",
    "        return vision_cache[cache_key]\n",
    "    \n",
    "    description = describe_image_with_vision(image, prompt)\n",
    "    vision_cache[cache_key] = description\n",
    "    \n",
    "    return description\n",
    "```\n",
    "\n",
    "### 6.3 Batch Processing\n",
    "\n",
    "```python\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def batch_process_images(image_paths: List[str], max_workers: int = 4) -> List[str]:\n",
    "    \"\"\"\n",
    "    Process multiple images in parallel.\n",
    "    \"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        descriptions = list(executor.map(\n",
    "            lambda path: describe_image_with_vision(Image.open(path)),\n",
    "            image_paths\n",
    "        ))\n",
    "    return descriptions\n",
    "```\n",
    "\n",
    "### 6.4 Cost Optimization\n",
    "\n",
    "**GPT-4 Vision Costs (as of 2025):**\n",
    "- Low detail: $0.00765 per image\n",
    "- High detail: ~$0.01590 per image (depends on resolution)\n",
    "\n",
    "**Optimization strategies:**\n",
    "1. Use OCR for text-heavy images (cheaper than vision)\n",
    "2. Cache descriptions aggressively\n",
    "3. Use low-detail mode when possible\n",
    "4. Batch process during off-peak hours\n",
    "5. Consider GPT-4o-mini for simpler images\n",
    "\n",
    "### 6.5 Error Handling\n",
    "\n",
    "```python\n",
    "def robust_image_analysis(image_path: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Analyze image with fallbacks:\n",
    "    1. Try GPT-4 Vision\n",
    "    2. Fall back to OCR if vision fails\n",
    "    3. Return error message if both fail\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "        return analyze_image_with_rag(image_path, question)\n",
    "    except Exception as vision_error:\n",
    "        print(f\"‚ö†Ô∏è  Vision failed: {vision_error}, trying OCR...\")\n",
    "        try:\n",
    "            text = extract_text_from_image(image)\n",
    "            return multimodal_chain.invoke(f\"{question}\\n\\nExtracted text: {text}\")\n",
    "        except Exception as ocr_error:\n",
    "            return f\"‚ùå Image analysis failed: {ocr_error}\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Best Practices\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "‚úÖ **Multimodal RAG combines:**\n",
    "- Text retrieval (standard RAG)\n",
    "- Image understanding (GPT-4 Vision)\n",
    "- OCR for text extraction\n",
    "- Unified vector store\n",
    "\n",
    "‚úÖ **Use cases:**\n",
    "- Technical documentation with diagrams\n",
    "- PDF reports with charts/tables\n",
    "- Visual Q&A systems\n",
    "- Architectural/design documents\n",
    "\n",
    "‚úÖ **Production considerations:**\n",
    "- Image preprocessing (resize, convert, compress)\n",
    "- Caching vision descriptions\n",
    "- Batch processing for efficiency\n",
    "- Error handling with fallbacks\n",
    "- Cost optimization (OCR vs Vision)\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "| Approach | Latency | Cost/Query | Accuracy |\n",
    "|---|---|---|---|\n",
    "| **Text-only RAG** | ~2s | $0.0004 | Good for text |\n",
    "| **OCR + RAG** | ~3s | $0.0005 | Good for text-heavy images |\n",
    "| **Vision + RAG** | ~5-8s | $0.012 | Excellent for all images |\n",
    "| **Hybrid (OCR + Vision)** | ~6-10s | $0.008 | Best overall |\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "**OCR + RAG:**\n",
    "- Text-heavy images (scanned documents, screenshots)\n",
    "- Simple charts with labels\n",
    "- Cost-sensitive applications\n",
    "\n",
    "**Vision + RAG:**\n",
    "- Complex diagrams, flowcharts\n",
    "- Charts with visual patterns\n",
    "- Images with spatial relationships\n",
    "- High-accuracy requirements\n",
    "\n",
    "**Hybrid:**\n",
    "- Production systems (use OCR first, vision as fallback)\n",
    "- Mixed document types\n",
    "- Balance cost and quality\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Integrate with PDF processing**: Use `pdf2image` to extract images from PDFs\n",
    "2. **Add image embeddings**: Use CLIP for semantic image search\n",
    "3. **Build UI**: Streamlit/Gradio interface for uploading images\n",
    "4. **Deploy**: Package as FastAPI endpoint with image upload\n",
    "5. **Monitor costs**: Track vision API usage and implement budgets\n",
    "\n",
    "---\n",
    "\n",
    "**üìö Related Notebooks:**\n",
    "- [03_simple_rag.ipynb](../fundamentals/03_simple_rag.ipynb) - Text-only RAG baseline\n",
    "- [12_contextual_rag.ipynb](12_contextual_rag.ipynb) - Context-augmented retrieval\n",
    "- [18_finetuning_embeddings.ipynb](18_finetuning_embeddings.ipynb) - Custom embeddings\n",
    "\n",
    "**üîó External Resources:**\n",
    "- [GPT-4 Vision API](https://platform.openai.com/docs/guides/vision)\n",
    "- [Tesseract OCR](https://github.com/tesseract-ocr/tesseract)\n",
    "- [PIL/Pillow Docs](https://pillow.readthedocs.io/)\n",
    "\n",
    "---\n",
    "\n",
    "üéâ **Multimodal RAG Complete!**\n",
    "\n",
    "You now have a production-ready system that handles both text and images. Experiment with your own documents and images!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}