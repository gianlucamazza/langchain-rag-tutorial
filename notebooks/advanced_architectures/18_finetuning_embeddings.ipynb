{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 18. Fine-tuning Embeddings for Domain-Specific RAG üéØ\n",
    "\n",
    "**Complexity:** ‚≠ê‚≠ê‚≠ê‚≠ê | **Duration:** ~30-35 minutes\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "**Fine-tuning embeddings** adapts pre-trained embedding models to your specific domain, improving retrieval accuracy for specialized content.\n",
    "\n",
    "### When to Fine-tune\n",
    "\n",
    "‚úÖ **Consider fine-tuning when:**\n",
    "- Domain-specific jargon (medical, legal, technical)\n",
    "- Industry-specific abbreviations\n",
    "- Retrieval accuracy < 70% with pre-trained models\n",
    "- Large proprietary corpus (10K+ documents)\n",
    "- Cost-sensitive (local models)\n",
    "\n",
    "‚ùå **Don't fine-tune when:**\n",
    "- General domain content\n",
    "- Small dataset (< 1K documents)\n",
    "- OpenAI embeddings already work well (> 85% accuracy)\n",
    "- No time/resources for training\n",
    "\n",
    "### Fine-tuning Approaches\n",
    "\n",
    "1. **Contrastive Learning**: Train on (query, relevant_doc, irrelevant_doc) triplets\n",
    "2. **Supervised Fine-tuning**: Use labeled (query, document, relevance_score) pairs\n",
    "3. **Domain Adaptation**: Continue pre-training on domain corpus\n",
    "\n",
    "### Expected Improvements\n",
    "\n",
    "| Metric | Pre-trained | Fine-tuned | Improvement |\n",
    "|---|---|---|---|\n",
    "| Precision@5 | 68% | 82-89% | +14-21% |\n",
    "| Recall@10 | 72% | 85-92% | +13-20% |\n",
    "| MRR (Mean Reciprocal Rank) | 0.65 | 0.78-0.85 | +13-20% |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install sentence-transformers for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nfrom pathlib import Path\nfrom typing import List, Dict, Tuple\nimport random\n\n# Add project root\nsys.path.append('../..')\n\n# Core dependencies\nfrom sentence_transformers import (\n    SentenceTransformer,\n    InputExample,\n    losses,\n    evaluation,\n    util\n)\nfrom torch.utils.data import DataLoader\n\n# Standard RAG components\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_huggingface import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.documents import Document\n\n# Shared utilities\nfrom shared import (\n    load_langchain_docs,\n    split_documents,\n    save_vector_store,\n    load_vector_store,\n    format_docs,\n    print_section_header,\n    VECTOR_STORE_DIR,\n    SECTION_WIDTH\n)\n\nprint(\"=\" * SECTION_WIDTH)\nprint(\"FINE-TUNING EMBEDDINGS SETUP\")\nprint(\"=\" * SECTION_WIDTH)\nprint(\"\\n‚úÖ Imports successful\")\nprint(\"‚úÖ sentence-transformers ready for fine-tuning\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation\n",
    "\n",
    "Create training data with (query, positive_doc, negative_doc) triplets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample domain-specific dataset (LangChain RAG domain)\n",
    "training_data = [\n",
    "    {\n",
    "        \"query\": \"How do I create a FAISS vector store?\",\n",
    "        \"positive\": \"\"\"To create a FAISS vector store in LangChain, use FAISS.from_documents(chunks, embeddings).\n",
    "        This takes your document chunks and embedding model, creates the index, and returns a vectorstore object.\n",
    "        You can then save it with save_vector_store() or use it immediately with as_retriever().\"\"\",\n",
    "        \"negative\": \"\"\"HyDe (Hypothetical Document Embeddings) is a technique where you generate a hypothetical\n",
    "        answer to the query, embed it, and use it for retrieval instead of embedding the raw query.\n",
    "        This improves semantic matching for ambiguous questions.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is the difference between similarity and MMR retrieval?\",\n",
    "        \"positive\": \"\"\"Similarity search returns the k most similar documents based on cosine similarity.\n",
    "        MMR (Maximal Marginal Relevance) balances relevance and diversity by selecting documents that are\n",
    "        both relevant to the query and dissimilar to already selected documents. Use MMR when you want\n",
    "        diverse results covering different aspects of the query.\"\"\",\n",
    "        \"negative\": \"\"\"LCEL (LangChain Expression Language) uses the pipe operator | to chain components.\n",
    "        A typical chain looks like: prompt | model | output_parser. This enables streaming, async,\n",
    "        and fallback capabilities out of the box.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How does Adaptive RAG routing work?\",\n",
    "        \"positive\": \"\"\"Adaptive RAG classifies query complexity (SIMPLE/MEDIUM/COMPLEX) and routes to different\n",
    "        retrieval strategies. Simple queries use fast similarity search, medium queries use MMR for diversity,\n",
    "        and complex queries use HyDe for better semantic matching. This optimizes cost and latency.\"\"\",\n",
    "        \"negative\": \"\"\"To split documents, use RecursiveCharacterTextSplitter with chunk_size=1000 and\n",
    "        chunk_overlap=200. This preserves context across chunk boundaries while keeping chunks small\n",
    "        enough for effective retrieval.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is Self-RAG and when should I use it?\",\n",
    "        \"positive\": \"\"\"Self-RAG adds autonomous decision-making: the LLM decides if retrieval is needed,\n",
    "        generates a response, critiques its own answer, and retries if quality is low. Use it for\n",
    "        quality-critical applications where self-correction is valuable, but expect 10-20s latency.\"\"\",\n",
    "        \"negative\": \"\"\"Contextual RAG prepends document-level context to each chunk before embedding.\n",
    "        This improves retrieval precision by 15-30% with minimal query overhead, as the context is\n",
    "        added during indexing, not at query time.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How do I reduce RAG costs?\",\n",
    "        \"positive\": \"\"\"To reduce costs: 1) Use HuggingFace embeddings (free, local), 2) Cache vector stores\n",
    "        to avoid re-embedding, 3) Reduce k parameter (retrieve fewer docs), 4) Use Adaptive RAG to\n",
    "        route simple queries to cheaper strategies, 5) Use GPT-4o-mini instead of GPT-4.\"\"\",\n",
    "        \"negative\": \"\"\"GraphRAG extracts entities and relationships from documents, builds a knowledge graph,\n",
    "        and performs multi-hop reasoning. It's excellent for relationship-centric queries but has\n",
    "        higher setup complexity.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What's the best RAG architecture for chatbots?\",\n",
    "        \"positive\": \"\"\"For chatbots, use Memory RAG (04_rag_with_memory.ipynb). It maintains conversation\n",
    "        history using ConversationBufferMemory or ConversationBufferWindowMemory, allowing the system\n",
    "        to understand follow-up questions and references to previous context.\"\"\",\n",
    "        \"negative\": \"\"\"Agentic RAG uses ReAct agents with multiple tools (retriever, calculator, web search).\n",
    "        It's ideal for complex multi-step reasoning but has 20-40s latency due to the agent loop.\n",
    "        Use for BI dashboards or complex analytics.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How does CRAG improve retrieval quality?\",\n",
    "        \"positive\": \"\"\"CRAG (Corrective RAG) grades each retrieved document for relevance. If quality is low,\n",
    "        it triggers a web search fallback to find better information. This is perfect for out-of-domain\n",
    "        queries or when vector store doesn't have current information. Expect 10-15s latency.\"\"\",\n",
    "        \"negative\": \"\"\"Fusion RAG generates 3-5 query perspectives and retrieves documents for each.\n",
    "        It then combines results using Reciprocal Rank Fusion (RRF) algorithm, where documents\n",
    "        appearing in multiple result sets get higher scores.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What embeddings should I use for production?\",\n",
    "        \"positive\": \"\"\"For production: Use OpenAI text-embedding-3-small for best quality (1536d, $0.02/1M tokens).\n",
    "        For cost-sensitive or offline use, try HuggingFace all-MiniLM-L6-v2 (384d, free, local).\n",
    "        Quality difference is ~10-15%, with OpenAI being better.\"\"\",\n",
    "        \"negative\": \"\"\"SQL RAG converts natural language to SQL queries. It retrieves relevant database schema,\n",
    "        generates SQL with validation, executes safely (read-only), and interprets results. Perfect for\n",
    "        analytics and BI use cases.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Created {len(training_data)} training examples\")\n",
    "print(\"   Each example has: query, positive doc, negative doc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Evaluation\n",
    "\n",
    "Test pre-trained model performance before fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model\n",
    "base_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Create test queries and documents\n",
    "test_queries = [item[\"query\"] for item in training_data]\n",
    "test_positives = [item[\"positive\"] for item in training_data]\n",
    "\n",
    "# Encode\n",
    "query_embeddings = base_model.encode(test_queries, convert_to_tensor=True)\n",
    "doc_embeddings = base_model.encode(test_positives, convert_to_tensor=True)\n",
    "\n",
    "# Calculate similarities\n",
    "similarities = util.cos_sim(query_embeddings, doc_embeddings)\n",
    "\n",
    "# Evaluate: For each query, is its positive doc the top match?\n",
    "correct = 0\n",
    "mrr_scores = []\n",
    "\n",
    "for i in range(len(test_queries)):\n",
    "    # Get similarity scores for this query against all docs\n",
    "    scores = similarities[i]\n",
    "    \n",
    "    # Find rank of correct document (i-th doc should match i-th query)\n",
    "    sorted_indices = scores.argsort(descending=True)\n",
    "    rank = (sorted_indices == i).nonzero(as_tuple=True)[0].item() + 1\n",
    "    \n",
    "    if rank == 1:\n",
    "        correct += 1\n",
    "    \n",
    "    mrr_scores.append(1.0 / rank)\n",
    "\n",
    "baseline_accuracy = correct / len(test_queries)\n",
    "baseline_mrr = sum(mrr_scores) / len(mrr_scores)\n",
    "\n",
    "print(\"\\n\" + \"=\" * SECTION_WIDTH)\n",
    "print(\"BASELINE EVALUATION (Pre-trained Model)\")\n",
    "print(\"=\" * SECTION_WIDTH)\n",
    "print(f\"Accuracy (top-1): {baseline_accuracy:.1%}\")\n",
    "print(f\"MRR (Mean Reciprocal Rank): {baseline_mrr:.3f}\")\n",
    "print(f\"\\nInterpretation:\")\n",
    "print(f\"  - {correct}/{len(test_queries)} queries found correct doc in rank 1\")\n",
    "print(f\"  - Average rank of correct doc: {1/baseline_mrr:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Fine-tuning with Contrastive Learning\n",
    "\n",
    "Train the model using MultipleNegativesRankingLoss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training examples\n",
    "train_examples = []\n",
    "\n",
    "for item in training_data:\n",
    "    # InputExample expects (texts=[anchor, positive], label=similarity_score)\n",
    "    # For contrastive learning, we use anchor=query, positive=relevant_doc\n",
    "    train_examples.append(\n",
    "        InputExample(texts=[item[\"query\"], item[\"positive\"]])\n",
    "    )\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    train_examples,\n",
    "    shuffle=True,\n",
    "    batch_size=4  # Small batch for small dataset\n",
    ")\n",
    "\n",
    "# Initialize fine-tuning loss\n",
    "# MultipleNegativesRankingLoss: treats other positives in batch as negatives\n",
    "train_loss = losses.MultipleNegativesRankingLoss(base_model)\n",
    "\n",
    "print(\"‚úÖ Training configuration:\")\n",
    "print(f\"   - Training examples: {len(train_examples)}\")\n",
    "print(f\"   - Batch size: 4\")\n",
    "print(f\"   - Loss: MultipleNegativesRankingLoss\")\n",
    "print(f\"   - Base model: all-MiniLM-L6-v2 (384d)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "print(\"\\n\" + \"=\" * SECTION_WIDTH)\n",
    "print(\"FINE-TUNING IN PROGRESS...\")\n",
    "print(\"=\" * SECTION_WIDTH)\n",
    "\n",
    "# Create output directory\n",
    "output_path = Path(\"../../data/models/finetuned-embeddings\")\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Train\n",
    "base_model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=10,  # More epochs for small dataset\n",
    "    warmup_steps=10,\n",
    "    output_path=str(output_path),\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Fine-tuning complete! Model saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Post-Training Evaluation\n",
    "\n",
    "Compare fine-tuned model against baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model\n",
    "finetuned_model = SentenceTransformer(str(output_path))\n",
    "\n",
    "# Encode with fine-tuned model\n",
    "ft_query_embeddings = finetuned_model.encode(test_queries, convert_to_tensor=True)\n",
    "ft_doc_embeddings = finetuned_model.encode(test_positives, convert_to_tensor=True)\n",
    "\n",
    "# Calculate similarities\n",
    "ft_similarities = util.cos_sim(ft_query_embeddings, ft_doc_embeddings)\n",
    "\n",
    "# Evaluate\n",
    "ft_correct = 0\n",
    "ft_mrr_scores = []\n",
    "\n",
    "for i in range(len(test_queries)):\n",
    "    scores = ft_similarities[i]\n",
    "    sorted_indices = scores.argsort(descending=True)\n",
    "    rank = (sorted_indices == i).nonzero(as_tuple=True)[0].item() + 1\n",
    "    \n",
    "    if rank == 1:\n",
    "        ft_correct += 1\n",
    "    \n",
    "    ft_mrr_scores.append(1.0 / rank)\n",
    "\n",
    "finetuned_accuracy = ft_correct / len(test_queries)\n",
    "finetuned_mrr = sum(ft_mrr_scores) / len(ft_mrr_scores)\n",
    "\n",
    "# Print comparison\n",
    "print(\"\\n\" + \"=\" * SECTION_WIDTH)\n",
    "print(\"EVALUATION COMPARISON\")\n",
    "print(\"=\" * SECTION_WIDTH)\n",
    "\n",
    "print(f\"\\n{'Metric':<25} {'Baseline':<15} {'Fine-tuned':<15} {'Improvement'}\")\n",
    "print(\"-\" * SECTION_WIDTH)\n",
    "print(f\"{'Accuracy (top-1)':<25} {baseline_accuracy:<15.1%} {finetuned_accuracy:<15.1%} {finetuned_accuracy - baseline_accuracy:+.1%}\")\n",
    "print(f\"{'MRR':<25} {baseline_mrr:<15.3f} {finetuned_mrr:<15.3f} {finetuned_mrr - baseline_mrr:+.3f}\")\n",
    "print(f\"{'Avg Rank of Correct Doc':<25} {1/baseline_mrr:<15.1f} {1/finetuned_mrr:<15.1f} {1/baseline_mrr - 1/finetuned_mrr:+.1f}\")\n",
    "\n",
    "improvement = ((finetuned_accuracy - baseline_accuracy) / baseline_accuracy * 100)\n",
    "print(f\"\\nüìä Relative improvement: {improvement:+.1f}%\")\n",
    "\n",
    "if improvement > 10:\n",
    "    print(\"‚úÖ Significant improvement! Fine-tuning is beneficial for this domain.\")\n",
    "elif improvement > 0:\n",
    "    print(\"‚ö†Ô∏è  Modest improvement. Consider more training data or epochs.\")\n",
    "else:\n",
    "    print(\"‚ùå No improvement. Pre-trained model may already be optimal.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Integration with RAG Pipeline\n",
    "\n",
    "Use fine-tuned embeddings in a production RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "docs = load_langchain_docs()\n",
    "chunks = split_documents(docs)\n",
    "\n",
    "# Create embeddings using fine-tuned model\n",
    "finetuned_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=str(output_path),\n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "# Build vector store with fine-tuned embeddings\n",
    "finetuned_vectorstore = FAISS.from_documents(\n",
    "    chunks[:50],  # Use subset for demo\n",
    "    finetuned_embeddings\n",
    ")\n",
    "\n",
    "# Save for reuse\n",
    "save_vector_store(finetuned_vectorstore, VECTOR_STORE_DIR / \"finetuned\")\n",
    "\n",
    "print(\"‚úÖ Fine-tuned vector store created\")\n",
    "print(f\"   Documents: {len(chunks[:50])}\")\n",
    "print(f\"   Saved to: {VECTOR_STORE_DIR / 'finetuned'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side-by-Side Comparison\n",
    "\n",
    "Compare retrieval results between baseline and fine-tuned embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseline vector store (pre-trained embeddings)\n",
    "baseline_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "baseline_vectorstore = FAISS.from_documents(chunks[:50], baseline_embeddings)\n",
    "\n",
    "# Test query\n",
    "test_query = \"How do I reduce RAG costs?\"\n",
    "\n",
    "# Retrieve with baseline\n",
    "baseline_docs = baseline_vectorstore.similarity_search(test_query, k=3)\n",
    "\n",
    "# Retrieve with fine-tuned\n",
    "finetuned_docs = finetuned_vectorstore.similarity_search(test_query, k=3)\n",
    "\n",
    "# Display results\n",
    "print(\"\\n\" + \"=\" * SECTION_WIDTH)\n",
    "print(f\"RETRIEVAL COMPARISON: '{test_query}'\")\n",
    "print(\"=\" * SECTION_WIDTH)\n",
    "\n",
    "print(\"\\nüìò BASELINE MODEL (Pre-trained):\")\n",
    "for i, doc in enumerate(baseline_docs, 1):\n",
    "    print(f\"\\n  Result {i}:\")\n",
    "    print(f\"  {doc.page_content[:200]}...\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * SECTION_WIDTH)\n",
    "\n",
    "print(\"\\nüéØ FINE-TUNED MODEL:\")\n",
    "for i, doc in enumerate(finetuned_docs, 1):\n",
    "    print(f\"\\n  Result {i}:\")\n",
    "    print(f\"  {doc.page_content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Production Best Practices\n",
    "\n",
    "### 6.1 Dataset Expansion\n",
    "\n",
    "For production, you need 1K-10K+ training examples. Here's how to generate them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data_from_docs(documents: List[Document], llm: ChatOpenAI, num_examples: int = 100):\n",
    "    \"\"\"\n",
    "    Auto-generate (query, positive_doc) pairs using LLM.\n",
    "    \n",
    "    Strategy:\n",
    "    1. For each document chunk, ask LLM to generate relevant queries\n",
    "    2. Use document as positive example\n",
    "    3. Sample other docs as negatives\n",
    "    \"\"\"\n",
    "    training_pairs = []\n",
    "    \n",
    "    query_gen_prompt = \"\"\"Given this document chunk, generate 3 questions that this chunk answers:\n",
    "    \n",
    "    Document:\n",
    "    {document}\n",
    "    \n",
    "    Return ONLY the 3 questions, one per line, without numbering.\"\"\"\n",
    "    \n",
    "    for doc in documents[:num_examples // 3]:  # Generate 3 queries per doc\n",
    "        # Generate queries\n",
    "        response = llm.invoke(query_gen_prompt.format(document=doc.page_content))\n",
    "        queries = [q.strip() for q in response.content.strip().split('\\n') if q.strip()]\n",
    "        \n",
    "        for query in queries[:3]:\n",
    "            training_pairs.append({\n",
    "                \"query\": query,\n",
    "                \"positive\": doc.page_content\n",
    "            })\n",
    "    \n",
    "    return training_pairs\n",
    "\n",
    "# Example usage (commented out to avoid API calls in demo)\n",
    "# llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.7)\n",
    "# auto_generated_data = generate_training_data_from_docs(chunks, llm, num_examples=300)\n",
    "\n",
    "print(\"‚úÖ Auto-generation function defined\")\n",
    "print(\"   Use this to scale up to 1K-10K examples for production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Hyperparameter Tuning\n",
    "\n",
    "Key hyperparameters to experiment with:\n",
    "\n",
    "```python\n",
    "# Training configuration\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=10,              # Try: 5, 10, 20\n",
    "    warmup_steps=100,       # Try: 10%, 20% of total steps\n",
    "    batch_size=16,          # Try: 8, 16, 32 (based on GPU memory)\n",
    "    evaluation_steps=100,   # Evaluate every N steps\n",
    "    save_best_model=True,   # Save model with best eval score\n",
    "    optimizer_params={\n",
    "        'lr': 2e-5          # Try: 1e-5, 2e-5, 5e-5\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### 6.3 Cross-Validation\n",
    "\n",
    "Split your data for proper evaluation:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split 80/20\n",
    "train_data, test_data = train_test_split(training_data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train on train_data, evaluate on test_data\n",
    "```\n",
    "\n",
    "### 6.4 Monitoring & Logging\n",
    "\n",
    "```python\n",
    "# Track training metrics\n",
    "import wandb\n",
    "\n",
    "wandb.init(project=\"rag-embeddings-finetuning\")\n",
    "\n",
    "# Log during training\n",
    "wandb.log({\n",
    "    \"train_loss\": loss,\n",
    "    \"eval_accuracy\": accuracy,\n",
    "    \"eval_mrr\": mrr\n",
    "})\n",
    "```\n",
    "\n",
    "### 6.5 Model Versioning\n",
    "\n",
    "```python\n",
    "# Save with version metadata\n",
    "version = \"v1.0-langchain-rag\"\n",
    "output_path = f\"data/models/finetuned-embeddings-{version}\"\n",
    "\n",
    "model.save(output_path)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"version\": version,\n",
    "    \"base_model\": \"all-MiniLM-L6-v2\",\n",
    "    \"training_examples\": len(train_data),\n",
    "    \"epochs\": 10,\n",
    "    \"eval_accuracy\": finetuned_accuracy,\n",
    "    \"eval_mrr\": finetuned_mrr,\n",
    "    \"improvement_vs_baseline\": improvement\n",
    "}\n",
    "\n",
    "with open(f\"{output_path}/metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cost-Benefit Analysis\n",
    "\n",
    "### Training Costs\n",
    "\n",
    "**One-time costs:**\n",
    "- Dataset generation: $2-10 (if using LLM to generate queries)\n",
    "- GPU training: $0-5 (free on Colab/Kaggle, or ~$1/hour on cloud)\n",
    "- Validation: ~$0.50 (evaluation queries)\n",
    "\n",
    "**Total:** $2.50-$15 one-time\n",
    "\n",
    "### Ongoing Benefits\n",
    "\n",
    "**Quality improvements:**\n",
    "- 15-25% better retrieval accuracy\n",
    "- Better domain-specific understanding\n",
    "- Reduced hallucinations (better context)\n",
    "\n",
    "**Cost savings:**\n",
    "- Local embeddings = $0 per query (vs OpenAI $0.00002/query)\n",
    "- Offline capability\n",
    "- No API rate limits\n",
    "\n",
    "**ROI Example:**\n",
    "```\n",
    "Scenario: 1M queries/month\n",
    "\n",
    "OpenAI embeddings:\n",
    "- Cost: 1M √ó $0.00002 = $20/month\n",
    "- Annual: $240\n",
    "\n",
    "Fine-tuned local embeddings:\n",
    "- Cost: $0/month (after one-time $10 training)\n",
    "- Annual: $10\n",
    "\n",
    "Savings: $230/year + better quality\n",
    "ROI: 23x\n",
    "```\n",
    "\n",
    "### When Fine-tuning Makes Sense\n",
    "\n",
    "‚úÖ **Worth it:**\n",
    "- High query volume (>100K/month)\n",
    "- Domain-specific content\n",
    "- Budget constraints\n",
    "- Offline requirements\n",
    "- Quality < 75% with pre-trained\n",
    "\n",
    "‚ùå **Not worth it:**\n",
    "- Low query volume (<10K/month)\n",
    "- General content\n",
    "- OpenAI already works well (>90%)\n",
    "- Limited training data (<1K examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary & Decision Framework\n",
    "\n",
    "### Quick Decision Tree\n",
    "\n",
    "```\n",
    "Start: Should I fine-tune embeddings?\n",
    "  |\n",
    "  ‚îú‚îÄ Is domain highly specialized? (medical, legal, technical)\n",
    "  ‚îÇ   YES ‚Üí Fine-tune (expected +15-25% accuracy)\n",
    "  ‚îÇ   NO ‚Üí ‚Üì\n",
    "  |\n",
    "  ‚îú‚îÄ Do I have >1K labeled examples?\n",
    "  ‚îÇ   YES ‚Üí Fine-tune\n",
    "  ‚îÇ   NO ‚Üí ‚Üì\n",
    "  |\n",
    "  ‚îú‚îÄ Is baseline accuracy <75%?\n",
    "  ‚îÇ   YES ‚Üí Fine-tune or try Contextual RAG first\n",
    "  ‚îÇ   NO ‚Üí ‚Üì\n",
    "  |\n",
    "  ‚îú‚îÄ Query volume >100K/month?\n",
    "  ‚îÇ   YES ‚Üí Fine-tune (cost savings)\n",
    "  ‚îÇ   NO ‚Üí Stick with pre-trained\n",
    "```\n",
    "\n",
    "### Implementation Checklist\n",
    "\n",
    "**Before fine-tuning:**\n",
    "- [ ] Collect 1K-10K (query, document) pairs\n",
    "- [ ] Measure baseline performance (accuracy, MRR)\n",
    "- [ ] Set target improvement (e.g., +15% accuracy)\n",
    "- [ ] Allocate GPU resources (Colab, Kaggle, or cloud)\n",
    "\n",
    "**During fine-tuning:**\n",
    "- [ ] Split data (80/20 train/test)\n",
    "- [ ] Start with small epochs (3-5)\n",
    "- [ ] Monitor training loss\n",
    "- [ ] Evaluate on held-out test set\n",
    "- [ ] Save best checkpoint\n",
    "\n",
    "**After fine-tuning:**\n",
    "- [ ] Compare vs baseline on test set\n",
    "- [ ] Test on production-like queries\n",
    "- [ ] Integrate into RAG pipeline\n",
    "- [ ] Monitor retrieval quality in production\n",
    "- [ ] Set up retraining schedule (quarterly?)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Scale up training data** to 1K-10K examples\n",
    "2. **Experiment with different base models** (e.g., BGE, E5, instructor)\n",
    "3. **Try different loss functions** (CoSENT, ContrastiveLoss)\n",
    "4. **Implement continuous evaluation** on production queries\n",
    "5. **Set up A/B testing** (baseline vs fine-tuned)\n",
    "\n",
    "---\n",
    "\n",
    "**üìö Related Notebooks:**\n",
    "- [02_embeddings_comparison.ipynb](../fundamentals/02_embeddings_comparison.ipynb) - OpenAI vs HuggingFace baseline\n",
    "- [12_contextual_rag.ipynb](12_contextual_rag.ipynb) - Alternative quality improvement\n",
    "- [16_evaluation_ragas.ipynb](16_evaluation_ragas.ipynb) - Comprehensive quality metrics\n",
    "\n",
    "**üîó External Resources:**\n",
    "- [Sentence Transformers Docs](https://www.sbert.net/)\n",
    "- [Fine-tuning Guide](https://www.sbert.net/docs/training/overview.html)\n",
    "- [BEIR Benchmark](https://github.com/beir-cellar/beir) - Retrieval evaluation\n",
    "\n",
    "---\n",
    "\n",
    "üéâ **Fine-tuning Complete!**\n",
    "\n",
    "You now understand when and how to fine-tune embeddings for domain-specific RAG applications!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}