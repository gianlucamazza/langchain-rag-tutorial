{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Branched RAG (Multi-Query Retrieval)\n",
    "\n",
    "**Complexity:** â­â­â­\n",
    "\n",
    "**Use Cases:** Multi-intent queries, cross-domain research, comprehensive topic exploration\n",
    "\n",
    "**Key Feature:** Generates multiple sub-queries from user question, retrieves in parallel for better coverage.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Query: \"Compare OpenAI and HuggingFace embeddings for cost and performance\"\n",
    "\n",
    "Generated sub-queries:\n",
    "1. \"OpenAI embeddings pricing and cost\"\n",
    "2. \"HuggingFace embeddings performance benchmarks\"\n",
    "3. \"Comparison of embedding providers\"\n",
    "\n",
    "â†’ Retrieves diverse documents covering all aspects\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SETUP: BRANCHED RAG\n",
      "================================================================================\n",
      "\n",
      "âœ“ Loaded vector store from /Users/gianlucamazza/Workspace/notebooks/llm_rag/notebooks/advanced_architectures/../../data/vector_stores/openai_embeddings\n",
      "âœ… Setup complete!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from shared.config import OPENAI_VECTOR_STORE_PATH, DEFAULT_MODEL\n",
    "from shared.utils import load_vector_store, print_section_header, format_docs\n",
    "from shared.prompts import RAG_PROMPT_TEMPLATE, MULTI_QUERY_PROMPT\n",
    "\n",
    "print_section_header(\"Setup: Branched RAG\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = load_vector_store(OPENAI_VECTOR_STORE_PATH, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "llm = ChatOpenAI(model=DEFAULT_MODEL, temperature=0)\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Query Retriever\n",
    "\n",
    "Uses `MultiQueryRetriever` to generate alternative queries and retrieve documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MULTI-QUERY RETRIEVER\n",
      "================================================================================\n",
      "\n",
      "âœ“ Custom MultiQueryRetriever created\n",
      "  - Generates 3 alternative queries\n",
      "  - Retrieves documents for each\n",
      "  - Deduplicates results\n",
      "  - No legacy dependencies!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "import logging\n",
    "\n",
    "print_section_header(\"Multi-Query Retriever\")\n",
    "\n",
    "# Custom multi-query retriever implementation (no legacy dependencies)\n",
    "class CustomMultiQueryRetriever:\n",
    "    \"\"\"Custom multi-query retriever that generates multiple query variations.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_retriever, llm, prompt):\n",
    "        self.base_retriever = base_retriever\n",
    "        self.llm = llm\n",
    "        self.prompt = prompt\n",
    "        self.logger = logging.getLogger(\"custom.multi_query\")\n",
    "        \n",
    "    def invoke(self, query: str) -> list:\n",
    "        \"\"\"Retrieve documents using multiple query variations.\"\"\"\n",
    "        # Generate alternative queries\n",
    "        query_gen_chain = self.prompt | self.llm | StrOutputParser()\n",
    "        generated = query_gen_chain.invoke({\"question\": query})\n",
    "        \n",
    "        # Parse generated queries (one per line)\n",
    "        alternative_queries = [q.strip() for q in generated.split('\\n') if q.strip()]\n",
    "        all_queries = [query] + alternative_queries[:3]  # Original + top 3 alternatives\n",
    "        \n",
    "        self.logger.info(f\"Generated queries: {all_queries}\")\n",
    "        \n",
    "        # Retrieve documents for each query\n",
    "        all_docs = []\n",
    "        for q in all_queries:\n",
    "            docs = self.base_retriever.invoke(q)\n",
    "            all_docs.extend(docs)\n",
    "        \n",
    "        # Deduplicate by content\n",
    "        unique_docs = []\n",
    "        seen_content = set()\n",
    "        for doc in all_docs:\n",
    "            content_hash = doc.page_content[:200]  # Use first 200 chars for dedup\n",
    "            if content_hash not in seen_content:\n",
    "                unique_docs.append(doc)\n",
    "                seen_content.add(content_hash)\n",
    "        \n",
    "        return unique_docs\n",
    "\n",
    "# Enable logging to see generated queries\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logging.getLogger(\"custom.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "# Create custom multi-query retriever\n",
    "multi_query_retriever = CustomMultiQueryRetriever(\n",
    "    base_retriever=retriever,\n",
    "    llm=llm,\n",
    "    prompt=MULTI_QUERY_PROMPT\n",
    ")\n",
    "\n",
    "print(\"âœ“ Custom MultiQueryRetriever created\")\n",
    "print(\"  - Generates 3 alternative queries\")\n",
    "print(\"  - Retrieves documents for each\")\n",
    "print(\"  - Deduplicates results\")\n",
    "print(\"  - No legacy dependencies!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Multi-Query Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "MULTI-QUERY TEST\n",
      "================================================================================\n",
      "\n",
      "Original Query: 'How to optimize RAG system performance?'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:custom.multi_query:Generated queries: ['How to optimize RAG system performance?', 'What are the best practices for improving the performance of a RAG system?', 'How can I enhance the efficiency of a RAG system?', 'What strategies can be implemented to optimize the performance of a RAG system?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ“ Retrieved 2 unique documents\n",
      "\n",
      "Multi-Query Results\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: We can see the full sequence of steps, along with latency and other metadata, in the LangSmith trace.\n",
      "You can add a deeper level of control and custom...\n",
      "\n",
      "2. Source: https://python.langchain.com/docs/use_cases/question_answering/\n",
      "   Type: web_documentation\n",
      "   Date: 2025-11-12\n",
      "   Content: A RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\n",
      "A two-step RAG chain that uses just a single LLM...\n"
     ]
    }
   ],
   "source": [
    "from shared.utils import print_results\n",
    "\n",
    "print_section_header(\"Multi-Query Test\")\n",
    "\n",
    "query = \"How to optimize RAG system performance?\"\n",
    "print(f\"Original Query: '{query}'\\n\")\n",
    "\n",
    "# Retrieve with multi-query\n",
    "docs = multi_query_retriever.invoke(query)\n",
    "\n",
    "print(f\"\\nâœ“ Retrieved {len(docs)} unique documents\")\n",
    "print_results(docs, \"Multi-Query Results\", max_docs=4, preview_length=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Branched RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BRANCHED RAG CHAIN\n",
      "================================================================================\n",
      "\n",
      "âœ“ Branched RAG chain created\n",
      "\n",
      "Query: 'What are the differences between similarity and MMR retrieval?'\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:custom.multi_query:Generated queries: ['What are the differences between similarity and MMR retrieval?', 'What distinguishes similarity retrieval from MMR retrieval?', 'How do similarity retrieval and MMR retrieval differ from each other?', 'Can you explain the differences between similarity-based retrieval and MMR retrieval?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The context provided does not contain any information about the differences between similarity and MMR (Maximal Marginal Relevance) retrieval. Therefore, I cannot answer your question based on the given context.\n",
      "\n",
      "================================================================================\n",
      "\n",
      "âœ… Branched RAG provides more comprehensive answers!\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Branched RAG Chain\")\n",
    "\n",
    "def multi_query_retrieve(query: str):\n",
    "    \"\"\"Wrapper function for multi-query retrieval.\"\"\"\n",
    "    return multi_query_retriever.invoke(query)\n",
    "\n",
    "branched_chain = (\n",
    "    {\"context\": RunnableLambda(multi_query_retrieve) | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | RAG_PROMPT_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"âœ“ Branched RAG chain created\")\n",
    "\n",
    "# Test\n",
    "query = \"What are the differences between similarity and MMR retrieval?\"\n",
    "print(f\"\\nQuery: '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "response = branched_chain.invoke(query)\n",
    "print(response)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\nâœ… Branched RAG provides more comprehensive answers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison: Simple vs Branched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Query: 'How to implement embeddings in RAG?'\n",
      "\n",
      "[SIMPLE RAG]\n",
      "Documents retrieved: 4\n",
      "\n",
      "[BRANCHED RAG]\n",
      "Documents retrieved: 4\n",
      "\n",
      "[BRANCHED RAG]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:custom.multi_query:Generated queries: ['How to implement embeddings in RAG?', 'What are the steps to implement embeddings in a Retrieval-Augmented Generation (RAG) model?', 'Can you explain the process of integrating embeddings into a RAG framework?', 'What methods can be used to apply embeddings in a Retrieval-Augmented Generation system?']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents retrieved: 5 (from multiple queries)\n",
      "\n",
      "ðŸ’¡ Branched RAG typically retrieves more diverse documents\n"
     ]
    }
   ],
   "source": [
    "print_section_header(\"Comparison\")\n",
    "\n",
    "# Simple RAG\n",
    "simple_chain = (\n",
    "    {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | RAG_PROMPT_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "query = \"How to implement embeddings in RAG?\"\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"[SIMPLE RAG]\")\n",
    "simple_docs = retriever.invoke(query)\n",
    "print(f\"Documents retrieved: {len(simple_docs)}\")\n",
    "\n",
    "print(\"\\n[BRANCHED RAG]\")\n",
    "branched_docs = multi_query_retriever.invoke(query)\n",
    "print(f\"Documents retrieved: {len(branched_docs)} (from multiple queries)\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Branched RAG typically retrieves more diverse documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "User Query â†’ Generate Sub-Queries â†’ Parallel Retrieval â†’ Deduplicate â†’ LLM â†’ Response\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "âœ… Better coverage for complex queries  \n",
    "âœ… Captures multiple aspects  \n",
    "âœ… More diverse perspectives  \n",
    "âœ… Handles multi-intent questions  \n",
    "\n",
    "**Limitations:**\n",
    "- Higher latency (multiple retrievals)\n",
    "- More API calls (cost)\n",
    "- May retrieve redundant info\n",
    "\n",
    "**When to Use:**\n",
    "- Broad research questions\n",
    "- Multi-concept queries\n",
    "- Comprehensive coverage needed\n",
    "\n",
    "**Next:** [06_hyde.ipynb](06_hyde.ipynb) - Hypothetical Document Embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
