{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Branched RAG (Multi-Query Retrieval)\n",
    "\n",
    "**Complexity:** ‚≠ê‚≠ê‚≠ê\n",
    "\n",
    "**Use Cases:** Multi-intent queries, cross-domain research, comprehensive topic exploration\n",
    "\n",
    "**Key Feature:** Generates multiple sub-queries from user question, retrieves in parallel for better coverage.\n",
    "\n",
    "**Example:**\n",
    "```\n",
    "Query: \"Compare OpenAI and HuggingFace embeddings for cost and performance\"\n",
    "\n",
    "Generated sub-queries:\n",
    "1. \"OpenAI embeddings pricing and cost\"\n",
    "2. \"HuggingFace embeddings performance benchmarks\"\n",
    "3. \"Comparison of embedding providers\"\n",
    "\n",
    "‚Üí Retrieves diverse documents covering all aspects\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from shared.config import OPENAI_VECTOR_STORE_PATH, DEFAULT_MODEL\n",
    "from shared.utils import load_vector_store, print_section_header, format_docs\n",
    "from shared.prompts import RAG_PROMPT_TEMPLATE, MULTI_QUERY_PROMPT\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "print_section_header(\"Setup: Branched RAG\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = load_vector_store(OPENAI_VECTOR_STORE_PATH, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "llm = ChatOpenAI(model=DEFAULT_MODEL, temperature=0)\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-Query Retriever\n",
    "\n",
    "Uses `MultiQueryRetriever` to generate alternative queries and retrieve documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "import logging\n",
    "\n",
    "print_section_header(\"Multi-Query Retriever\")\n",
    "\n",
    "# Enable logging to see generated queries\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)\n",
    "\n",
    "# Create MultiQueryRetriever\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=retriever,\n",
    "    llm=llm,\n",
    "    prompt=MULTI_QUERY_PROMPT\n",
    ")\n",
    "\n",
    "print(\"‚úì MultiQueryRetriever created\")\n",
    "print(\"  - Generates 3 alternative queries\")\n",
    "print(\"  - Retrieves documents for each\")\n",
    "print(\"  - Deduplicates results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Multi-Query Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.utils import print_results\n",
    "\n",
    "print_section_header(\"Multi-Query Test\")\n",
    "\n",
    "query = \"How to optimize RAG system performance?\"\n",
    "print(f\"Original Query: '{query}'\\n\")\n",
    "\n",
    "# Retrieve with multi-query\n",
    "docs = multi_query_retriever.invoke(query)\n",
    "\n",
    "print(f\"\\n‚úì Retrieved {len(docs)} unique documents\")\n",
    "print_results(docs, \"Multi-Query Results\", max_docs=4, preview_length=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build Branched RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Branched RAG Chain\")\n",
    "\n",
    "branched_chain = (\n",
    "    {\"context\": multi_query_retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | RAG_PROMPT_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"‚úì Branched RAG chain created\")\n",
    "\n",
    "# Test\n",
    "query = \"What are the differences between similarity and MMR retrieval?\"\n",
    "print(f\"\\nQuery: '{query}'\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "response = branched_chain.invoke(query)\n",
    "print(response)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"\\n‚úÖ Branched RAG provides more comprehensive answers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison: Simple vs Branched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Comparison\")\n",
    "\n",
    "# Simple RAG\n",
    "simple_chain = (\n",
    "    {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "    | RAG_PROMPT_TEMPLATE\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "query = \"How to implement embeddings in RAG?\"\n",
    "\n",
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"[SIMPLE RAG]\")\n",
    "simple_docs = retriever.invoke(query)\n",
    "print(f\"Documents retrieved: {len(simple_docs)}\")\n",
    "\n",
    "print(\"\\n[BRANCHED RAG]\")\n",
    "branched_docs = multi_query_retriever.invoke(query)\n",
    "print(f\"Documents retrieved: {len(branched_docs)} (from multiple queries)\")\n",
    "\n",
    "print(\"\\nüí° Branched RAG typically retrieves more diverse documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Flow:**\n",
    "```\n",
    "User Query ‚Üí Generate Sub-Queries ‚Üí Parallel Retrieval ‚Üí Deduplicate ‚Üí LLM ‚Üí Response\n",
    "```\n",
    "\n",
    "**Advantages:**\n",
    "‚úÖ Better coverage for complex queries  \n",
    "‚úÖ Captures multiple aspects  \n",
    "‚úÖ More diverse perspectives  \n",
    "‚úÖ Handles multi-intent questions  \n",
    "\n",
    "**Limitations:**\n",
    "- Higher latency (multiple retrievals)\n",
    "- More API calls (cost)\n",
    "- May retrieve redundant info\n",
    "\n",
    "**When to Use:**\n",
    "- Broad research questions\n",
    "- Multi-concept queries\n",
    "- Comprehensive coverage needed\n",
    "\n",
    "**Next:** [06_hyde.ipynb](06_hyde.ipynb) - Hypothetical Document Embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
